{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.17.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NEvjeYgh__0e",
        "outputId": "d89096cc-6b3f-4a67-91a6-ddb6d450f743"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.17.2\n",
            "  Downloading torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.32.3)\n",
            "Collecting torch==2.2.2 (from torchtext==0.17.2)\n",
            "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2->torchtext==0.17.2)\n",
            "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchtext==0.17.2) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2->torchtext==0.17.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2->torchtext==0.17.2) (1.3.0)\n",
            "Downloading torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 torchtext-0.17.2 triton-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "6d4072a2f4ea42648f81f54a08664788"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loaVT5fT_RAG",
        "outputId": "db080fa9-2c28-4dc8-934a-c12cdae6a917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-5951bf76e549>\", line 9, in <cell line: 0>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import accumulate\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "import torch\n",
        "torch.set_num_threads(1)\n",
        "from torch import nn\n",
        "import os\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from IPython.display import Markdown as md\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, GloVe, Vectors\n",
        "from torchtext.datasets import IMDB\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "import pickle\n",
        "\n",
        "from urllib.request import urlopen\n",
        "import io\n",
        "\n",
        "import tarfile\n",
        "import tempfile\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(COST,ACC):\n",
        "\n",
        "    fig, ax1 = plt.subplots()\n",
        "    color = 'tab:red'\n",
        "    ax1.plot(COST, color=color)\n",
        "    ax1.set_xlabel('epoch', color=color)\n",
        "    ax1.set_ylabel('total loss', color=color)\n",
        "    ax1.tick_params(axis='y', color=color)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:blue'\n",
        "    ax2.set_ylabel('accuracy', color=color)  # you already handled the x-label with ax1\n",
        "    ax2.plot(ACC, color=color)\n",
        "    ax2.tick_params(axis='y', color=color)\n",
        "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def save_list_to_file(lst, filename):\n",
        "    \"\"\"\n",
        "    Save a list to a file using pickle serialization.\n",
        "\n",
        "    Parameters:\n",
        "        lst (list): The list to be saved.\n",
        "        filename (str): The name of the file to save the list to.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(lst, file)\n",
        "\n",
        "def load_list_from_file(filename):\n",
        "    \"\"\"\n",
        "    Load a list from a file using pickle deserialization.\n",
        "\n",
        "    Parameters:\n",
        "        filename (str): The name of the file to load the list from.\n",
        "\n",
        "    Returns:\n",
        "        list: The loaded list.\n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as file:\n",
        "        loaded_list = pickle.load(file)\n",
        "    return loaded_list"
      ],
      "metadata": {
        "id": "qXUnnAdcIW3n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional encodings\n",
        "\n",
        "Positional encodings play a pivotal role in transformers and various sequence-to-sequence models, aiding in conveying critical information regarding the positions or sequencing of elements within a given sequence. To illustrate, let's examine the sentences: \"He painted the car red\" and \"He painted the red car.\" Despite their distinct meanings, it's worth noting that the embeddings for these sentences remain identical in the absence of positional encodings. The following class defines positional encodings by inheriting from PyTorch's `Module` class.\n"
      ],
      "metadata": {
        "id": "3U98URt-Jiq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(vocab_size, d_model)\n",
        "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float()\n",
        "            * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "rU4sCEtZJjpN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import IMDB data set\n"
      ],
      "metadata": {
        "id": "T5lHbqubJnmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/35t-FeC-2uN1ozOwPs7wFg.gz')\n",
        "tar = tarfile.open(fileobj=io.BytesIO(urlopened.read()))\n",
        "tempdir = tempfile.TemporaryDirectory()\n",
        "tar.extractall(tempdir.name)\n",
        "tar.close()"
      ],
      "metadata": {
        "id": "CF_vQn2WJon-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMDB data set overview\n",
        "\n",
        "The **IMDB data set** contains movie reviews from the Internet Movie Database (IMDB) and is commonly used for binary sentiment classification tasks. It's a popular data set for training and testing models in natural language processing (NLP), particularly in the context of sentiment analysis.\n",
        "\n",
        "### Data set composition\n",
        "\n",
        "- **Reviews**: The data set consists of 50,000 movie reviews, divided evenly into 25,000 training and 25,000 testing samples.\n",
        "- **Sentiment labels**: Each review is labeled as either positive or negative, indicating the sentiment expressed in the review. The data set is balanced, with an equal number of positive and negative reviews in both the training and testing sets.\n",
        "- **Text content**: Reviews are presented as plain text and have been preprocessed to some extent. For example, HTML tags are removed, but the text retains its original punctuation and capitalization.\n",
        "- **Usage**: The data set is commonly used to train models for binary sentiment classification, where the goal is to predict whether a given review is positive or negative based on its text content.\n",
        "\n",
        "### Applications\n",
        "\n",
        "- **Sentiment analysis**: The primary application of the IMDB data set is in sentiment analysis, where it serves as a benchmark for various text classification algorithms.\n",
        "- **Natural language processing**: The data set is widely used in NLP research and applications, providing a basis for testing the effectiveness of different models and approaches in understanding human language.\n",
        "\n",
        "### Challenges\n",
        "\n",
        "The data set is small, so it's hard to train a model from scratch.\n",
        "\n",
        "The following class is defined to traverse the IMDB data set. The need to define this class arises from the fact that the IMDB data set is split across a large number of files.\n"
      ],
      "metadata": {
        "id": "4oougga7Jw35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, root_dir, train=True):\n",
        "        \"\"\"\n",
        "        root_dir: The base directory of the IMDB dataset.\n",
        "        train: A boolean flag indicating whether to use training or test data.\n",
        "        \"\"\"\n",
        "        self.root_dir = os.path.join(root_dir, \"train\" if train else \"test\")\n",
        "        self.neg_files = [os.path.join(self.root_dir, \"neg\", f) for f in os.listdir(os.path.join(self.root_dir, \"neg\")) if f.endswith('.txt')]\n",
        "        self.pos_files = [os.path.join(self.root_dir, \"pos\", f) for f in os.listdir(os.path.join(self.root_dir, \"pos\")) if f.endswith('.txt')]\n",
        "        self.files = self.neg_files + self.pos_files\n",
        "        self.labels = [0] * len(self.neg_files) + [1] * len(self.pos_files)\n",
        "        self.pos_inx=len(self.pos_files)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.files[idx]\n",
        "        label = self.labels[idx]\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "        return label, content"
      ],
      "metadata": {
        "id": "W6tk1iJbJtit"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code uses the `IMDBDataset` class previously defined to create iterators for the train and test data sets. In the latter part of the cell, you can return 20 examples from the train set.\n"
      ],
      "metadata": {
        "id": "vuA95U95KBnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = tempdir.name + '/' + 'imdb_dataset'\n",
        "train_iter = IMDBDataset(root_dir=root_dir, train=True)  # For training data\n",
        "test_iter = IMDBDataset(root_dir=root_dir, train=False)  # For test data\n",
        "\n",
        "start=train_iter.pos_inx\n",
        "for i in range(-10,10):\n",
        "    print(train_iter[start+i])"
      ],
      "metadata": {
        "id": "7DqXtrPHKDv6",
        "outputId": "5a03fe8c-cdd9-4ddc-e113-094992482237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 'This is one of those horror flicks where twenty-somethings fool around with the dark arts around a camp fire, getting into a heap of trouble for doing so. A portal was opened containing a world of demons known as the Kelippoth of the Sitra Achra by a man whose daughter, Summer, gets kidnapped by something, taken into it. Summer is trained by a mysterious group whose identities are never revealed to battle the demon monsters. This is a portion of the plot which lends itself to scrutiny. Anyway, three wannabee witches, who went to high school together, Renea, the most enthusiastic, serious practitioner in the dark arts, and her lesbian cohorts, Jasmine and Marlene(..it\\'s more or less a passing fad with them, though..) join up with buddies, Jason and Ricky, on a trip in the wilderness where Summer vanished from her home ten years ago. Opening the portal through a spoken text written in an ancient book, a demon is set free, as is Summer, now a warrior babe whose training has led to a very fit and athletic body and skills that have been needed to ward off monsters in the other world.<br /><br />Low budget contains a loopy, but ambitious story, restraining it into a confined setting. These young adults spend a lot of time running around in the woods hoping not to be fodder for a beast. As can be the case in these movies, the demon stands on the sidelines while the story develops as Summer attempts to remember how everything came to pass, while befriending Jason who wishes to help her restore the lost time. The action is shot mostly in the dark, making any violence hard to decipher. Brigitte Kingsley(and the rest of the female cast for that matter), is some mighty nice eye candy, dressed scantily clad as a female Conan, a gorgeous body we have to pleasure to gaze upon from the moment she appears until the closing of the movie. Some lesbianism(..some kissing and fondling)and nudity spice things up nicely, and the cast seem to be having fun with the goofy plot..it\\'s so preposterous that the silly tone is probably appropriate for the material.<br /><br />Might be of interest for co-starring World Wrestling Entertainment\\'s \"Captain Courageous\" Christian(real name, Jason Reso)as one of the group, spoofing his alter ego, as a chicken, quivering at the sound of a snapping tree twig. Landy Cannon is likable as unlikely hero, Jason, a lovestruck, naive young man whose ex-fiancé, Jasmine(Vanessa James)is now bi-sexual and in love with Marlene(..Jasmine\\'s cruelty is in toying with Jason\\'s feelings by hiding her affair with Marlene from his knowledge), while Ricky and Renea attempt to steer him away from this idea that he can rekindle a dead flame that gone out, never to ignite again. The Kelippoth demon is mostly darkly lit, I guess to refrain from showing how ludicrous/laughable it looks if presented in full. The lesbian antics of Jasmine and Marlene(Haley Shannon) is mostly tame, their love making, once alone in the woods up against a tree, is toned down and also lighted using the blackness of night. My rating is a bit favorable towards it, almost solely because of Kingsley, for purely superficial reasons, rather than the plot or film-making. The movie aims to please and is marketed to the boys(and girls who love hot women). I think, though, for the most part, the humor falls a bit flat.')\n",
            "(0, 'Having the In-Laws over for the weekend? Then this is the film to hasten their departure, failing that it will induce a catatonic state to bring a welcome relief from constant nagging.<br /><br />The film is supposedly set on board a luxury cruise ship, which is more superannuated car ferry; the plot has more holes than the average colander and a cast dredged from the depths of the celebrity D list. An interesting piece of added amusement is playing \"Spot the Villain\" as passengers join the ship. You won\\'t be wrong!!!! With a script that sinks faster than a brick, clichéd set pieces and copious amounts of raspberry jam doubling as blood this film attempts to encompass the genres of thriller, action movie and gore-fest and simultaneously fails to fulfil any of them.<br /><br />A must watch film, if only to laugh at how bad it is.')\n",
            "(0, \"No redeeming features, this film is rubbish. Its jokes don't begin to be funny. The humour for children is pathetic, and the attempts to appeal to adults just add a tacky smuttishness to the whole miserable package. Sitting through it with my children just made me uncomfortable about what might be coming next. I couldn't enjoy the film at all. Although my child for whom the DVD was bought enjoyed the fact that she owned a new DVD, neither she nor her sisters expressed much interest in seeing it again, unlike with Monsters inc, Finding Nemo, Jungle Book, Lion King, etc. which all get frequent requests for replays.\")\n",
            "(0, 'I am a big fan of Ludlum\\'s work, and of the Covert-one books, and I had often thought how incredible they would be made into a film. Imagine my excitement, then, on learning that such a movie actually existed! The \\'Hades Factor\\' being the first in the series seemed an obvious place to start.<br /><br />From the outset the film was disappointing. Simple elements from the film such as Griffin\\'s first meeting with Smith are needlessly different from the book, and much less exhilarating. Several characters are poorly cast, too. For starters Dorff is woeful as Smith. Not a bad actor, just an incredibly bad choice as he is far too soft, and fails to exhibit many of the features that are definitive of John Smith.<br /><br />Re-naming, re-assignment and even omission of certain characters further degrades this film. For example the removal of Victor Tremont and the entire back-story of the virus, including the involvement of VAXHAM makes the entire point to the film somewhat hazy. Marty Zellerbach is a very large part of the book, and in the seat he takes vary much a back seat (not to mention that the film character shares nothing in common with the character in the book) is another big mistake.<br /><br />Rachel Russel is presumably supposed to be Randi Russel from the book. Not only is she supposed to be the sister of Sophie Amsden (should be called Sophia Russel) but she is also supposed to work from the CIA, NOT \"Covert-one\". Which brings me to my final point, and I think one of the most important. COVERT-ONE doesn\\'t even exist at this point! Not until the second book of the series is Covert-One devised by the president as a preventative measure against further biological terrorism.<br /><br />To be honest I could go on all day. In short - if you like the books and want to see a good adaptation, I\\'m afraid you\\'ll be bitterly disappointed. Even as an action movie it is thoroughly average, mainly due to very lack-luster editing and poor effects. The bumbled story line and dull-as-ditch-water script are the final nails in the very cheap coffin of this film.')\n",
            "(0, 'This is a movie about how men think women think about love. No woman describes a one-night sexual encounter and declares it a love story.<br /><br />Of the ten monologues I felt only three really had any kind of truth ring through them. I kept waiting for the film to get better, and it did a bit, but never better enough.<br /><br />This is an interesting concept, and I kept wanting it to be good, but it never succeeded. Maybe if they actually WERE love stories it would have worked.')\n",
            "(0, \"I never saw the original 1954 version with Judy Garland, so have no means of comparison. Also, it's been some years, but I found this tale neither gripping nor its romance captivating. The movie tells the story of two lovers whose musical careers are headed in opposite directions. John Norman Howard is a worn out, disillusioned rock star on the decline, embarking upon a romance with a fresh, talented new singing sensation, Esther Hoffman. Her dramatic success only serves to emphasize his decline.<br /><br />The lead actors, Kris Kristofferson and Barbra Streisand, are adequate in their roles, but neither their chemistry nor the plot left much of a mark with me. The film is noteworthy to me for only one aspect, Streisand's beautiful rendition of the Oscar winning song 'Evergreen'. She truly has a powerful and magnificent voice.\")\n",
            "(0, 'The ending of this movie made absolutely NO SENSE. What a waste of 2 perfectly good hours. If you can explain it to me...PLEASE DO. I don\\'t usually consider myself unable to \"get\" a movie, but this was a classic example for me, so either I\\'m slower than I think, or this was a REALLY bad movie.')\n",
            "(0, \"Take your basic Frankenstein flick, inject some Reanimator (but not the good parts), and you have Doctor Hackenstein. Certainly, this was obviously inspired by aforementioned films but it never materializes as anything special on its own.<br /><br />A scientist accidentally kills his wife, so the whole movie takes place over the course of one night as he attempts to revive his wife. To revive his wife, he decides to chop off body parts from some women that have become stranded and, coincidentally, decide to stay the night at his place.<br /><br />I can't really say the acting is bad, nor is the directing. Everything here is just way too standard. What little attempts there are at humor actually work (check out the scene when Hackenstein keeps hiding behind his deaf assistant because she would undoubtedly be very upset if she saw him clutching a woman and a needle), but that's hardly enough to recommend this film. The music is decent, what blood that's there is decent, and the cast looks quite good. And for half of the time, I was even entertained by this film. But I never felt like this was anything more than a time waster. Avoidable.<br /><br />Try Frankenhooker instead.\")\n",
            "(0, \"Dull haunted house thriller finds an American family moving into a 200 year old house in Japan where a violent murder suicide love triangle occurred. <br /><br />Novel setting is about the only element of interest in this very slow moving horror flick by the director of Motel Hell. The film generates zero suspense and is composed of somewhat choppy scenes that rarely seem to be leading anywhere overall. <br /><br />One obvious example is a fairly early scene where the male lead visits a temple after realizing that his house is haunted as the monk had earlier warned. The monk recounts the history of the house (which the viewer is already familiar with from the opening sequence) and then the film simply cuts away to something else. Earlier the monk had offered to help. Well, where is the help? The family continues to stay in the haunted house as things get worse and worse and no mention of the monk is made until nearly the very end when he turns up again to do what he should have done an hour earlier--try to drive the spirits out of the house, although by this time it's difficult for the viewers to care.<br /><br />There are some (probably) unintentional campy laughs in seeing the American actors at the end become possessed by the Japanese spirits and suddenly start doing bad martial arts, I say probably because the scene is more than a little reminiscent of the chainsaw duel from the same director's Motel Hell which was more obviously meant to be amusing, but on the whole this is a forgettable dud.\")\n",
            "(0, \"Heavily re-edited and often confusing, the original screen version of Man On Fire was at least ten years out of date when it was made and the passing years haven't made it any better. This is the kind of movie that producers with too much money and too little experience make to get attention and everyone else does just to pay off their outstanding alimony or their drug dealer, with Scott Glenn's bodyguard going out on a limb to rescue his 12-year-old charge, the kidnapped daughter of a wealthy Italian family. An interesting cast - Joe Pesci, Brooke Adams, Danny Aiello, Jonathan Pryce - have all done better, the action is sluggish and sparse and only John Scott's exceptionally fine score (part of which turned up in the last reel of Die Hard) makes a positive impression. One case where the remake (made by Tony Scott, the original choice of director for this version) is an improvement.\")\n",
            "(1, \"When I first read the plot of this drama i assumed it was going to be like Sex and the City, however this drama is nothing like it. The stories the characters seem more real and you empathise with the situations more. The concept of the drama is similar, four 30 something women guide us through there friendships and relationships with problems and strife along the way. Katie the GP is a dark and brooding character who you find difficult to relate too and is best friends with Trudi a widow. Trudi's character is heart warming as you can relate to difficulties she is having along with the fact she is the only mother of the four. Jessica is the party girl very single minded and knows what she wants and how to get it. She is a likable character and is closest to Siobhan the newly wed who whilst loving her husband completely can't help her eyes wandering to her work colleague. Over all the drama is surprisingly addictive and if the BBC continue to produce the series it could do well. It is unlike other female cast dramas such as Sex and the city, or Desperate Housewives. This if played right could be the next Cold feet. Plus the male cast are not bad on the eyes too.\")\n",
            "(1, 'This movie has been made by one of the most absurd humorists in Canada, Yves P. Pelletier. I was shocked for a second that he made a ROMANTIC comedy, but knowing he was a heavy cinephile, was seen in every local festival and in the local cinematheque, I had a positive feeling about this movie.<br /><br />Hell, I was right. Right off the bat, the scenario (written by Pelletier himself) is a bit twisted and hard to follow, but, in Pelletier\\'s fashion it\\'s a one-of-a-kind 90 minutes jack-in-the-box.<br /><br />Loosely inspired and mostly transformed allusion to Dangerous Liaisons (by Laclos) \"Les Aimants\" consists of a twisted game of writing notes on the fridge. Throughout the movie you\\'ll get the occasion to find out who\\'s who and who\\'s writing to who on that goddam fridge....which pops up in an interesting love affair.<br /><br />Great storyline, great photography, great quotations of other movies. Should we ask more for a first movie?')\n",
            "(1, 'First ever viewing: July 21, 2008<br /><br />Very impressive screenplay and comedic acting and timing in this film. Now 40 years old, it has lost none of it\\'s power. Neil Simon displays excellent insight into human nature and relationships as well as how to create genuine comedy from unusual situations. Jack Lemmon and Walter Matthau give great comedic performances. Neil Simon was inspired by actual events in his own life to write the play this film is based on.<br /><br />One of the best written and acted Hollywood comedies of all time!<br /><br />Surprisingly, only nominated for 2 Academy Awards: \"Best Adapted Screenplay\" and \"Best Film Editing\". Hollywood rarely awards comedies, no matter how well they are made.')\n",
            "(1, 'myself and 2 sisters watched all 3 series of Tenko and agree this is by far one of the BBC better series.The whole cast were very convincing in the parts they portrayed and although the 3rd series was somewhat slower it was compelling viewing and my evenings wont be the same without it.No doubt we will be watching it again as it is a series which I would never get sick of watching.Excellent viewing and full marks to the BBC for such a brilliant series and the casting.First rate in all departments and would recommend this series to anyone although some age limits must be considered because of some adult material.So grateful to the BBC for releasing this series on DVD and Video.')\n",
            "(1, 'This is a film.., not porn.<br /><br />This is a wonderful film!!! Full of tender moments and memories!! A beautiful piece of work!!! Excellent!!! For intelligent, viewers only!!!<br /><br />If you are a film lover. A romantic. A person who has loved deeply, this is your film!!!!<br /><br />It has a beautiful surreal quality. Fine acting and directing. Watching this film made me remember my first love.<br /><br />Thi is a film for those who want to reflect on life, love and the meaning of loss.<br /><br />Highly recommended for all film lovers.')\n",
            "(1, \"Meryl Streep is such a genius. Well, at least as an actress. I know she's been made fun of for doing a lot of roles with accents, but she nails the accent every time. Her performance as Lindy Chamberlain was inspiring. Mrs. Chamberlain, as portrayed here, was not particularly likable, nor all that smart. But that just makes Streep's work all the more remarkable. I think she is worth all 10 or so of her Oscar nominations. About the film, well, there were a couple of interesting things. I don't know much about Australia, but the theme of religious bigotry among the general public played a big part in the story. I had largely missed this when I first saw the film some years ago, but it came through loud and clear yesterday. And it seems the Australian press is just as accomplished at misery-inducing pursuit and overkill as their American colleagues. A pretty good film. A bit different. Grade: B\")\n",
            "(1, 'THE NOTORIOUS BETTIE PAGE (2006) ***1/2 Gretchen Mol, Lilli Taylor, Chris Bauer, Jared Harris, Sarah Paulson, David Strathairn, Austin Pendleton, Norman Reedus, Dallas Roberts. <br /><br />Mol shines as legendary pin-up queen Bettie Page in a fine biopic.<br /><br />Gretchen Mol is probably best known for being tauted as The Next-It-Girl a few years ago when no one knew who she was despite an infamous cover story by Vanity Fair among other media dubbings but despite a few co-starring roles here and there her predicted stardom seemed to twinkle less brightly until now. Here as the famous pin-up queen Bettie Page, Gretchen Mol is indeed a shining star on the rise.<br /><br />Bettie Page was Tennessee born raised as a God-fearing and family oriented proper girl who seemed to find herself the unwarranted object of lust and affection, as she grew older. A gang-rape that is flashbacked is wisely not graphically depicted nor is the subtle showing of her own father having less- than-decent designs for his own kin which is important to understand how Page managed to escape the possible nightmarish life for a career as an actress by heading to New York City in the 1950s when in fact that was the time and place to be to catch lighting in a bottle. What Page didn\\'t realize is that in fact she would be just that when she arrived.<br /><br />A beautiful raven haired sweetheart with a divine figure, Page is spotted on Coney Island one summer day by a black policeman asking to take her photograph which leads to her posing in his basement and eventually to the studios of Irving Klaw (Bauer) and his sister Paula (Taylor) who cater their kitschy but considered pornographic stills to a unique clientele: fetish types.<br /><br />Although Page is rather naïve she is undeniably smart and knows that her body is not a sin and can see the forest for the trees in the sense that she is in control - or at least abides in what is offered her as work in that it is not indecent and she is having fun in her increasingly less-clothed portraits - until a Congressional witch-hunt seeks out a few scapegoats to make pornographic images a crime. <br /><br />Talented filmmaker Mary Harron and her screen writing partner Guinevere Turner (\"An American Psycho\" and \"I Shot Andy Warhol\") streamline the biopic trappings rather neatly and maybe a bit too hasty in getting at who really was Bettie Page although they do justice in depicting an era of uptightness at its zenith. Large thanks to gifted cinematographer Mott Hopfel for his languorously gorgeous black and white images and also the sprinkled color segments that recall Douglas Sirk films of the era for its melodramatic kindlings. Part feminist treaty and part American dream fulfilled the story chugs along nicely with fine performances by its ensemble including the comical Bauer and Taylor as siblings in smut and Harris having a field day as a fellow fotog with a taste for wine and chatter. It\\'s amusing to see Strathairn in a bit of stunt casting as a senator on a campaign for righteousness after portraying news bulldog Edward R. Murrow in his last outing, \"Good Night, and Good Luck\" as an opposite the table role.<br /><br />But hats off to the truly fine talents of Mol as the uninhibited yet deeply morale and most importantly intelligent Bettie Page who lets her child-like innocence beam through her bold nudity and now-considered-tame-and-quaint-borderline- kitschy poses that caught the male fancy for decades and is still a bookmark for human sexuality in this country and perhaps the world overall. Mol is perfect and uncannily resembles her portrait\\'s subject down to her knowing-teasing smile. The real Bettie Page was reportedly not involved with the project but apparently gave her blessing and continues to live a somewhat secluded life that is alluded to in the final moments as to having found Jesus and shirking her \\'notorious\\' image once and for all. A shame since this film oddly embraces the resounding decency imbued within its subject, radiating for all to see in its naked splendor.')\n",
            "(1, 'The movie was very sweet and heartwarming! I cry almost every time I watch the movie. I would recommend this movie for every one. The movie was so inspiring to me. The actors did a great job of acting, and the movie was very well played and done. The movie was about a little girl who owned a parrot of whom she named Paulie. Paulie had gotten separated from her by her parents because they thought it would be best for her.<br /><br />After Paulie had experienced so many people he ended up in a cage by himself in a basement. Finally this Russian man who had gotten a job at a place as a janitor. Had found Paulie in the basement and Paulie began to talk to him telling him the story of his life. In the end the man helped Paulie reunite with Marie (the little girl who raised Paulie). Love overcame all the obstacles.')\n",
            "(1, 'Legendary movie producer Walt Disney brought three of the world\\'s greatest fairy tales to the screen. They remain among the most popular animated films of all time. The first was his groundbreaking classic \"Snow White and the Seven Dwarfs\" released in 1937. The last was the then-under appreciated \"Sleeping Beauty\" which made it\\'s debut in 1959. In between these two was perhaps his most satisfying adaptation of a classic fairy tale: \"Cinderella\" (1950). Of the three films, \"Cinderella\" is the one most faithful to its origins. Ironically, unlike \"Snow White\", which for better or worse, became for many the definitive version of the story. \"Cinderella\" did not follow the same path. Although it was a hit and, like \"Snow White\", was responsible for restoring the dwindling Disney fortunes, it never achieved the same audience recognition which it certainly deserved. Disney, for once, did himself proud, electing not to tamper with a classic, instead elaborating and adding substance to the tale, rather than rewriting it for the screen. The result was enchanting. <br /><br />A combination of superb animation (in beautifully soft Technicolor) and the perfect voice talents brought the story to life with a radiance that endures to this day. Ilene Woods, who was a radio performer, recorded demonstration discs of the songs as a favor to the authors of the material, Al Hoffman, Mack David, and Jerry Livingston. When Disney heard them, he knew he had found his Cinderella. And indeed he had. Woods heartfelt renditions of \"A Dream Is A Wish Your Heart Makes\", \"So This Is Love\" and \"Oh Sing Sweet Nightingale\" are perfect. Eleanor Audley, who would go on to voice Maleficent in \"Sleeping Beauty\", masterfully captured the icy cruelty of the stepmother, while Rhoda Williams and Lucille Bliss were convincingly nasty stepsisters. Luis Van Rooten admirably performed as both the King and the Grand Duke, and James Macdonald was endearing as both Jaq and Gus, Cinderella\\'s devoted mice. William Phipps has little dialog as the prince (future talk show host Mike Douglas provided his singing voice) but film (and Disney) veteran, Verna Felton was born to play the fairy godmother, and she made the best number, (the Oscar-nominated \"Bibbidi-Bobbidi-Boo\") her own show-stopper. <br /><br />Among the artists responsible for the \"look\" of the film, was Mary Blair, whose inspired use of color was greatly admired by Disney. Her elegant French-period backgrounds add tremendously to the quality of the movie. But, most important of all\\' are the believable characters--from Cinderella, right down to Lucifer, the stepmother\\'s deliciously evil cat. They bring both life and vibrancy to the often told story, something very difficult to create in an animated film.<br /><br />In conjunction with the film\\'s 55-year anniversary, (and, not so coincidentally, the coming holiday season) \"Cinderella\" has just been released on a special edition DVD. It simply has never looked better. The fully restored film must be seen to be appreciated--suffice it to say, it looks wonderful. An enhanced stereo soundtrack has been added, and serves the music well. The DVD extras, now a standard part of Disney Platinum Editions, are too numerous to list here, but as usual, some are directed towards children, some are slanted to adults, and the rest fall somewhere in between. But real fans will want to get the Deluxe Gift Set, because, along with an actual cell from the film and eight character sketches, it includes a 160-page hardback book, which not only incorporates most of the material found in the book with the 1995 special edition home video release, but much more as well. As usual for Disney, \"Cinderella\" will only be available for a limited time. So, if like me, you are a \"Cinderella\" lover, get it NOW! This edition is truly a \"Dream Come True.\"')\n",
            "(1, 'In the immediate aftermath following World War II, sound minds in Hollywood tried to distance themselves from the mindless flag-waving that is a natural ingredient in a war effort. \"Best Years of Our Lives\\' and even \\'Gentleman\\'s Agreement\\' investigated the way Americans looked at themselves in the wake of the war, but Delmer Daves\\' \"Pride of the Marines\" beat them to it.<br /><br />The film is about Philadelphia smart alec John Garfield who goes to war as a marine and after a nightmarish evening in a foxhole, with Japanese soldiers eerily crying out at him and his buddies \"Mariiines, tonight you die!\", he is blinded by a hand-grenade, and dumps his girlfriend back home rather than have to depend on her after coming home.<br /><br />Delmer Daves is uncompromising in his depiction on these men who are brave, as it were, almost by coincidence. They are there, in the foxhole, and when shot at, they react. So much for heroism, but they get the job done. And then comes the self-pity, the dark, gloomy sense of humor. Garfield is in angry denial of his blindness and the film makes no excuses, \"There\\'s no free candy for anyone in this world\", as his buddy tells him. The same guy, a Jew, played by Dane Clark, reminds him, \"In a war somebody gets it, and you\\'re it. Everybody\\'s got problems! When I get back, some guys won\\'t hire me, because my name is Diamond\".<br /><br />Great movies are made with guts like these, and if the first half hour of \\'Pride of the Marines\\' fails to rise to the occasion completely, from then on it evolves into a true work of art. You weep, and you ponder, you ache and you hope against hope. Well, simply: art.<br /><br />')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code defines the mapping of numeric labels to positive and negative reviews.\n"
      ],
      "metadata": {
        "id": "FO1vFGEEKNOj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQbOSXB5KQ0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_label = {0: \" negative review\", 1: \"positive review\"}\n",
        "imdb_label[1]"
      ],
      "metadata": {
        "id": "ur7t1x_kKCgU",
        "outputId": "13181f6d-b965-49b4-9f97-1b2845ffa0c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'positive review'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code checks to ensure that there are exactly two classes in the train data set.\n"
      ],
      "metadata": {
        "id": "p_C6uyxBKQdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "num_class"
      ],
      "metadata": {
        "id": "_TalVn9bKSLe",
        "outputId": "f61f3bc1-5af9-4e97-cc09-6edcf1bb78a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K7AY0vfIKR9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "def yield_tokens(data_iter):\n",
        "    \"\"\"Yield tokens for each data sample.\"\"\"\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)"
      ],
      "metadata": {
        "id": "Y97v8Fv9KaIW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " The following code loads a pretrained word embedding model called GloVe into a variable called `glove_embedding`.\n"
      ],
      "metadata": {
        "id": "bMYjEi_rKd3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that GloVe embeddings are typically downloaded using:\n",
        "#glove_embedding = GloVe(name=\"6B\", dim=100)\n",
        "# However, the GloVe server is frequently down. The code below offers a workaround\n",
        "\n",
        "\n",
        "class GloVe_override(Vectors):\n",
        "    url = {\n",
        "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
        "    }\n",
        "\n",
        "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
        "        url = self.url[name]\n",
        "        name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
        "        #name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
        "        super(GloVe_override, self).__init__(name, url=url, **kwargs)\n",
        "\n",
        "class GloVe_override2(Vectors):\n",
        "    url = {\n",
        "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
        "    }\n",
        "\n",
        "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
        "        url = self.url[name]\n",
        "        #name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
        "        name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
        "        super(GloVe_override2, self).__init__(name, url=url, **kwargs)\n",
        "\n",
        "try:\n",
        "    glove_embedding = GloVe_override(name=\"6B\", dim=100)\n",
        "except:\n",
        "    try:\n",
        "        glove_embedding = GloVe_override2(name=\"6B\", dim=100)\n",
        "    except:\n",
        "        glove_embedding = GloVe(name=\"6B\", dim=100)"
      ],
      "metadata": {
        "id": "15g9ILB0Kbd4",
        "outputId": "49fd3a4f-25ff-4e15-9db2-487403c7e1bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove-6B.zip: 134MB [00:02, 51.9MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:25<00:00, 15813.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code builds a vocabulary object from a pretrained GloVe word embedding model and sets the default index to the <unk> token.\n",
        "\n",
        "GloVe is a word embedding technique that represents words as fixed-size dense vectors.\n",
        "\n",
        "It captures semantic relationships between words.\n",
        "\n",
        "Example:\n",
        "\n",
        " - \"king\" → [0.52, -0.63, ..., 0.10] (100D vector)\n",
        " - \"queen\" → [0.48, -0.59, ..., 0.15]\n",
        "\n",
        "Words with similar meanings have similar vectors.\n",
        "\n",
        "To use GloVe in PyTorch (torchtext), we follow these steps:\n",
        "\n",
        "🔹 Step 1: Convert Words to Indices (stoi)\n",
        "\n",
        "Each word has an index (integer ID) in the GloVe vocabulary.\n",
        "\n",
        "Step 2: Convert Indices to Word Embeddings\n",
        "\n",
        "We then use pre-trained embeddings to get the vector.\n",
        "\n",
        " Final Flow:\n",
        "\n",
        " Word → Index (stoi)\n",
        "\n",
        "\"apple\" → 123\n",
        "\n",
        "2️ Index → Embedding (vectors)\n",
        "123 → [0.45, -0.12, ..., 0.88]\n"
      ],
      "metadata": {
        "id": "2xLC0EwfKkq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import GloVe,vocab\n",
        "# Build vocab from glove_vectors\n",
        "vocab = vocab(glove_embedding .stoi, 0,specials=('<unk>', '<pad>'))\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "MdDRf6OrKlu_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=len(vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "KhrrVUTVL32b",
        "outputId": "1519b2a2-2724-4534-bb8e-6206083c3794",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400002"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab(['he'])"
      ],
      "metadata": {
        "id": "aLGa2L3QL53m",
        "outputId": "d2bec9b1-691d-4e48-b9dc-d7ac084f97cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[20]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}