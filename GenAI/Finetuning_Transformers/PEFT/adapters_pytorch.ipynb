{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u70jns9iX4s5",
    "outputId": "04009b7c-e9a3-4e92-d41b-e8e82d36b614"
   },
   "outputs": [],
   "source": [
    "!pip install torchdata==0.7.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yH0zmaUkYKkB",
    "outputId": "800dc123-98fd-408a-abfe-329f2b6c6ba0"
   },
   "outputs": [],
   "source": [
    "!pip install portalocker==2.8.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEvjeYgh__0e",
    "outputId": "d0c9555d-7187-43f2-9677-4a258660cb55"
   },
   "outputs": [],
   "source": [
    "!pip install torchtext==0.17.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loaVT5fT_RAG",
    "outputId": "3fae6e1d-0687-4c98-cfb2-d8946c774245"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import accumulate\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe, Vectors\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import io\n",
    "\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXUnnAdcIW3n"
   },
   "outputs": [],
   "source": [
    "def plot(COST,ACC):\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoch', color=color)\n",
    "    ax1.set_ylabel('total loss', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)  # you already handled the x-label with ax1\n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def save_list_to_file(lst, filename):\n",
    "    \"\"\"\n",
    "    Save a list to a file using pickle serialization.\n",
    "\n",
    "    Parameters:\n",
    "        lst (list): The list to be saved.\n",
    "        filename (str): The name of the file to save the list to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(lst, file)\n",
    "\n",
    "def load_list_from_file(filename):\n",
    "    \"\"\"\n",
    "    Load a list from a file using pickle deserialization.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The name of the file to load the list from.\n",
    "\n",
    "    Returns:\n",
    "        list: The loaded list.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3U98URt-Jiq5"
   },
   "source": [
    "# Positional encodings\n",
    "\n",
    "Positional encodings play a pivotal role in transformers and various sequence-to-sequence models, aiding in conveying critical information regarding the positions or sequencing of elements within a given sequence. To illustrate, let's examine the sentences: \"He painted the car red\" and \"He painted the red car.\" Despite their distinct meanings, it's worth noting that the embeddings for these sentences remain identical in the absence of positional encodings. The following class defines positional encodings by inheriting from PyTorch's `Module` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU4sCEtZJjpN"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(vocab_size, d_model)\n",
    "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5lHbqubJnmH"
   },
   "source": [
    "# Import IMDB data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CF_vQn2WJon-"
   },
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/35t-FeC-2uN1ozOwPs7wFg.gz')\n",
    "tar = tarfile.open(fileobj=io.BytesIO(urlopened.read()))\n",
    "tempdir = tempfile.TemporaryDirectory()\n",
    "tar.extractall(tempdir.name)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oougga7Jw35"
   },
   "source": [
    "## IMDB data set overview\n",
    "\n",
    "The **IMDB data set** contains movie reviews from the Internet Movie Database (IMDB) and is commonly used for binary sentiment classification tasks. It's a popular data set for training and testing models in natural language processing (NLP), particularly in the context of sentiment analysis.\n",
    "\n",
    "### Data set composition\n",
    "\n",
    "- **Reviews**: The data set consists of 50,000 movie reviews, divided evenly into 25,000 training and 25,000 testing samples.\n",
    "- **Sentiment labels**: Each review is labeled as either positive or negative, indicating the sentiment expressed in the review. The data set is balanced, with an equal number of positive and negative reviews in both the training and testing sets.\n",
    "- **Text content**: Reviews are presented as plain text and have been preprocessed to some extent. For example, HTML tags are removed, but the text retains its original punctuation and capitalization.\n",
    "- **Usage**: The data set is commonly used to train models for binary sentiment classification, where the goal is to predict whether a given review is positive or negative based on its text content.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Sentiment analysis**: The primary application of the IMDB data set is in sentiment analysis, where it serves as a benchmark for various text classification algorithms.\n",
    "- **Natural language processing**: The data set is widely used in NLP research and applications, providing a basis for testing the effectiveness of different models and approaches in understanding human language.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "The data set is small, so it's hard to train a model from scratch.\n",
    "\n",
    "The following class is defined to traverse the IMDB data set. The need to define this class arises from the fact that the IMDB data set is split across a large number of files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6tk1iJbJtit"
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True):\n",
    "        \"\"\"\n",
    "        root_dir: The base directory of the IMDB dataset.\n",
    "        train: A boolean flag indicating whether to use training or test data.\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root_dir, \"train\" if train else \"test\")\n",
    "        self.neg_files = [os.path.join(self.root_dir, \"neg\", f) for f in os.listdir(os.path.join(self.root_dir, \"neg\")) if f.endswith('.txt')]\n",
    "        self.pos_files = [os.path.join(self.root_dir, \"pos\", f) for f in os.listdir(os.path.join(self.root_dir, \"pos\")) if f.endswith('.txt')]\n",
    "        self.files = self.neg_files + self.pos_files\n",
    "        self.labels = [0] * len(self.neg_files) + [1] * len(self.pos_files)\n",
    "        self.pos_inx=len(self.pos_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return label, content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuA95U95KBnR"
   },
   "source": [
    "The following code uses the `IMDBDataset` class previously defined to create iterators for the train and test data sets. In the latter part of the cell, you can return 20 examples from the train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DqXtrPHKDv6",
    "outputId": "ff4b6135-814d-41cf-e182-3f94e43c6afb"
   },
   "outputs": [],
   "source": [
    "root_dir = tempdir.name + '/' + 'imdb_dataset'\n",
    "train_iter = IMDBDataset(root_dir=root_dir, train=True)  # For training data\n",
    "test_iter = IMDBDataset(root_dir=root_dir, train=False)  # For test data\n",
    "\n",
    "start=train_iter.pos_inx\n",
    "for i in range(-10,10):\n",
    "    print(train_iter[start+i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FO1vFGEEKNOj"
   },
   "source": [
    "The following code defines the mapping of numeric labels to positive and negative reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQbOSXB5KQ0e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ur7t1x_kKCgU",
    "outputId": "b6eb73ec-68c4-4a99-f6bb-20fad3e9d236"
   },
   "outputs": [],
   "source": [
    "imdb_label = {0: \" negative review\", 1: \"positive review\"}\n",
    "imdb_label[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_C6uyxBKQdh"
   },
   "source": [
    "The following code checks to ensure that there are exactly two classes in the train data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TalVn9bKSLe",
    "outputId": "50cd7143-afa9-4e6b-d066-d89641567eb2"
   },
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7AY0vfIKR9K"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y97v8Fv9KaIW"
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "def yield_tokens(data_iter):\n",
    "    \"\"\"Yield tokens for each data sample.\"\"\"\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMYjEi_rKd3Z"
   },
   "source": [
    " The following code loads a pretrained word embedding model called GloVe into a variable called `glove_embedding`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15g9ILB0Kbd4"
   },
   "outputs": [],
   "source": [
    "# Note that GloVe embeddings are typically downloaded using:\n",
    "#glove_embedding = GloVe(name=\"6B\", dim=100)\n",
    "# However, the GloVe server is frequently down. The code below offers a workaround\n",
    "\n",
    "\n",
    "class GloVe_override(Vectors):\n",
    "    url = {\n",
    "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
    "        url = self.url[name]\n",
    "        name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
    "        #name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
    "        super(GloVe_override, self).__init__(name, url=url, **kwargs)\n",
    "\n",
    "class GloVe_override2(Vectors):\n",
    "    url = {\n",
    "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
    "        url = self.url[name]\n",
    "        #name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
    "        name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
    "        super(GloVe_override2, self).__init__(name, url=url, **kwargs)\n",
    "\n",
    "try:\n",
    "    glove_embedding = GloVe_override(name=\"6B\", dim=100)\n",
    "except:\n",
    "    try:\n",
    "        glove_embedding = GloVe_override2(name=\"6B\", dim=100)\n",
    "    except:\n",
    "        glove_embedding = GloVe(name=\"6B\", dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xLC0EwfKkq-"
   },
   "source": [
    "The following code builds a vocabulary object from a pretrained GloVe word embedding model and sets the default index to the <unk> token.\n",
    "\n",
    "GloVe is a word embedding technique that represents words as fixed-size dense vectors.\n",
    "\n",
    "It captures semantic relationships between words.\n",
    "\n",
    "Example:\n",
    "\n",
    " - \"king\" ‚Üí [0.52, -0.63, ..., 0.10] (100D vector)\n",
    " - \"queen\" ‚Üí [0.48, -0.59, ..., 0.15]\n",
    "\n",
    "Words with similar meanings have similar vectors.\n",
    "\n",
    "To use GloVe in PyTorch (torchtext), we follow these steps:\n",
    "\n",
    "üîπ Step 1: Convert Words to Indices (stoi)\n",
    "\n",
    "Each word has an index (integer ID) in the GloVe vocabulary.\n",
    "\n",
    "Step 2: Convert Indices to Word Embeddings\n",
    "\n",
    "We then use pre-trained embeddings to get the vector.\n",
    "\n",
    " Final Flow:\n",
    "\n",
    " Word ‚Üí Index (stoi)\n",
    "\n",
    "\"apple\" ‚Üí 123\n",
    "\n",
    "2Ô∏è Index ‚Üí Embedding (vectors)\n",
    "123 ‚Üí [0.45, -0.12, ..., 0.88]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdDRf6OrKlu_"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe,vocab\n",
    "# Build vocab from glove_vectors\n",
    "vocab = vocab(glove_embedding .stoi, 0,specials=('<unk>', '<pad>'))\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhrrVUTVL32b",
    "outputId": "f7ab7fb3-ab9e-4624-c429-f37f79e40de5"
   },
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aLGa2L3QL53m",
    "outputId": "45cc3455-e5fa-4c09-a1cc-7595a47bf6e7"
   },
   "outputs": [],
   "source": [
    "vocab(['he'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3K1lo-WVAhE"
   },
   "source": [
    "### Data set splits\n",
    "\n",
    "The following converts the data set into map-style data sets and then performs a random split to create separate training and validation data sets. The training data set will contain 95% of the samples in the original training set, while the validation data set will contain the remaining 5%. These data sets can be used for training and evaluating a machine learning model for text classification on the IMDB data set. The final performance of the model will be evaluated on the hold-out test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knsCECwLU54d"
   },
   "outputs": [],
   "source": [
    "# Convert the training and testing iterators to map-style datasets.\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Determine the number of samples to be used for training and validation (5% for validation).\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "# Randomly split the training dataset into training and validation datasets using `random_split`.\n",
    "# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oPg6GwsV62k"
   },
   "outputs": [],
   "source": [
    "num_train = int(len(train_dataset) * 0.05)\n",
    "split_train_, _ = random_split(split_train_, [num_train, len(split_train_) - num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmtREAi1V-Nl",
    "outputId": "9e5decfb-c022-4cb7-d3a4-938b48c7040b"
   },
   "outputs": [],
   "source": [
    "num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rji7n1ZQWUWy",
    "outputId": "6d679deb-2481-471b-eb73-28f28ccb038e"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNvvAeKRWXH8"
   },
   "source": [
    "### Data loader\n",
    "\n",
    "The following code prepares the text processing pipeline with the tokenizer and vocabulary. The text pipeline is used to process the raw data strings from the data set iterators.\n",
    "\n",
    "The function **```text_pipeline```** first tokenizes the input text, then **```vocab```** is applied to get the token indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hieCexWzWU79"
   },
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    return vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvgmj106WnRr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "su3LSbSwWotU"
   },
   "source": [
    "In PyTorch, the **`collate_fn`** function is used in conjunction with data loaders to customize the way batches are created from individual samples. The provided code defines a `collate_batch` function in PyTorch, which is used with data loaders to customize batch creation from individual samples. It processes a batch of data, including labels and text sequences. It applies the `text_pipeline` function to preprocess the text. The processed data is then converted into PyTorch tensors and returned as a tuple containing the label tensor, text tensor, and offsets tensor representing the starting positions of each text sequence in the combined tensor. The function also ensures that the returned tensors are moved to the specified device (for example, GPU) for efficient computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nViMqvOWpqC"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "\n",
    "        label_list.append(_label)\n",
    "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62-xDoNGWvJZ"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pGHAQE0WyfI",
    "outputId": "812b4068-ddf3-439a-c7ef-2acbdeb4d025"
   },
   "outputs": [],
   "source": [
    "label,seqence=next(iter(valid_dataloader))\n",
    "label,seqence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtjEd9fRW1et"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Text classifier based on a pytorch TransformerEncoder.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "\n",
    "        self,\n",
    "        num_class,vocab_size,\n",
    "        freeze=True,\n",
    "        nhead=2,\n",
    "        dim_feedforward=128,\n",
    "        num_layers=2,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        classifier_dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        #self.emb = embedding=nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=freeze)\n",
    "        self.emb = nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=freeze)\n",
    "        embedding_dim = self.emb.embedding_dim\n",
    "\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=embedding_dim,\n",
    "            dropout=dropout,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.classifier = nn.Linear(embedding_dim, num_class)\n",
    "        self.d_model = embedding_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xicruS6DXFSF",
    "outputId": "abb80458-b8bf-4e8e-ab2b-6b3cee60a1c1"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net(num_class=2,vocab_size=vocab_size).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQgLC7NaXInZ"
   },
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline, model):\n",
    "    with torch.no_grad():\n",
    "        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)\n",
    "        model.to(device)\n",
    "        output = model(text)\n",
    "        return imdb_label[output.argmax(1).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ScuS09EMXPdK",
    "outputId": "54e4c172-55b5-4ab0-8af8-62f8a71c0d5a"
   },
   "outputs": [],
   "source": [
    "predict(\"I like sports and stuff\", text_pipeline, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An-HI4YaXQWF"
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model_eval):\n",
    "    model_eval.eval()\n",
    "    total_acc, total_count= 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text in tqdm(dataloader):\n",
    "            label, text = label.to(device), text.to(device)\n",
    "            output = model_eval(text)\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            total_acc += (predicted == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0S-AMHILXR3-"
   },
   "outputs": [],
   "source": [
    "def evaluate_no_tqdm(dataloader, model_eval):\n",
    "    model_eval.eval()\n",
    "    total_acc, total_count= 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text in dataloader:\n",
    "            label, text = label.to(device), text.to(device)\n",
    "            output = model_eval(text)\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            total_acc += (predicted == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUcknXyQXUDO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9vznzCNXWPf"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrMoYOBiXYL6"
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_dataloader, valid_dataloader,  epochs=1000, save_dir=\"\", file_name=None):\n",
    "    cum_loss_list = []\n",
    "    acc_epoch = []\n",
    "    acc_old = 0\n",
    "    model_path = os.path.join(save_dir, file_name)\n",
    "    acc_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + \"_acc\")\n",
    "    loss_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + \"_loss\")\n",
    "    time_start = time.time()\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        model.train()\n",
    "        #print(model)\n",
    "        #for parm in model.parameters():\n",
    "        #    print(parm.requires_grad)\n",
    "\n",
    "        cum_loss = 0\n",
    "        for idx, (label, text) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            label, text = label.to(device), text.to(device)\n",
    "\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            loss.backward()\n",
    "            #print(loss)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            cum_loss += loss.item()\n",
    "        print(f\"Epoch {epoch}/{epochs} - Loss: {cum_loss}\")\n",
    "\n",
    "        cum_loss_list.append(cum_loss)\n",
    "        accu_val = evaluate_no_tqdm(valid_dataloader,model)\n",
    "        acc_epoch.append(accu_val)\n",
    "\n",
    "        if model_path and accu_val > acc_old:\n",
    "            print(accu_val)\n",
    "            acc_old = accu_val\n",
    "            if save_dir is not None:\n",
    "                pass\n",
    "                #print(\"save model epoch\",epoch)\n",
    "                #torch.save(model.state_dict(), model_path)\n",
    "                #save_list_to_file(lst=acc_epoch, filename=acc_dir)\n",
    "                #save_list_to_file(lst=cum_loss_list, filename=loss_dir)\n",
    "\n",
    "    time_end = time.time()\n",
    "    print(f\"Training time: {time_end - time_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "FJ23lNZJXZpM",
    "outputId": "702d9cc4-6a80-4336-c814-eb490349b15c"
   },
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/sybqacL5p1qeEO8d4xRZNg/model-IMDB%20dataset%20small2-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eOt6woGoaOB565T0RLH5WA/model-IMDB%20dataset%20small2-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtrghZ5NXj-5",
    "outputId": "d8b87108-4ddd-475f-f877-9d320e269230"
   },
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/q66IH6a7lglkZ4haM6hB1w/model-IMDB%20dataset%20small2.pth')\n",
    "model_ = Net(vocab_size=vocab_size, num_class=2).to(device)\n",
    "model_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "#evaluate(test_dataloader, model_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EekxQIQDXoK2"
   },
   "source": [
    "### Fine-tune a model pretrained on the AG News data set\n",
    "\n",
    "Rather than training a model on the IMDB data set as you did earlier, you can fine-tune a model that has been pretrained on the AG News data set, which is a collection of news articles. The goal of the AG News data set is to categorize news articles into one of four categories: Sports, Business, Sci/tech, or World. You‚Äôll start training a model from scratch on the AG News data set. To save time, you can do this in just one cell. Also, for efficiency, ** comment out the training bit**. If you want to train the model for 2 epochs on a smaller data set to demonstrate what the training process would look like, uncomment the part that says `### Uncomment to Train ###` before running the cell. Training for 2 epochs on the reduced data set can take approximately 3 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "-CwIDgGXXmlY",
    "outputId": "c46d2797-15ec-4136-d289-c2af7c0ba8ca"
   },
   "outputs": [],
   "source": [
    "train_iter_ag_news = AG_NEWS(split=\"train\")\n",
    "\n",
    "num_class_ag_news = len(set([label for (label, text) in train_iter_ag_news ]))\n",
    "num_class_ag_news\n",
    "\n",
    "# Split the dataset into training and testing iterators.\n",
    "train_iter_ag_news, test_iter_ag_news = AG_NEWS()\n",
    "\n",
    "# Convert the training and testing iterators to map-style datasets.\n",
    "train_dataset_ag_news = to_map_style_dataset(train_iter_ag_news)\n",
    "test_dataset_ag_news = to_map_style_dataset(test_iter_ag_news)\n",
    "\n",
    "# Determine the number of samples to be used for training and validation (5% for validation).\n",
    "num_train_ag_news = int(len(train_dataset_ag_news) * 0.95)\n",
    "\n",
    "# Randomly split the training dataset into training and validation datasets using `random_split`.\n",
    "# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.\n",
    "split_train_ag_news_, split_valid_ag_news_ = random_split(train_dataset_ag_news, [num_train_ag_news, len(train_dataset_ag_news) - num_train_ag_news])\n",
    "\n",
    "# Make the training set smaller to allow it to run fast as an example.\n",
    "# IF YOU WANT TO TRAIN ON THE AG_NEWS DATASET, COMMENT OUT THE 2 LINEs BELOW.\n",
    "# HOWEVER, NOTE THAT TRAINING WILL TAKE A LONG TIME\n",
    "num_train_ag_news = int(len(train_dataset_ag_news) * 0.05)\n",
    "split_train_ag_news_, _ = random_split(split_train_ag_news_, [num_train_ag_news, len(split_train_ag_news_) - num_train_ag_news])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "def label_pipeline(x):\n",
    "   return int(x) - 1\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch_ag_news(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n",
    "\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader_ag_news = DataLoader(\n",
    "    split_train_ag_news_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news\n",
    ")\n",
    "valid_dataloader_ag_news = DataLoader(\n",
    "    split_valid_ag_news_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news\n",
    ")\n",
    "test_dataloader_ag_news = DataLoader(\n",
    "    test_dataset_ag_news, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_ag_news\n",
    ")\n",
    "\n",
    "\n",
    "model_ag_news = Net(num_class=4,vocab_size=vocab_size).to(device)\n",
    "model_ag_news.to(device)\n",
    "\n",
    "'''\n",
    "### Uncomment to Train ###\n",
    "LR=1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_ag_news.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "save_dir = \"\"\n",
    "file_name = \"model_AG News small1.pth\"\n",
    "train_model(model=model_ag_news, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader_ag_news, valid_dataloader=valid_dataloader_ag_news,  epochs=2, save_dir=save_dir, file_name=file_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "lw1gsabcY5ph",
    "outputId": "978f69e4-393e-485f-ef41-f91c3ca571f7"
   },
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bQk8mJu3Uct3I4JEsEtRnw/model-AG%20News%20small1-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/KNQkqJWWwY_XfbFBRFhZNA/model-AG%20News%20small1-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6dN4nC_ZCSc",
    "outputId": "28b2c5c3-9dee-4122-9d81-1aa048221bf9"
   },
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth')\n",
    "model_ag_news_ = Net(vocab_size=vocab_size, num_class=4).to(device)\n",
    "model_ag_news_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "#evaluate(test_dataloader_ag_news, model_ag_news_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwQQepI1ZZY6",
    "outputId": "dafcae3c-bd46-4bcc-8683-4a8186f8bcd3"
   },
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth')\n",
    "model_fine1 = Net(vocab_size=vocab_size, num_class=4).to(device)\n",
    "model_fine1.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69QxlM3CZbeX",
    "outputId": "8928600a-a6cf-4937-dd79-bb9665ff3bb1"
   },
   "outputs": [],
   "source": [
    "model_fine1.classifier\n",
    "in_features = model_fine1.classifier.in_features\n",
    "print(\"Original final layer:\", model_fine1.classifier)\n",
    "print(\"Input dimention  final layer:\", in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uiMe1S2xZcYt",
    "outputId": "f0631668-3ab1-4199-a69e-c060d06fb2e5"
   },
   "outputs": [],
   "source": [
    "model_fine1.classifier = nn.Linear(in_features, 2)\n",
    "model_fine1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ky9rUyv1Ze14",
    "outputId": "324b13b4-1bb1-4a77-a054-4f3b5e2cb7ed"
   },
   "outputs": [],
   "source": [
    "for name, param in model_fine1.named_parameters():\n",
    "    print(f\"{name} requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "7m_tTNAQZjGT",
    "outputId": "92b26fb9-2316-466a-f224-d960f71a96b1"
   },
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/3LEJw8BRgJJFGqlLxaETxA/model-fine1-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/-CT1h97vjv0TolY82Nw29g/model-fine1-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KtqrM1GZkLs",
    "outputId": "06ebdda9-44e0-4d56-fe09-9554728439f4"
   },
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/e0WOHKh5dnrbC2lGhpsMMw/model-fine1.pth')\n",
    "model_fine1_ = Net(vocab_size=vocab_size, num_class=2).to(device)\n",
    "model_fine1_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "#evaluate(test_dataloader, model_fine1_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xW3ITdPlZo5m"
   },
   "source": [
    "### FInetune final layer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLUffXdsZlwP",
    "outputId": "ff1d7229-8fc2-4459-a998-b561aac5f4f3"
   },
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth')\n",
    "model_fine2 = Net(vocab_size=vocab_size, num_class=4).to(device)\n",
    "model_fine2.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNY2o_61ZoFs"
   },
   "outputs": [],
   "source": [
    "# Freeze all layers in the model\n",
    "for param in model_fine2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7ysBoDgZti9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMnfR-qWZzge"
   },
   "source": [
    "Replace the final layer to reflect the fact that you are solving a two-class problem. Note that the new layer will be unfrozen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAcwjng8ZylC"
   },
   "outputs": [],
   "source": [
    "dim=model_fine2.classifier.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OepnxutZ6KM",
    "outputId": "a2385f47-bb6d-4f76-b661-c164a3de8472"
   },
   "outputs": [],
   "source": [
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUKTX3dOZ5pK"
   },
   "outputs": [],
   "source": [
    "model_fine2.classifier = nn.Linear(dim, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Uq_qNODZ9DL",
    "outputId": "a07de570-2092-4423-c420-a04997ab5d27"
   },
   "outputs": [],
   "source": [
    "model_fine2.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gsj-JC3OaDR0"
   },
   "source": [
    "The following block simulates fine-tuning on the shortened training set for just 2 epochs. **For the sake of time efficiency, this code block has been commented out**. The following code should take a shorter amount of time to train than the full fine-tuning conducted previously because only the final layer is unfrozen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9RjFPEcZ_A4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "LR=1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_fine2.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "save_dir = \"\"\n",
    "file_name = \"model_fine2.pth\"\n",
    "train_model(model=model_fine2, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=2,  save_dir=save_dir ,file_name=file_name )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_Laxy6LaGrw"
   },
   "source": [
    "Once again, you will not use the model that you just fine-tuned, but instead inspect the final layer fine-tuning process of a model fine-tuned on the full IMDB training set for 100 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "R9N2yHVPaErz",
    "outputId": "fe0ab333-6114-4d15-92af-3b232e170f13"
   },
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/UdR3ApQnxSeV2mrA0CbiLg/model-fine2-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/rWGDIF-uL2dEngWcIo9teQ/model-fine2-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AInmogxOaJsy",
    "outputId": "56ab5140-b153-4b09-8331-10baff4c0120"
   },
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/B-1H6lpDg-A0zRwpB6Ek2g/model-fine2.pth')\n",
    "model_fine2_ = Net(vocab_size=vocab_size, num_class=2).to(device)\n",
    "model_fine2_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "#evaluate(test_dataloader, model_fine2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9k16v9faNnI"
   },
   "source": [
    "The previous code indicates that although fine-tuning the final layer takes a significantly smaller amount of time than fine-tuning the whole model, the performance of the model with just the last layer unfrozen is significantly worse (64% accuracy) than the fine-tuned model with all layers unfrozen (86% accuracy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgMrI_zjaTm6"
   },
   "source": [
    "# Adapters\n",
    "FeatureAdapter is a neural network module that introduces a low-dimensional bottleneck in a transformer architecture to allow fine-tuning with fewer parameters. It compresses the original high-dimensional embeddings into a lower dimension, applies a non-linear transformation, and then expands it back to the original dimension. This process is followed by a residual\n",
    "connection that adds the transformed output back to the original input to preserve information and\n",
    "promote gradient flow.\n",
    "\n",
    "## Benefits of using adapters in neural networks\n",
    "\n",
    "- **Efficient fine-tuning**: Adapters allow for targeted updates to specific parts of the model, reducing the need to retrain large sections of the network.\n",
    "\n",
    "- **Parameter efficiency**: By adding only a few parameters, adapters make it feasible to modify large models without substantial computational overhead.\n",
    "\n",
    "- **Preservation of pretrained features**: Adapters enable the modification of a model while retaining the valuable features learned during extensive pretraining.\n",
    "\n",
    "- **Modularity and flexibility**: They enhance the modularity of models, allowing easy adaptation to various tasks without altering core architecture.\n",
    "\n",
    "- **Task-specific adaptation**: Adapters can be tailored to improve performance on particular tasks, optimizing the model‚Äôs effectiveness.\n",
    "\n",
    "- **Transfer learning and domain adaptation**: They facilitate the adaptation of models to new domains, bridging gaps between different data distributions.\n",
    "\n",
    "- **Continual learning**: Adapters support the model's ability to learn new information continuously without forgetting previous knowledge.\n",
    "\n",
    "- **Reduced risk of overfitting**: With fewer trainable parameters, adapters help prevent overfitting, especially on smaller data sets.\n",
    "\n",
    "The following code shows an adapter model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiZcQX_kaLIh"
   },
   "outputs": [],
   "source": [
    "class FeatureAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        size (int): The bottleneck dimension to which the embeddings are temporarily reduced.\n",
    "        model_dim (int): The original dimension of the embeddings or features in the transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, bottleneck_size=50, model_dim=100):\n",
    "        super().__init__()\n",
    "        self.bottleneck_transform = nn.Sequential(\n",
    "            nn.Linear(model_dim, bottleneck_size),  # Down-project to a smaller dimension\n",
    "            nn.ReLU(),                             # Apply non-linearity\n",
    "            nn.Linear(bottleneck_size, model_dim)  # Up-project back to the original dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the FeatureAdapter. Applies the bottleneck transformation to the input\n",
    "        tensor and adds a skip connection.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor with shape (batch_size, seq_length, model_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying the adapter transformation and skip connection,\n",
    "                    maintaining the original input shape.\n",
    "        \"\"\"\n",
    "        transformed_features = self.bottleneck_transform(x)  # Transform features through the bottleneck\n",
    "        output_with_residual = transformed_features + x      # Add the residual connection\n",
    "        return output_with_residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3zpqK2ga4Rf"
   },
   "source": [
    "The adapted class wraps this adapter functionality around any specified linear layer, enhancing its output with the non-linearity of a ReLU activation function. This setup is particularly useful for experimenting with subtle architectural modifications in deep learning models, facilitating fine-tuning and potentially improving model performance on complex tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kn3ZnTfaammS"
   },
   "outputs": [],
   "source": [
    "class Adapted(nn.Module):\n",
    "    def __init__(self, linear,bottleneck_size=None):\n",
    "        super(Adapted, self).__init__()\n",
    "        self.linear = linear\n",
    "        model_dim = linear.out_features\n",
    "        if bottleneck_size is None:\n",
    "          bottleneck_size = model_dim//2   # Define default bottleneck size as half the model_dim\n",
    "\n",
    "        # Initialize FeatureAdapter with calculated bottleneck_size and model_dim\n",
    "        self.adaptor = FeatureAdapter(bottleneck_size=bottleneck_size, model_dim=model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First, the input x is passed through the linear layer\n",
    "        x=self.linear(x)\n",
    "        # Then it's adapted using FeatureAdapter\n",
    "        x= self.adaptor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLxr8RyBa9fC"
   },
   "source": [
    "You load the pretrained transformer model that was trained on the AG News dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JwWRWDta-Wm",
    "outputId": "199ec40e-e44f-4563-db18-b65cac14b500"
   },
   "outputs": [],
   "source": [
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth')\n",
    "model_adapters = Net(vocab_size=vocab_size, num_class=4).to(device)\n",
    "model_adapters.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCZIbRzTbAy4"
   },
   "source": [
    "\n",
    "First, freeze the parameters of a model named model_adapters to prevent them from being updated during training. Then, retrieve the number of input features for the classifier, and replace the classifier with a new linear layer that outputs to two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Nb_7JsEbBPs"
   },
   "outputs": [],
   "source": [
    "for param in model_adapters.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "dim= model_adapters.classifier.in_features\n",
    "model_adapters.classifier = nn.Linear(dim, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgUWedGObRU7"
   },
   "source": [
    "Let's explore how to apply the adapted object to a linear layer to obtain the first output. You can obtain the unadapted linear layer for the first output by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "No_q4nM_bQHK",
    "outputId": "2e7e537b-37fc-4dc7-cdac-8ef712f4099a"
   },
   "outputs": [],
   "source": [
    "my_example_layer=model_adapters.transformer_encoder.layers[0].linear1\n",
    "print(my_example_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Y6YQTTLbQDd",
    "outputId": "2474048a-b719-4480-de18-904e8112adba"
   },
   "outputs": [],
   "source": [
    "my_adapeted_layer=Adapted(my_example_layer)\n",
    "print(my_adapeted_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qvJ0ObvbZXI"
   },
   "source": [
    "You can print the adapted layer and show that the new layers have their requires_grad attribute set to True, indicating that these layers will be updated during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBqdfxuYbXag",
    "outputId": "5a5da801-d294-4d98-91ea-360453e6ed7e"
   },
   "outputs": [],
   "source": [
    "for parm in my_adapeted_layer.parameters():\n",
    "    print(parm.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6_sus9cbcb0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rThEj0GbfLY"
   },
   "source": [
    "You can set a layer in the model to the adapter layer, as shown in the following code in the commented-out line. However, because there are many layers, a more systematic approach would be to traverse the model and replace specific layers with the adapter layer. Note that you should set the bottleneck size to 24, ensuring that there are fewer parameters to train than during a full fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Sz-yUhfbfo_",
    "outputId": "f026989e-9ad1-42e7-ec45-cf9fa08dfe72"
   },
   "outputs": [],
   "source": [
    "# Find number of layers\n",
    "N_layers=len(model_adapters.transformer_encoder.layers)\n",
    "\n",
    "# Traverse model and adapt\n",
    "for n in range(N_layers):\n",
    "  encoder=model_adapters.transformer_encoder.layers[n]\n",
    "  if encoder.linear1:\n",
    "    print(\" before linear1\")\n",
    "    print(encoder.linear1)\n",
    "    model_adapters.transformer_encoder.layers[n].linear1=Adapted(encoder.linear1, bottleneck_size=24)\n",
    "    print(\" after  linear1\")\n",
    "    print(model_adapters.transformer_encoder.layers[n].linear1)\n",
    "\n",
    "  if encoder.linear2:\n",
    "    print(\" before linear2\")\n",
    "    print(model_adapters.transformer_encoder.layers[n].linear2)\n",
    "    model_adapters.transformer_encoder.layers[n].linear2=Adapted(encoder.linear2, bottleneck_size=24)\n",
    "    print(\" after linear2\")\n",
    "    print(model_adapters.transformer_encoder.layers[n].linear2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "l-Ad1Qw3bjIV",
    "outputId": "3b8652e3-a5a9-4ba5-daf3-43df9b475436"
   },
   "outputs": [],
   "source": [
    "LR=1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_adapters.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "save_dir = \"\"\n",
    "file_name = \"model_adapters.pth\"\n",
    "train_model(model=model_adapters, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=2,  save_dir=save_dir ,file_name=file_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "ZVn_LIpIbrMd",
    "outputId": "3d3f78fc-2423-4b6f-96ed-a1452e613574"
   },
   "outputs": [],
   "source": [
    "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/D49zrrMPWO_ktwQo7PSHIQ/model-adapters-acc')\n",
    "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RXWlmyaco695RiaoU7QsnA/model-adapters-loss')\n",
    "acc_epoch = pickle.load(acc_urlopened)\n",
    "cum_loss_list = pickle.load(loss_urlopened)\n",
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vn0OHe7sbu88",
    "outputId": "9668a7bc-902f-4896-abe8-905888a647b2"
   },
   "outputs": [],
   "source": [
    "model_adapters_ = Net(vocab_size=vocab_size, num_class=2).to(device)\n",
    "for n in range(N_layers):\n",
    "  encoder=model_adapters_.transformer_encoder.layers[n]\n",
    "  if encoder.linear1:\n",
    "    print(\" before linear1\")\n",
    "    print(encoder.linear1)\n",
    "    model_adapters_.transformer_encoder.layers[n].linear1=Adapted(encoder.linear1, bottleneck_size=24)\n",
    "    print(\" after  linear1\")\n",
    "    print(model_adapters_.transformer_encoder.layers[n].linear1)\n",
    "\n",
    "  if encoder.linear2:\n",
    "    print(\" before linear2\")\n",
    "    print(model_adapters_.transformer_encoder.layers[n].linear2)\n",
    "    model_adapters_.transformer_encoder.layers[n].linear2=Adapted(encoder.linear2, bottleneck_size=24)\n",
    "    print(\" after linear2\")\n",
    "    print(model_adapters_.transformer_encoder.layers[n].linear2)\n",
    "\n",
    "model_adapters_.to(device)\n",
    "for param in model_adapters_.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/PGhd5G_NVrWNH-_jdjwNlw/model-adapters.pth')\n",
    "model_adapters_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
    "evaluate(test_dataloader, model_adapters_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psH2fPscisre"
   },
   "source": [
    "### Adapter Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lp03ppB9jJ8g",
    "outputId": "06165011-8164-491b-daba-ba477e07a78a"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UxgtcZNbj-Y_",
    "outputId": "d054ff0b-41d6-4d6d-92b3-f41a416e6c37"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade torch torchvision torchaudio transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ESnOLyJZZbE",
    "outputId": "c5d5de81-0a95-4c32-86f6-20cd4b40217f"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "Gw66HMZHbwX3",
    "outputId": "ba4409c8-588a-4411-cf50-7d58c5a30976"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertModel, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Convert dataset into PyTorch format\n",
    "def format_dataset(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(batch[\"input_ids\"]),\n",
    "        \"attention_mask\": torch.tensor(batch[\"attention_mask\"]),\n",
    "        \"labels\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(format_dataset, remove_columns=[\"text\"])\n",
    "dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=16, shuffle=True)\n",
    "\n",
    "# Define Adapter Module\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, hidden_size, adapter_size=64):\n",
    "        super().__init__()\n",
    "        self.down_proj = nn.Linear(hidden_size, adapter_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.up_proj = nn.Linear(adapter_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up_proj(self.activation(self.down_proj(x))) + x  # Residual connection\n",
    "\n",
    "# Load Pretrained BERT Model\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Freeze BERT parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Insert Adapter Modules\n",
    "class BertWithAdapters(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.bert = base_model\n",
    "        self.adapter = Adapter(hidden_size=768)\n",
    "        self.classifier = nn.Linear(768, 4)  # AG NEWS has 4 classes\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        adapted_output = self.adapter(hidden_states)\n",
    "        logits = self.classifier(adapted_output[:, 0, :])  # Using CLS token\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "# Initialize Model\n",
    "model = BertWithAdapters(bert)\n",
    "\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    report_to=\"none\"  # Disable WandB and logging\n",
    ")\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))  # Only 1000 samples\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nofVpQ7L64iy",
    "outputId": "bcfc43de-7250-4dc9-a323-81b6db34e790"
   },
   "outputs": [],
   "source": [
    "# Define the directory where you want to save the model\n",
    "save_directory = \"/content/sample_data/\"\n",
    "\n",
    "# Save the model\n",
    "model.bert.save_pretrained(save_directory)\n",
    "\n",
    "# Save the classifier and adapter separately\n",
    "torch.save(model.classifier.state_dict(), os.path.join(save_directory, \"classifier.pth\"))\n",
    "torch.save(model.adapter.state_dict(), os.path.join(save_directory, \"adapter.pth\"))\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPmNibFk7E06",
    "outputId": "9d47d106-33af-4a3b-bcd8-af41bfd7f6be"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load the pretrained BERT model\n",
    "loaded_bert = BertModel.from_pretrained(save_directory)\n",
    "\n",
    "# Load the classifier and adapter\n",
    "loaded_model = BertWithAdapters(loaded_bert)\n",
    "loaded_model.classifier.load_state_dict(torch.load(os.path.join(save_directory, \"classifier.pth\")))\n",
    "loaded_model.adapter.load_state_dict(torch.load(os.path.join(save_directory, \"adapter.pth\")))\n",
    "\n",
    "# Load tokenizer\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkqP-hEK7w6x",
    "outputId": "4c4f70e6-8d54-4246-edc7-bc47240c8c77"
   },
   "outputs": [],
   "source": [
    "pip install evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFHL4EzuAkXL"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iU3398NAkzO"
   },
   "outputs": [],
   "source": [
    "saved_directory = \"/content/sample_data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KV6RfwRx7LRN",
    "outputId": "816126b5-b7e6-4bfd-8f7e-b0b137035c00"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load saved model and tokenizer\n",
    "\n",
    "loaded_bert = BertModel.from_pretrained(saved_directory)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(saved_directory)\n",
    "\n",
    "# Load classifier and adapter\n",
    "loaded_model = BertWithAdapters(loaded_bert)\n",
    "loaded_model.classifier.load_state_dict(torch.load(f\"{saved_directory}/classifier.pth\"))\n",
    "loaded_model.adapter.load_state_dict(torch.load(f\"{saved_directory}/adapter.pth\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "CMqxClnOBH28",
    "outputId": "66b1629e-714b-4932-bcd0-ac90ae5ac253"
   },
   "outputs": [],
   "source": [
    "# Select only 500 samples for faster evaluation\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500))\n",
    "\n",
    "# Run evaluation only on this subset\n",
    "trainer.eval_dataset = small_test_dataset\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kYQU2BlKhHw",
    "outputId": "39433b0b-7b3a-4906-a4a7-f07cec23d231"
   },
   "outputs": [],
   "source": [
    "print(\"Evaluation Results:\", eval_results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
