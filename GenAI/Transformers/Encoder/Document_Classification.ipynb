{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EiT9dYHPuNGs",
    "outputId": "4d811fbc-0ee8-440b-ede5-c7df264f736e"
   },
   "outputs": [],
   "source": [
    "!pip install dash-core-components==2.0.0\n",
    "!pip install dash-table==5.0.0\n",
    "!pip install dash==2.9.3\n",
    "!pip install -Uqq dash-html-components==2.0.0\n",
    "!pip install -Uqq portalocker>=2.0.0\n",
    "!pip install -qq torchtext\n",
    "!pip install -qq torchdata\n",
    "!pip install -Uqq plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkMTzLwbCU7t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1XE3f9pue6h",
    "outputId": "c7ff01ed-11e2-424f-cc24-e77804ed91b9"
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.3.0 torchtext==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8JmZb3nCW5H",
    "outputId": "002bf880-9033-4f83-e2b8-1f8661b338ff"
   },
   "outputs": [],
   "source": [
    "!pip install -qq torchdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_60WI2tCZxL",
    "outputId": "a8d20894-f0d2-4076-aa9c-c2499749a182"
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.0.1+cpu torchvision==0.15.2+cpu torchtext==0.15.2+cpu --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFPgzXlat7UD",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import accumulate\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from IPython.display import Markdown as md\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objs as go\n",
    "import pickle\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HeDBN7Y-w2bo"
   },
   "outputs": [],
   "source": [
    "def plot(COST,ACC):\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoch', color=color)\n",
    "    ax1.set_ylabel('total loss', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)  # you already handled the x-label with ax1\n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WYBhRKSw3NJ"
   },
   "outputs": [],
   "source": [
    "def plot_embdings(my_embdings,name,vocab):\n",
    "\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "  # Plot the data points\n",
    "  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n",
    "\n",
    "  # Label the points\n",
    "  for j, label in enumerate(name):\n",
    "      i=vocab.get_stoi()[label]\n",
    "      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n",
    "\n",
    "  # Set axis labels\n",
    "  ax.set_xlabel('X Label')\n",
    "  ax.set_ylabel('Y Label')\n",
    "  ax.set_zlabel('Z Label')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64xGAuCfw7XJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_tras(words, model):\n",
    "    # Tokenize the input words using a tokenizer function\n",
    "    tokens = tokenizer(words)\n",
    "\n",
    "    # Define the model's embedding dimension (d_model)\n",
    "    d_model = 100\n",
    "\n",
    "    # Convert the input words to a PyTorch tensor and move it to the specified device\n",
    "    x = torch.tensor(text_pipeline(words)).unsqueeze(0).to(device)\n",
    "\n",
    "    # Apply the model's embedding layer and scale the embeddings by sqrt(d_model)\n",
    "    x_ = model.emb(x) * math.sqrt(d_model)\n",
    "\n",
    "    # Apply the model's positional encoder to the embeddings\n",
    "    x = model.pos_encoder(x_)\n",
    "\n",
    "    # Extract projection weights for query, key, and value from the model's state_dict\n",
    "    q_proj_weight = model.state_dict()['transformer_encoder.layers.0.self_attn.in_proj_weight'][0:embed_dim].t()\n",
    "    k_proj_weight = model.state_dict()['transformer_encoder.layers.0.self_attn.in_proj_weight'][embed_dim:2*embed_dim].t()\n",
    "    v_proj_weight = model.state_dict()['transformer_encoder.layers.0.self_attn.in_proj_weight'][2*embed_dim:3*embed_dim].t()\n",
    "\n",
    "    # Calculate query (Q), key (K), and value (V) matrices\n",
    "    Q = (x @ q_proj_weight).squeeze(0)\n",
    "    K = (x @ k_proj_weight).squeeze(0)\n",
    "    V = (x @ v_proj_weight).squeeze(0)\n",
    "\n",
    "    # Calculate attention scores using dot-product attention\n",
    "    scores = Q @ K.T\n",
    "\n",
    "    # Set row and column labels for the attention matrix\n",
    "    row_labels = tokens\n",
    "    col_labels = row_labels\n",
    "\n",
    "    # Create a heatmap of the attention scores\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(scores.cpu().detach().numpy())\n",
    "    plt.yticks(range(len(row_labels)), row_labels)\n",
    "    plt.xticks(range(len(col_labels)), col_labels, rotation=90)\n",
    "    plt.title(\"Dot-Product Attention\")\n",
    "    plt.show()\n",
    "\n",
    "    # Apply softmax to the attention scores and create a heatmap\n",
    "    att = nn.Softmax(dim=1)(scores)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(att.cpu().detach().numpy())\n",
    "    plt.yticks(range(len(row_labels)), row_labels)\n",
    "    plt.xticks(range(len(col_labels)), col_labels, rotation=90)\n",
    "    plt.title(\"Scaled Dot-Product Attention\")\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the attention head by multiplying softmax scores with values (V)\n",
    "    head = nn.Softmax(dim=1)(scores) @ V\n",
    "\n",
    "    # Visualize the embeddings and attention heads using t-SNE\n",
    "    tsne(x_, tokens, title=\"Embeddings\")\n",
    "    tsne(head, tokens, title=\"Attention Heads\")\n",
    "\n",
    "\n",
    "def tsne(embeddings, tokens, title=\"Embeddings\"):\n",
    "    # Initialize t-SNE with 2 components and a fixed random state\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "    # Fit t-SNE to the embeddings (converting from GPU if necessary)\n",
    "    tsne_result = tsne.fit_transform(embeddings.squeeze(0).cpu().detach().numpy())\n",
    "\n",
    "    # Create a scatter plot of the t-SNE results\n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1])\n",
    "\n",
    "    # Set a title for the plot\n",
    "    plt.title(title)\n",
    "\n",
    "    # Add labels for each point in the scatter plot\n",
    "    for j, label in enumerate(tokens):\n",
    "        # Place the label text at the corresponding t-SNE coordinates\n",
    "        plt.text(tsne_result[j, 0], tsne_result[j, 1], label)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_MUtQiPy-cm"
   },
   "outputs": [],
   "source": [
    "def save_list_to_file(lst, filename):\n",
    "    \"\"\"\n",
    "    Save a list to a file using pickle serialization.\n",
    "\n",
    "    Parameters:\n",
    "        lst (list): The list to be saved.\n",
    "        filename (str): The name of the file to save the list to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(lst, file)\n",
    "\n",
    "def load_list_from_file(filename):\n",
    "    \"\"\"\n",
    "    Load a list from a file using pickle deserialization.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The name of the file to load the list from.\n",
    "\n",
    "    Returns:\n",
    "        list: The loaded list.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q2CyaITzDd-"
   },
   "source": [
    "## Toy dataset\n",
    "These are essentially the same steps that you have done in the previous labs. However, let's have a brief explanation. The code defines a dataset, tokenizes the text data using a basic English tokenizer, creates a vocabulary from the tokenized data, and sets up a default index for handling unknown tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YEXPUA6zCoL"
   },
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \"),\n",
    "    (3,\"he painted the car red\"),\n",
    "    (1,\"he painted the red car\")\n",
    "    ]\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for  _,text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9W8unjezJzQ"
   },
   "source": [
    "Prepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to process the raw data strings from the dataset iterators.\n",
    "\n",
    "The function text_pipeline will tokenize the input text, and vocab will then be applied to get the token indices. The label_pipeline will ensure that the labels start at zero.\n",
    "\n",
    "These pipelines are defined here for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UfCyJuRzKMC"
   },
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "  return vocab(tokenizer(x))\n",
    "\n",
    "def label_pipeline(x):\n",
    "   return int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nr2BFKAczOeg",
    "outputId": "f27c3c50-bfea-4c8f-fb36-c8ccda4b77ce"
   },
   "outputs": [],
   "source": [
    "sequences = [torch.tensor([j for j in range(1,i)]) for i in range(2,10)]\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgy2RSZF_TPc"
   },
   "source": [
    "The function pad_sequence from torch.nn.utils.rnn in PyTorch is used to pad a sequence of tensors to the same length along a specified dimension. This function is commonly used when dealing with sequences of variable lengths, such as in natural language processing (NLP) tasks when working with tokenized sentences of different lengths.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "Input: pad_sequence takes a list of tensors as input, where each tensor represents a sequence. These sequences can have different lengths.\n",
    "\n",
    "Padding: The function pads the sequences with zeros (or another specified padding value) to make them all the same length. It pads sequences to match the length of the longest sequence in the list.\n",
    "\n",
    "Output: The output is a single tensor where all sequences are stacked along the specified dimension (by default, it's the first dimension). The result is a batch of sequences with the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7y8zUr3y_ZFz",
    "outputId": "727916d5-3fd1-494c-af5b-dca6cfb73a4e"
   },
   "outputs": [],
   "source": [
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bODA0ua_z7t"
   },
   "source": [
    "The TransformerEncoder is a stack of multiple TransformerEncoderLayers that process input sequences.\n",
    "\n",
    "How It Works\n",
    "Input Sequence: The encoder takes a sequence of words (converted into embeddings).\n",
    "\n",
    "Positional Encoding: Since transformers don't process words one by one,\n",
    "positional encoding is added to track word order.\n",
    "Pass Through Layers: The input sequence is passed through multiple\n",
    "\n",
    "TransformerEncoderLayers, each refining the understanding of the text.\n",
    "Final Output: The processed sequence is passed to the next part of the model (e.g., a classifier or decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLxlDcyC_dJp"
   },
   "outputs": [],
   "source": [
    "encoder_layer=nn.TransformerEncoderLayer(\n",
    "            d_model=3,\n",
    "            nhead=1,\n",
    "            dim_feedforward=1,\n",
    "            dropout=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NosRnDYlBluZ"
   },
   "source": [
    "In the context of transformers, your objective is to train the model to take an input sequence and effectively generate another sequence as its output, a fundamental task that underlies a wide range of natural language processing and sequence-to-sequence tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyXaDR8jB1P_"
   },
   "outputs": [],
   "source": [
    "train_iter= AG_NEWS(split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfb9JsIAEMyO"
   },
   "source": [
    "The AG_NEWS dataset in torchtext does not support direct indexing like a list or tuple. It is not a random access dataset but rather an iterable dataset that needs to be used with an iterator. This approach is more effective for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgMB-D7KBOww",
    "outputId": "90903d6b-97d1-4a3b-8709-975ee9af1ecf"
   },
   "outputs": [],
   "source": [
    "y,text= next(iter(train_iter ))\n",
    "print(y,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "SAIJLBbGEo6A",
    "outputId": "67104088-c222-4082-f9c2-31a4ce2259a4"
   },
   "outputs": [],
   "source": [
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "ag_news_label[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hZPuH_AExZp",
    "outputId": "ab4cc31c-68f6-42a4-e06e-61d53f8fd846"
   },
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter ]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsCmKTYBEzOn"
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n4yOnQmME6Tw",
    "outputId": "72fc98ef-10b2-4cfe-a723-b33a8f5097fd"
   },
   "outputs": [],
   "source": [
    "vocab([\"age\",\"hello\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIHGPDWYFII9"
   },
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdUf9osBE_Tf"
   },
   "source": [
    "We can convert the dataset into map-style datasets and then perform a random split to create separate training and validation datasets. The training dataset will contain 95% of the samples, while the validation dataset will contain the remaining 5%. These datasets can be used for training and evaluating a machine learning model for text classification on the AG_NEWS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CwspdfXE71j"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing iterators.\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "\n",
    "# Convert the training and testing iterators to map-style datasets.\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Determine the number of samples to be used for training and validation (5% for validation).\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "# Randomly split the training dataset into training and validation datasets using `random_split`.\n",
    "# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6EadYxg2FK7Y",
    "outputId": "dcb45a5c-221e-41ae-cfa9-154f913b4023"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz5t-8HH9MG5"
   },
   "source": [
    "### Data Loader\n",
    "\n",
    "In PyTorch, the collate_fn function is used in conjunction with data loaders to customize the way batches are created from individual samples. The provided code defines a collate_batch function in PyTorch, which is used with data loaders to customize batch creation from individual samples. It processes a batch of data, including labels and text sequences. It applies the label_pipeline and text_pipeline functions to preprocess the labels and texts, respectively. The processed data is then converted into PyTorch tensors and returned as a tuple containing the label tensor, text tensor, and offsets tensor representing the starting positions of each text sequence in the combined tensor. The function also ensures that the returned tensors are moved to the specified device (e.g., GPU) for efficient computation.\n",
    "\n",
    "#### collate_batch(batch)\n",
    "\n",
    "Converts text sequences into tensors (text_list).\n",
    "\n",
    "Pads sequences to the same length (pad_sequence).\n",
    "\n",
    "Converts labels into tensors (label_list).\n",
    "\n",
    "Moves data to the correct device (CPU/GPU).\n",
    "\n",
    "#### DataLoader\n",
    "\n",
    "Loads data from split_train_, split_valid_, and test_dataset.\n",
    "\n",
    "Uses a batch size of 64.\n",
    "\n",
    "Applies shuffling (randomizes data for better training).\n",
    "\n",
    "Uses collate_fn=collate_batch to ensure padded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDgZDdKO9Sb4"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n",
    "\n",
    "\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "\n",
    "    return label_list.to(device), text_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Fbb2v19-MW9"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVAN1HDLFxiy"
   },
   "source": [
    "DataLoader is an iterator that batches, shuffles, and loads data efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wy3GXO_3FUl9"
   },
   "outputs": [],
   "source": [
    "label,seqence=next(iter(valid_dataloader ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4cUOa5DHLRd",
    "outputId": "a07cc624-12eb-40df-e28a-5039f6d34a43"
   },
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0zRbxXgL3UW"
   },
   "source": [
    "BATCH_SIZE = 64 → The batch contains 64 samples.\n",
    "Each sample is a sequence (text) of variable length.\n",
    "collate_batch uses pad_sequence(batch_first=True) → This ensures all sequences in a batch have the same length by padding shorter ones.\n",
    "pad_sequence outputs shape as:\n",
    "\n",
    "(BATCH_SIZE,MAX_SEQ_LEN)\n",
    "\n",
    "where:\n",
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LEN is the longest sequence in the batch after padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vlo7IVARHOAu",
    "outputId": "cd77b026-fdc0-4f85-ea42-a0dd5dc93486"
   },
   "outputs": [],
   "source": [
    "seqence.shape   ## 64 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agTqWbQ_G1d1"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Text classifier based on a pytorch TransformerEncoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "\n",
    "        self,\n",
    "        vocab_size,\n",
    "        num_class,\n",
    "        embedding_dim=100,\n",
    "        nhead=5,\n",
    "        dim_feedforward=2048,\n",
    "        num_layers=6,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        classifier_dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size,embedding_dim)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=embedding_dim,\n",
    "            dropout=dropout,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "\n",
    "\n",
    "        ## this is one layer with multiple attention heads, these layers can be multiple\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        ## Transformer encoder contains encoder layer, and we specify how many we require\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.classifier = nn.Linear(embedding_dim, num_class)\n",
    "        self.d_model = embedding_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljUjidvmSdpZ"
   },
   "outputs": [],
   "source": [
    "y,x=next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2NVzGIGJSeuT",
    "outputId": "9673c47e-cd1d-4a8a-a1e5-cc65a2e55eb7"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWw6_XHmSiJu",
    "outputId": "6ab816a6-d5ea-4e1e-e02f-266faad1e87b"
   },
   "outputs": [],
   "source": [
    "x.shape ## 64 batch size and 88 vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EghLglz2SuHh",
    "outputId": "9ed926d6-f5a9-466a-e628-f3b744f7c318"
   },
   "outputs": [],
   "source": [
    "y,x=next(iter(train_dataloader))\n",
    "print(y.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13HbAb7TXN6m"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sF3D768tS6SC"
   },
   "outputs": [],
   "source": [
    "emsize=64 ## this is free parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ALuEsFuTEUN",
    "outputId": "2d890163-03fd-485c-e029-d19ebf83134b"
   },
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12kE4T1KTGzZ",
    "outputId": "2275a749-79de-41bd-b821-1b68bbd5d4fc"
   },
   "outputs": [],
   "source": [
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIYzpDm9UpOr"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(vocab_size, d_model)\n",
    "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOVzKGunTJyy",
    "outputId": "d6898e5a-096d-41cb-852e-054982ebdcd1"
   },
   "outputs": [],
   "source": [
    "model = Net(vocab_size=vocab_size,num_class=4).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilnsfy4rWxez"
   },
   "source": [
    "Summary\n",
    "\n",
    "✅ Sequence Length (seq_len) = Number of tokens in a sentence\n",
    "\n",
    "✅ Sequence Length varies for different sentences\n",
    "\n",
    "✅ Padding is used to make all sentences in a batch the same length\n",
    "\n",
    "\n",
    "Embedding Layer: So here above as we can see, we have batch size x sequence length (eg: 64 x 113). Then when we add embedding, it becomes 64 x 113 x embeddings\n",
    "\n",
    " Positional Encoding (self.pos_encoder): Adds positional encodings to embeddings to provide word order information. Uses a dropout layer (p=0.1) to prevent overfitting. Shape remains the same\n",
    "\n",
    " Transformer Encoder (self.transformer_encoder): A stack of 6 Transformer Encoder Layers (as seen in ModuleList(0-5)).\n",
    "Each layer applies self-attention, feedforward networks, and normalization.\n",
    "\n",
    "Input Shape:(batch_size,seq_len,100)\n",
    "\n",
    "Output Shape:(batch_size,seq_len,100)\n",
    "\n",
    "\n",
    "Each Transformer Encoder Layer contains:\n",
    "\n",
    " - Multi-head Self-Attention (self_attn)\n",
    "\n",
    " - Uses multiple attention heads (not explicitly shown but inferred).\n",
    "Learns relationships between words.\n",
    "\n",
    " - Projection Layer: Linear(100, 100), meaning the input and output remain the same shape.\n",
    "\n",
    " - Feedforward Network (linear1 → linear2)\n",
    "\n",
    " - Expands embeddings to 2048 dimensions (linear1).\n",
    "\n",
    " - Applies non-linearity and projects back to 100 (linear2).\n",
    "\n",
    "Shape Changes:\n",
    "\n",
    " - After linear1: (batch_size, seq_len, 2048)\n",
    "\n",
    " - After linear2: (batch_size, seq_len, 100)\n",
    "\n",
    "Layer Normalization (norm1, norm2)\n",
    "\n",
    " - Normalizes across features (100 dimensions).\n",
    "\n",
    " - Dropout Layers (dropout1, dropout2)\n",
    "\n",
    " - Applies dropout (p=0.1) for regularization.\n",
    "\n",
    "\n",
    " Mean Pooling (x.mean(dim=1))\n",
    "\n",
    "Aggregates the sequence dimension to create a single vector per input.\n",
    "Converts sequence representation into a fixed-size feature vector before classification.\n",
    "\n",
    "(batch_size,seq_len,100)→(batch_size,100)\n",
    "\n",
    "Classifier (self.classifier)\n",
    "\n",
    "Fully connected linear layer that maps the encoded representation to 4 output classes.\n",
    "\n",
    "Input Shape: (batch_size, 100)\n",
    "\n",
    "Output Shape: (batch_size, 4)\n",
    "\n",
    "Final Output: Predicted logits (can be converted to probabilities using softmax).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbH_QgASWIdD"
   },
   "outputs": [],
   "source": [
    "predicted_label=model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RtOjdcLIWLkk",
    "outputId": "ac2aa6d9-f6c7-49e9-9f32-1172a32d8156"
   },
   "outputs": [],
   "source": [
    "predicted_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXu3q_foWNeE",
    "outputId": "811fcd92-a5b5-4540-d16d-5e8cd96c10a2"
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80mOnwHIbaSG"
   },
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)\n",
    "\n",
    "        output = model(text)\n",
    "        return ag_news_label[output.argmax(1).item() + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "p3rkorKTbb3z",
    "outputId": "3179b0ca-9a6e-4744-f094-75720b0ac884"
   },
   "outputs": [],
   "source": [
    "predict(\"I like sports and stuff\",text_pipeline )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSJY2dusbdd1"
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model_eval):\n",
    "    model_eval.eval()\n",
    "    total_acc, total_count= 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model_eval(text.to(device))\n",
    "\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMVDq9DjbiQc",
    "outputId": "7c647a1b-1e4b-4c5b-d614-fa5ef18657d3"
   },
   "outputs": [],
   "source": [
    "evaluate(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGQYhzZDb1k2"
   },
   "outputs": [],
   "source": [
    "article=\"\"\"Canada navigated a stiff test against the Republic of Ireland on a rain soaked evening in Perth, coming from behind to claim a vital 2-1 victory at the Women’s World Cup.\n",
    "Katie McCabe opened the scoring with an incredible Olimpico goal – scoring straight from a corner kick – as her corner flew straight over the despairing Canada goalkeeper Kailen Sheridan at Perth Rectangular Stadium in Australia.\n",
    "Just when Ireland thought it had safely navigated itself to half time with a lead, Megan Connolly failed to get a clean connection on a clearance with the resulting contact squirming into her own net to level the score.\n",
    "Minutes into the second half, Adriana Leon completed the turnaround for the Olympic champion, slotting home from the edge of the area to seal the three points.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "znpaJgvNb3vH",
    "outputId": "864848ca-72cc-4ccb-8a26-3a97a2526c0f"
   },
   "outputs": [],
   "source": [
    "predict(article,text_pipeline )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
