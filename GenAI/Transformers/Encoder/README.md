## Bert

<img width="164" alt="image" src="https://github.com/user-attachments/assets/d8ffe8d8-a3c0-4600-b18a-002b867e52c0" />


Summary: Understanding BERT and Masked Language Modeling (MLM)

1Ô∏è‚É£ Introduction to BERT

BERT (Bidirectional Encoder Representations from Transformers) was developed by Google and has revolutionized NLP by providing deep contextual understanding of text.
It is pre-trained using self-supervised learning on large text datasets.
Unlike GPT (which is a decoder-only model), BERT is an encoder-only model.
It is not designed for text generation but excels at language understanding tasks like:

‚úÖ Text Summarization

‚úÖ Question Answering

‚úÖ Sentiment Analysis

2Ô∏è‚É£ BERT‚Äôs Encoder-Only Architecture

BERT processes entire sequences of text simultaneously rather than in an autoregressive manner like GPT.
This allows BERT to understand the full context of a sentence, making it more effective for tasks requiring deep comprehension.
Unlike GPT, which can only consider the words that came before a given token, BERT looks at both past and future words, making it bi-directional.

üîπ Example:

For the sentence:

üìù "The farmers cultivate the ___ to grow crops."

A decoder model (like GPT) would only see: "The farmers cultivate the"
BERT sees the full sentence, helping it make a better guess.

3Ô∏è‚É£ Masked Language Modeling (MLM) - How BERT Learns
BERT is trained using Masked Language Modeling (MLM), where random words in a sentence are replaced with a special [MASK] token.
The model then tries to predict the masked words using surrounding context.

üîπ Example:

Input: "IBM [MASK] me BERT."

BERT Prediction: "IBM taught me BERT."

Unlike decoder models, which only have access to past words, BERT uses both left and right context to make better predictions.

4Ô∏è‚É£ Why MLM Uses a Mix of Masking, Randomization, and Retention

If BERT only used the [MASK] token during training, it might struggle during fine-tuning since [MASK] doesn't appear in real-world tasks.
To reduce this gap, during training:

‚úÖ 80% of selected words are replaced with [MASK]

‚úÖ 10% are replaced with a random word

‚úÖ 10% remain unchanged

üîπ Example (Training Data Handling)

Given sentence:

üìù "The cat sat on the mat."

85% of words remain unchanged

For the other 15%:

‚úÖ 80% ‚Üí Replace with [MASK] ("The [MASK] sat on the mat.")

‚úÖ 10% ‚Üí Replace with random word ("The dog sat on the mat.")

‚úÖ 10% ‚Üí Keep unchanged for prediction

üìå This ensures BERT learns robust word relationships rather than just memorizing [MASK].


5Ô∏è‚É£ BERT‚Äôs Prediction Process

1Ô∏è‚É£ BERT encodes the input and generates contextual embeddings for each word.

2Ô∏è‚É£ These embeddings pass through a final layer to generate logits (numerical values representing predicted words).

3Ô∏è‚É£ The masked word is identified by selecting the word with the highest logit value.


üîπ Example:

Input: "She is a [MASK] engineer."

Prediction based on logits: "She is a software engineer."

This method allows BERT to deeply understand language structure and word relationships.

Applications of BERT

‚úÖ Text Classification (Sentiment Analysis, Spam Detection, etc.)

‚úÖ Named Entity Recognition (NER) for extracting names, places, organizations

‚úÖ Question Answering (e.g., powering search engines and chatbots)

‚úÖ Machine Reading Comprehension (Summarization, Context Extraction)

‚úÖ Semantic Similarity & Search (Used in Google Search algorithms)

üîπ Final Recap

BERT is an encoder-only model, meaning it processes entire text sequences at once.
It uses Masked Language Modeling (MLM), where words are randomly masked and predicted.
Unlike GPT, BERT is bi-directional, meaning it considers both past and future words for context.
MLM training uses a mix of masking, randomization, and unchanged words to improve performance.
BERT is best suited for tasks like classification, sentiment analysis, and question answering, but not text generation.

‚úÖ BERT revolutionized NLP by providing deep contextual understanding, making it a foundation for many modern AI applications! üöÄ



Understanding the BERT Model Structure

üîπ Key Components of BERT

‚úÖ Tokenization: Breaks input text into smaller tokens using WordPiece Tokenizer.

‚úÖ Token Embeddings: Converts words into dense numerical vectors.

‚úÖ Positional Embeddings: Adds learned position information (unlike Transformers, which use a fixed formula).

‚úÖ Multi-Head Self-Attention: 12 parallel attention heads (in BERT-base) capture different word relationships.

‚úÖ Stacked Encoders: 12 encoders in BERT-base, 24 in BERT-large.

‚úÖ Classification Head: A final linear + softmax layer for classification tasks.

üìå Key Numbers:

Embedding size: 768 for BERT-base, 1024 for BERT-large.

Maximum input length: 512 tokens.

Vocabulary size: 30,000 tokens.

3Ô∏è‚É£ Difference Between Positional Encoding & Positional Embedding

Traditional Transformers (like GPT) use fixed sinusoidal positional encoding (not trainable).

BERT uses learned positional embeddings, meaning the model learns position information during training.

üìå Why is this important?

Learned positional embeddings help adapt better to different sentence structures, making BERT more flexible.

4Ô∏è‚É£ Multi-Head Attention in BERT

BERT-base has 12 attention heads inside each encoder.

Each attention head learns different relationships between words.

Final output embeddings = concatenation of all attention heads.

üìå Example:

If the sentence is:

üìù "The cat sat on the mat."

One attention head might focus on "The ‚Üí cat" relationship.

Another head might focus on "sat ‚Üí mat".

All heads together capture deeper contextual meaning.

5Ô∏è‚É£ How BERT Uses Multiple Encoders

BERT-base has 12 encoders, meaning each input token passes through 12 layers.

Each encoder refines the representation of words before final output.

The final embedding size is 768 (BERT-base) or 1024 (BERT-large).

üìå Analogy:

Think of each encoder as a layer in a cake. The deeper you go, the richer the representation.

6Ô∏è‚É£ Classification Tasks in BERT

BERT can be used for classification tasks by adding a "Classification Head" on top.

The classification head consists of: 

‚úÖ A Linear Layer (fully connected layer).

‚úÖ A Softmax Activation Function (to compute probabilities).


Final Architecture

![Uploading image.png‚Ä¶]()



## Positional Encoding

Without positional encoding, the Transformer would treat the sentence "I like cats" the same as "cats like I", since it processes all tokens in parallel and lacks the sequential nature of RNNs. By adding positional encodings, the model can learn the relative or absolute position of tokens in a sequence, which is crucial for understanding language.
It uses sine and cosine functions to encode positions, ensuring that each position is uniquely represented.
Without positional encoding, Transformers would lose the ability to understand the sequence order.
Both fixed positional encoding and learned positional embeddings are used in practice, depending on the model architecture and task.

### Positional Encoding Explanation Using a Word Example
Let‚Äôs take a simple sentence:

"She loves cats"

The computer doesn‚Äôt know the order of the words, so we need positional encoding to tell it that:

"She" is the first word.

"loves" is the second word.

"cats" is the third word.

But instead of just saying first, second, and third, we use more complex position tags for each word. These tags are calculated using mathematical functions (sine and cosine), which give us different values for different positions.

#### Step-by-Step Example
Word Embedding: First, each word ("She", "loves", "cats") is converted into a vector (a list of numbers) using a word embedding like GloVe or Word2Vec.

Let‚Äôs say:

"She" becomes [0.5, 0.8, 0.1]

"loves" becomes [0.6, 0.9, 0.3]

"cats" becomes [0.4, 0.7, 0.2]

Positional Encoding: Now, we add positional tags to these word vectors to tell the model where each word is in the sentence.

Using sine and cosine functions, the positional encoding might give us values like:

Position tag for the 1st word ("She"): [0.0, 1.0, 0.5]

Position tag for the 2nd word ("loves"): [0.8, 0.5, 0.2]

Position tag for the 3rd word ("cats"): [0.9, 0.2, 0.1]

Adding Word Embedding + Positional Encoding: For each word, we add the word embedding and the positional encoding together:

"She": [0.5, 0.8, 0.1] + [0.0, 1.0, 0.5] = [0.5, 1.8, 0.6]

"loves": [0.6, 0.9, 0.3] + [0.8, 0.5, 0.2] = [1.4, 1.4, 0.5]

"cats": [0.4, 0.7, 0.2] + [0.9, 0.2, 0.1] = [1.3, 0.9, 0.3]

Now, each word not only has its meaning (from word embedding) but also its position in the sentence (from positional encoding).

#### Why This Helps the Model?

If we change the order of the words:

"Cats love she"

Even though the words are the same, their positional encoding will be different, so the model will know it‚Äôs a different sentence because the position tags will have changed.

For example:

"Cats" will now have the position tag for the 1st word.

"Love" will have the position tag for the 2nd word.

"She" will have the position tag for the 3rd word.

This way, the model understands both the words and their correct order, which is crucial for tasks like language translation or text summarization.

Summary (in Simple Words)

 - Word embeddings tell the model what each word means.
 - Positional encoding tells the model where each word is in the sentence.
 - Together, they help the model understand both the meaning and order of the words, which is essential for processing natural language.

## Query Key and Value


### Key Steps in the Attention Mechanism

### Input Processing:

An input sentence (e.g., "The bank of the river was flooded") is fed into the model.

Each word in the sentence is tokenized and converted into an embedding vector.

For each word (or token), three vectors are created by multiplying the embedding vector with three learned matrices:

Query vector (Q)

Key vector (K)

Value vector (V)

### Generating Attention Scores:

To determine how much attention each word should pay to other words, the model:

Takes the query vector of the current word (e.g., "bank").

Multiplies it with the key vectors of all other words in the sentence using a dot product to compute attention scores.

A high attention score indicates that a word is highly relevant in the given context (e.g., "river" and "flooded" are highly relevant to "bank" in this example).

Using Attention Scores:

The attention scores are used to weigh the value vectors of the corresponding words.

The model combines the weighted value vectors to produce a context-aware representation of the current word.

For example, in the sentence "The bank of the river was flooded," the attention mechanism helps the model understand that "bank" refers to a river bank and not a financial institution, based on its relationship with "river" and "flooded."

Understanding Query, Key, and Value Matrices

### Key Matrix (K):
Learns distinguishing features of each word, helping the model compare words and identify relevant ones.

### Query Matrix (Q):
Learns parameters that, when multiplied with key vectors, produce meaningful attention scores.

### Value Matrix (V):
Stores foundational knowledge about each word (e.g., meanings of "bank") and is refined by attention scores to produce context-aware word representations.

### Purpose of the Attention Mechanism

The attention mechanism allows the model to create a context-aware representation of each word in the sentence.

This helps the model disambiguate words with multiple meanings (like "bank") by focusing on other relevant words in the context.





The final output is a set of refined, context-aware vectors for each word, which the model uses for further tasks like translation or text generation.

