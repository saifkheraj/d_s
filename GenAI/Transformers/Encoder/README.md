## Bert

<img width="164" alt="image" src="https://github.com/user-attachments/assets/d8ffe8d8-a3c0-4600-b18a-002b867e52c0" />


Summary: Understanding BERT and Masked Language Modeling (MLM)

1Ô∏è‚É£ Introduction to BERT

BERT (Bidirectional Encoder Representations from Transformers) was developed by Google and has revolutionized NLP by providing deep contextual understanding of text.
It is pre-trained using self-supervised learning on large text datasets.
Unlike GPT (which is a decoder-only model), BERT is an encoder-only model.
It is not designed for text generation but excels at language understanding tasks like:

‚úÖ Text Summarization

‚úÖ Question Answering

‚úÖ Sentiment Analysis

2Ô∏è‚É£ BERT‚Äôs Encoder-Only Architecture

BERT processes entire sequences of text simultaneously rather than in an autoregressive manner like GPT.
This allows BERT to understand the full context of a sentence, making it more effective for tasks requiring deep comprehension.
Unlike GPT, which can only consider the words that came before a given token, BERT looks at both past and future words, making it bi-directional.

üîπ Example:

For the sentence:

üìù "The farmers cultivate the ___ to grow crops."

A decoder model (like GPT) would only see: "The farmers cultivate the"
BERT sees the full sentence, helping it make a better guess.

3Ô∏è‚É£ Masked Language Modeling (MLM) - How BERT Learns
BERT is trained using Masked Language Modeling (MLM), where random words in a sentence are replaced with a special [MASK] token.
The model then tries to predict the masked words using surrounding context.

üîπ Example:

Input: "IBM [MASK] me BERT."

BERT Prediction: "IBM taught me BERT."

Unlike decoder models, which only have access to past words, BERT uses both left and right context to make better predictions.

4Ô∏è‚É£ Why MLM Uses a Mix of Masking, Randomization, and Retention

If BERT only used the [MASK] token during training, it might struggle during fine-tuning since [MASK] doesn't appear in real-world tasks.
To reduce this gap, during training:

‚úÖ 80% of selected words are replaced with [MASK]

‚úÖ 10% are replaced with a random word

‚úÖ 10% remain unchanged

üîπ Example (Training Data Handling)

Given sentence:

üìù "The cat sat on the mat."

85% of words remain unchanged

For the other 15%:

‚úÖ 80% ‚Üí Replace with [MASK] ("The [MASK] sat on the mat.")

‚úÖ 10% ‚Üí Replace with random word ("The dog sat on the mat.")

‚úÖ 10% ‚Üí Keep unchanged for prediction

üìå This ensures BERT learns robust word relationships rather than just memorizing [MASK].


5Ô∏è‚É£ BERT‚Äôs Prediction Process

1Ô∏è‚É£ BERT encodes the input and generates contextual embeddings for each word.

2Ô∏è‚É£ These embeddings pass through a final layer to generate logits (numerical values representing predicted words).

3Ô∏è‚É£ The masked word is identified by selecting the word with the highest logit value.


üîπ Example:

Input: "She is a [MASK] engineer."

Prediction based on logits: "She is a software engineer."

This method allows BERT to deeply understand language structure and word relationships.

Applications of BERT

‚úÖ Text Classification (Sentiment Analysis, Spam Detection, etc.)

‚úÖ Named Entity Recognition (NER) for extracting names, places, organizations

‚úÖ Question Answering (e.g., powering search engines and chatbots)

‚úÖ Machine Reading Comprehension (Summarization, Context Extraction)

‚úÖ Semantic Similarity & Search (Used in Google Search algorithms)

üîπ Final Recap

BERT is an encoder-only model, meaning it processes entire text sequences at once.
It uses Masked Language Modeling (MLM), where words are randomly masked and predicted.
Unlike GPT, BERT is bi-directional, meaning it considers both past and future words for context.
MLM training uses a mix of masking, randomization, and unchanged words to improve performance.
BERT is best suited for tasks like classification, sentiment analysis, and question answering, but not text generation.

‚úÖ BERT revolutionized NLP by providing deep contextual understanding, making it a foundation for many modern AI applications! üöÄ



Understanding the BERT Model Structure

üîπ Key Components of BERT

‚úÖ Tokenization: Breaks input text into smaller tokens using WordPiece Tokenizer.

‚úÖ Token Embeddings: Converts words into dense numerical vectors.

‚úÖ Positional Embeddings: Adds learned position information (unlike Transformers, which use a fixed formula).

‚úÖ Multi-Head Self-Attention: 12 parallel attention heads (in BERT-base) capture different word relationships.

‚úÖ Stacked Encoders: 12 encoders in BERT-base, 24 in BERT-large.

‚úÖ Classification Head: A final linear + softmax layer for classification tasks.

üìå Key Numbers:

Embedding size: 768 for BERT-base, 1024 for BERT-large.

Maximum input length: 512 tokens.

Vocabulary size: 30,000 tokens.

3Ô∏è‚É£ Difference Between Positional Encoding & Positional Embedding

Traditional Transformers (like GPT) use fixed sinusoidal positional encoding (not trainable).

BERT uses learned positional embeddings, meaning the model learns position information during training.

üìå Why is this important?

Learned positional embeddings help adapt better to different sentence structures, making BERT more flexible.

4Ô∏è‚É£ Multi-Head Attention in BERT

BERT-base has 12 attention heads inside each encoder.

Each attention head learns different relationships between words.

Final output embeddings = concatenation of all attention heads.

üìå Example:

If the sentence is:

üìù "The cat sat on the mat."

One attention head might focus on "The ‚Üí cat" relationship.

Another head might focus on "sat ‚Üí mat".

All heads together capture deeper contextual meaning.

5Ô∏è‚É£ How BERT Uses Multiple Encoders

BERT-base has 12 encoders, meaning each input token passes through 12 layers.

Each encoder refines the representation of words before final output.

The final embedding size is 768 (BERT-base) or 1024 (BERT-large).

üìå Analogy:

Think of each encoder as a layer in a cake. The deeper you go, the richer the representation.

6Ô∏è‚É£ Classification Tasks in BERT

BERT can be used for classification tasks by adding a "Classification Head" on top.

The classification head consists of: 

‚úÖ A Linear Layer (fully connected layer).

‚úÖ A Softmax Activation Function (to compute probabilities).


Final Architecture

![Uploading image.png‚Ä¶]()

** Detailed BERT **

1Ô∏è‚É£ Understanding Tokenization in BERT

Before feeding text into BERT, the input must be tokenized.

BERT uses WordPiece Tokenization, which splits words into subwords and adds special markers (##) to reduce vocabulary size.
The BERT vocabulary size is limited to 30,000 tokens, making processing more efficient.

üìå Example of WordPiece Tokenization:

Original Word	Tokenized Form

"playing"	["play", "##ing"]

"running"	["run", "##ning"]

"unhappiness"	["un", "##happiness"]

üîπ Why does BERT do this?

If BERT stored separate embeddings for every word variation (play, playing, played), the vocabulary size would be too large.

Instead, it splits words into smaller pieces, reducing memory usage and making the model generalize better.

2Ô∏è‚É£ Special Tokens Used in BERT
BERT adds special tokens to process sentences correctly:

Special Token	Purpose

[CLS]	Marks the start of the sentence. Helps in classification tasks.

[SEP]	Separates two sentences in Next Sentence Prediction (NSP).

[MASK]	Used during Masked Language Modeling (MLM).

üìå Example:

For two input sentences:

"My dog is cute." and "He likes playing."

After tokenization, BERT transforms it into:

[CLS] My dog is cute [SEP] He likes play ##ing [SEP]

3Ô∏è‚É£ Types of Embeddings in BERT

Once the input is tokenized, BERT transforms it into embeddings using three types of embeddings:

Embedding Type	Purpose
 - Token Embedding	Converts each token into a vector.
 - Segment Embedding	Tells the model whether a token belongs to sentence A or sentence B.
 - Positional Embedding	Helps the model understand word order and position in the sentence.
 - These embeddings are added together to form a final embedding for each word.
   
4Ô∏è‚É£ Token Embeddings: Converting Words into Vectors

 - Each token is converted into a fixed-size vector representation using pre-trained embeddings.
 - In BERT-base, the embedding size is 768, and in BERT-large, it is 1024.
 - Token embeddings are learned during training and capture semantic meaning.

üìå Example of Token Embeddings (BERT-base, 768-dimensions)

"My"      ‚Üí [0.12, 0.45, ..., 0.78] (768 values)

"dog"     ‚Üí [0.23, 0.56, ..., 0.89]

"is"      ‚Üí [0.34, 0.67, ..., 0.91]

"cute"    ‚Üí [0.41, 0.78, ..., 0.94]

5Ô∏è‚É£ Segment Embeddings: Differentiating Sentences

BERT can process two sentences at once (useful for Next Sentence Prediction tasks).
Segment embeddings help BERT understand which tokens belong to which sentence.

Sentence A gets Segment ID = 0

Sentence B gets Segment ID = 1

üìå Example:

[CLS] My dog is cute [SEP] He likes play ##ing [SEP]

  (0)  0   0   0   0   0   (1)  1   1     1     1   (1)

All tokens from the first sentence = 0

All tokens from the second sentence = 1

üîπ Why are segment embeddings useful?

They allow BERT to distinguish between two different sentences, which is critical for Next Sentence Prediction tasks.

6Ô∏è‚É£ Positional Embeddings: Understanding Word Order

Unlike RNNs, transformers don‚Äôt have a built-in sense of word order.

BERT adds positional embeddings to give a sense of word position.

These embeddings are trainable and have the same size as token embeddings (768 for BERT-base).
üìå Example:

"My"      ‚Üí Position 0

"dog"     ‚Üí Position 1

"is"      ‚Üí Position 2

"cute"    ‚Üí Position 3

"[SEP]"   ‚Üí Position 4

"He"      ‚Üí Position 5

"likes"   ‚Üí Position 6

"playing" ‚Üí Position 7

Each position has a unique embedding.

This helps BERT understand that "My dog" is different from "dog My".

üîπ Why are positional embeddings useful?

They help the model understand sentence structure and grammar.

7Ô∏è‚É£ Final Embedding Calculation

Each token‚Äôs final embedding is calculated by adding:

‚úÖ Token Embedding

‚úÖ Segment Embedding

‚úÖ Positional Embedding

üìå Formula for Final Embedding

Final Embedding = Token Embedding + Segment Embedding + Positional Embedding

9Ô∏è‚É£ Summary: How BERT Prepares Input

1Ô∏è‚É£ Tokenization: Splits words using WordPiece Tokenizer.

2Ô∏è‚É£ Add Special Tokens: [CLS] at the beginning, [SEP] at sentence boundaries.

3Ô∏è‚É£ Convert Tokens to Embeddings: Generate Token Embeddings, Segment Embeddings, and Positional Embeddings.

4Ô∏è‚É£ Sum All Embeddings: Final embedding = Token Embedding + Segment Embedding + Positional Embedding.

5Ô∏è‚É£ Pass to Transformer Encoder: The processed input is now ready for BERT‚Äôs multi-head attention layers.

üîπ Final Takeaway

‚úÖ BERT uses WordPiece Tokenization to handle vocabulary efficiently.

‚úÖ Token, Segment, and Positional Embeddings are combined to form final embeddings.

‚úÖ Segment Embeddings help distinguish between two sentences.

‚úÖ Positional Embeddings help BERT understand word order.

‚úÖ Final embeddings are passed into the Transformer Encoder for further processing.

üìå These embeddings are the foundation of BERT‚Äôs ability to understand text deeply.



## Positional Encoding

Without positional encoding, the Transformer would treat the sentence "I like cats" the same as "cats like I", since it processes all tokens in parallel and lacks the sequential nature of RNNs. By adding positional encodings, the model can learn the relative or absolute position of tokens in a sequence, which is crucial for understanding language.
It uses sine and cosine functions to encode positions, ensuring that each position is uniquely represented.
Without positional encoding, Transformers would lose the ability to understand the sequence order.
Both fixed positional encoding and learned positional embeddings are used in practice, depending on the model architecture and task.

### Positional Encoding Explanation Using a Word Example
Let‚Äôs take a simple sentence:

"She loves cats"

The computer doesn‚Äôt know the order of the words, so we need positional encoding to tell it that:

"She" is the first word.

"loves" is the second word.

"cats" is the third word.

But instead of just saying first, second, and third, we use more complex position tags for each word. These tags are calculated using mathematical functions (sine and cosine), which give us different values for different positions.

#### Step-by-Step Example
Word Embedding: First, each word ("She", "loves", "cats") is converted into a vector (a list of numbers) using a word embedding like GloVe or Word2Vec.

Let‚Äôs say:

"She" becomes [0.5, 0.8, 0.1]

"loves" becomes [0.6, 0.9, 0.3]

"cats" becomes [0.4, 0.7, 0.2]

Positional Encoding: Now, we add positional tags to these word vectors to tell the model where each word is in the sentence.

Using sine and cosine functions, the positional encoding might give us values like:

Position tag for the 1st word ("She"): [0.0, 1.0, 0.5]

Position tag for the 2nd word ("loves"): [0.8, 0.5, 0.2]

Position tag for the 3rd word ("cats"): [0.9, 0.2, 0.1]

Adding Word Embedding + Positional Encoding: For each word, we add the word embedding and the positional encoding together:

"She": [0.5, 0.8, 0.1] + [0.0, 1.0, 0.5] = [0.5, 1.8, 0.6]

"loves": [0.6, 0.9, 0.3] + [0.8, 0.5, 0.2] = [1.4, 1.4, 0.5]

"cats": [0.4, 0.7, 0.2] + [0.9, 0.2, 0.1] = [1.3, 0.9, 0.3]

Now, each word not only has its meaning (from word embedding) but also its position in the sentence (from positional encoding).

#### Why This Helps the Model?

If we change the order of the words:

"Cats love she"

Even though the words are the same, their positional encoding will be different, so the model will know it‚Äôs a different sentence because the position tags will have changed.

For example:

"Cats" will now have the position tag for the 1st word.

"Love" will have the position tag for the 2nd word.

"She" will have the position tag for the 3rd word.

This way, the model understands both the words and their correct order, which is crucial for tasks like language translation or text summarization.

Summary (in Simple Words)

 - Word embeddings tell the model what each word means.
 - Positional encoding tells the model where each word is in the sentence.
 - Together, they help the model understand both the meaning and order of the words, which is essential for processing natural language.

## Query Key and Value


### Key Steps in the Attention Mechanism

### Input Processing:

An input sentence (e.g., "The bank of the river was flooded") is fed into the model.

Each word in the sentence is tokenized and converted into an embedding vector.

For each word (or token), three vectors are created by multiplying the embedding vector with three learned matrices:

Query vector (Q)

Key vector (K)

Value vector (V)

### Generating Attention Scores:

To determine how much attention each word should pay to other words, the model:

Takes the query vector of the current word (e.g., "bank").

Multiplies it with the key vectors of all other words in the sentence using a dot product to compute attention scores.

A high attention score indicates that a word is highly relevant in the given context (e.g., "river" and "flooded" are highly relevant to "bank" in this example).

Using Attention Scores:

The attention scores are used to weigh the value vectors of the corresponding words.

The model combines the weighted value vectors to produce a context-aware representation of the current word.

For example, in the sentence "The bank of the river was flooded," the attention mechanism helps the model understand that "bank" refers to a river bank and not a financial institution, based on its relationship with "river" and "flooded."

Understanding Query, Key, and Value Matrices

### Key Matrix (K):
Learns distinguishing features of each word, helping the model compare words and identify relevant ones.

### Query Matrix (Q):
Learns parameters that, when multiplied with key vectors, produce meaningful attention scores.

### Value Matrix (V):
Stores foundational knowledge about each word (e.g., meanings of "bank") and is refined by attention scores to produce context-aware word representations.

### Purpose of the Attention Mechanism

The attention mechanism allows the model to create a context-aware representation of each word in the sentence.

This helps the model disambiguate words with multiple meanings (like "bank") by focusing on other relevant words in the context.





The final output is a set of refined, context-aware vectors for each word, which the model uses for further tasks like translation or text generation.

