{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwCAF_4WnGrm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM7UEy92nGrn"
      },
      "source": [
        "print(\"hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXCwcSponGrp",
        "outputId": "300c438c-67d7-4eec-b03e-9761ced165b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello\n"
          ]
        }
      ],
      "source": [
        "print(\"hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this lab, let's us the following libraries:\n",
        "\n",
        "- [`torch`](https://pytorch.org/): The core library for building and training neural network models in this project, including the implementation of Self-Attention mechanisms and Positional Encodings.\n",
        "- [`torch.nn`](https://pytorch.org/docs/stable/nn.html), [`torch.nn.functional`](https://pytorch.org/docs/stable/nn.functional.html): These PyTorch submodules are used to define the neural network layers and apply functions such as activations, which are essential in building the model architecture.\n",
        "- [`Levenshtein`](https://pypi.org/project/python-Levenshtein/): This library is used for calculating the Levenshtein distance, which can be useful for evaluating model performance in tasks like text generation or translation by measuring the difference between the predicted and actual text sequences.\n",
        "- [`get_tokenizer`](https://pytorch.org/text/stable/data_utils.html), [`build_vocab_from_iterator`](https://pytorch.org/text/stable/vocab.html) from `torchtext`: These functions are crucial for preprocessing text data, including tokenizing text into words or subwords and building a vocabulary from the dataset, which are foundational steps in preparing data for NLP models.\n"
      ],
      "metadata": {
        "id": "MgR85alqozFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "! pip install Levenshtein\n",
        "#! pip install matplotlib\n",
        "!pip install torch==2.3.0 torchtext==0.18.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zjExYuO4omC3",
        "outputId": "482544eb-fda6-462b-a43a-b9cff2e092e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.10/dist-packages (0.26.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein) (3.11.0)\n",
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchtext==0.18.0\n",
            "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18.0) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18.0) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18.0) (2024.12.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m835.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchtext\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 torchtext-0.18.0 triton-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "04d69ee0e8fe42a1abcc0079b1be11f0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "\n",
        "from Levenshtein import distance\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlJowdNkorSH",
        "outputId": "da01aa6e-35ac-4956-fe28-bb342d33f466"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device for training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "split = 'train'\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 3e-4\n",
        "batch_size = 64\n",
        "max_iters = 5000              # Maximum training iterations\n",
        "eval_interval = 200           # Evaluate model every 'eval_interval' iterations in the training loop\n",
        "eval_iters = 100              # When evaluating, approximate loss using 'eval_iters' batches\n",
        "\n",
        "# Architecture parameters\n",
        "max_vocab_size = 256          # Maximum vocabulary size\n",
        "vocab_size = max_vocab_size   # Real vocabulary size (e.g. BPE has a variable length, so it can be less than 'max_vocab_size')\n",
        "block_size = 16               # Context length for predictions\n",
        "n_embd = 32                   # Embedding size\n",
        "num_heads = 2                 # Number of head in multi-headed attention\n",
        "n_layer = 2                   # Number of Blocks\n",
        "ff_scale_factor = 4           # Note: The '4' magic number is from the paper: In equation 2 uses d_model=512, but d_ff=2048\n",
        "dropout = 0.0                 # Normalization using dropout# 10.788929 M parameters\n",
        "\n",
        "head_size = n_embd // num_heads\n",
        "assert (num_heads * head_size) == n_embd"
      ],
      "metadata": {
        "id": "MJfe5mX9o7Wp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embdings(my_embdings,name,vocab):\n",
        "\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "  # Plot the data points\n",
        "  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])\n",
        "\n",
        "  # Label the points\n",
        "  for j, label in enumerate(name):\n",
        "      i=vocab.get_stoi()[label]\n",
        "      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)\n",
        "\n",
        "  # Set axis labels\n",
        "  ax.set_xlabel('X Label')\n",
        "  ax.set_ylabel('Y Label')\n",
        "  ax.set_zlabel('Z Label')\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ay5oVtLOt_2c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {\n",
        "    'le': 'the'\n",
        "    , 'chat': 'cat'\n",
        "    , 'est': 'is'\n",
        "    , 'sous': 'under'\n",
        "    , 'la': 'the'\n",
        "    , 'table': 'table'\n",
        "}"
      ],
      "metadata": {
        "id": "XKjG8ITqwKya"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and sort the input vocabulary from the dictionary's keys\n",
        "vocabulary_in = sorted(list(set(dictionary.keys())))\n",
        "# Display the size and the sorted vocabulary for the input language\n",
        "print(f\"Vocabulary input ({len(vocabulary_in)}): {vocabulary_in}\")\n",
        "\n",
        "# Create and sort the output vocabulary from the dictionary's values\n",
        "vocabulary_out = sorted(list(set(dictionary.values())))\n",
        "# Display the size and the sorted vocabulary for the output language\n",
        "print(f\"Vocabulary output ({len(vocabulary_out)}): {vocabulary_out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ljAt_ODuDQi",
        "outputId": "eab599a4-c92d-4ab3-fbf2-a0dc9a821179"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary input (6): ['chat', 'est', 'la', 'le', 'sous', 'table']\n",
            "Vocabulary output (5): ['cat', 'is', 'table', 'the', 'under']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - The tokenize function is responsible for breaking down a sentence into individual words.\n",
        " - The translate function uses this tokenize function to split the input sentence and then translates each word according to the dictionary. The translated words are concatenated to form the output sentence."
      ],
      "metadata": {
        "id": "a4UGSNq-wTvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert a list of vocabulary words into one-hot encoded vectors\n",
        "def encode_one_hot(vocabulary):\n",
        "    vocabulary_size = len(vocabulary)  # Get the size of the vocabulary\n",
        "    one_hot = dict()  # Initialize a dictionary to hold our one-hot encodings\n",
        "    LEN = len(vocabulary)  # The length of each one-hot encoded vector will be equal to the vocabulary size\n",
        "\n",
        "    # Iterate over the vocabulary to create a one-hot encoded vector for each word\n",
        "    for i, key in enumerate(vocabulary):\n",
        "        one_hot_vector = torch.zeros(LEN)  # Start with a vector of zeros\n",
        "        one_hot_vector[i] = 1  # Set the i-th position to 1 for the current word\n",
        "        one_hot[key] = one_hot_vector  # Map the word to its one-hot encoded vector\n",
        "        print(f\"{key}\\t: {one_hot[key]}\")  # Print each word and its encoded vector\n",
        "\n",
        "    return one_hot  # Return the dictionary of words and their one-hot encoded vectors"
      ],
      "metadata": {
        "id": "5YVTtBDbwQXF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the one-hot encoding function to the input vocabulary and store the result\n",
        "one_hot_in = encode_one_hot(vocabulary_in)"
      ],
      "metadata": {
        "id": "ShcjKOdk6y9_",
        "outputId": "5839bc4d-93d9-4f5d-f761-6b4602836266",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chat\t: tensor([1., 0., 0., 0., 0., 0.])\n",
            "est\t: tensor([0., 1., 0., 0., 0., 0.])\n",
            "la\t: tensor([0., 0., 1., 0., 0., 0.])\n",
            "le\t: tensor([0., 0., 0., 1., 0., 0.])\n",
            "sous\t: tensor([0., 0., 0., 0., 1., 0.])\n",
            "table\t: tensor([0., 0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the one-hot encoded input vocabulary and print each vector\n",
        "# This visualizes the one-hot representation for each word in the input vocabulary\n",
        "for k, v in one_hot_in.items():\n",
        "    print(f\"E_{{ {k} }} = \" , v)"
      ],
      "metadata": {
        "id": "kr8xBDtk61iQ",
        "outputId": "9d8cb506-1306-4261-f6ed-4e8b41e5ea75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E_{ chat } =  tensor([1., 0., 0., 0., 0., 0.])\n",
            "E_{ est } =  tensor([0., 1., 0., 0., 0., 0.])\n",
            "E_{ la } =  tensor([0., 0., 1., 0., 0., 0.])\n",
            "E_{ le } =  tensor([0., 0., 0., 1., 0., 0.])\n",
            "E_{ sous } =  tensor([0., 0., 0., 0., 1., 0.])\n",
            "E_{ table } =  tensor([0., 0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the one-hot encoding function to the output vocabulary and store the result\n",
        "# This time we're encoding the target language vocabulary\n",
        "one_hot_out = encode_one_hot(vocabulary_out)"
      ],
      "metadata": {
        "id": "6NoRHEzp693z",
        "outputId": "91f227bf-0040-4741-9485-5f166287f7c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\t: tensor([1., 0., 0., 0., 0.])\n",
            "is\t: tensor([0., 1., 0., 0., 0.])\n",
            "table\t: tensor([0., 0., 1., 0., 0.])\n",
            "the\t: tensor([0., 0., 0., 1., 0.])\n",
            "under\t: tensor([0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decode_one_hot function is designed to decode a one-hot encoded vector back into the corresponding token (word). It does this by finding the token whose one-hot representation has the highest cosine similarity with the given vector, which is effectively just the dot product due to the nature of one-hot vectors."
      ],
      "metadata": {
        "id": "oPqBDjD67QH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_one_hot(one_hot, vector):\n",
        "    \"\"\"\n",
        "    Decode a one-hot encoded vector to find the best matching token in the vocabulary.\n",
        "    \"\"\"\n",
        "    best_key, best_cosine_sim = None, 0\n",
        "    for k, v in one_hot.items():  # Iterate over the one-hot encoded vocabulary\n",
        "        cosine_sim = torch.dot(vector, v)  # Calculate dot product (cosine similarity)\n",
        "        if cosine_sim > best_cosine_sim:  # If this is the best similarity we've found\n",
        "            best_cosine_sim, best_key = cosine_sim, k  # Update the best similarity and token\n",
        "    return best_key  # Return the token corresponding to the one-hot vector"
      ],
      "metadata": {
        "id": "wCQj-0Ex7OiK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    \"\"\"\n",
        "    Translate a sentence using matrix multiplication, treating the dictionaries as matrices.\n",
        "    \"\"\"\n",
        "    sentence_out = ''  # Initialize the output sentence\n",
        "    for token_in in tokenize(sentence):  # Tokenize the input sentence\n",
        "        q = one_hot_in[token_in]  # Find the one-hot vector for the token\n",
        "        out = q @ K.T @ V  # Multiply with the input and output matrices to find the translation\n",
        "        token_out = decode_one_hot(one_hot_out, out)  # Decode the output one-hot vector to a token\n",
        "        sentence_out += token_out + ' '  # Append the translated token to the output sentence\n",
        "    return sentence_out.strip()  # Return the translated sentence"
      ],
      "metadata": {
        "id": "3ItaJpYyJgaf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5ByyIVdcLlQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" Self attention head. This class implements a self-attention mechanism\n",
        "        which is a key component of transformer-based neural network architectures. \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()  # Initialize the superclass (nn.Module)\n",
        "        # Embedding layer to convert input token indices to vectors of fixed size (n_embd)\n",
        "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        # Linear layers to compute the queries, keys, and values from the embeddings\n",
        "        self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
        "\n",
        "    def attention(self, x):\n",
        "        embedded_x = self.embedding(x)\n",
        "        k = self.key(embedded_x)\n",
        "        q = self.query(embedded_x)\n",
        "        v = self.value(embedded_x)\n",
        "        # Attention score\n",
        "        w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5   # Query * Keys / normalization\n",
        "        w = F.softmax(w, dim=-1)  # Do a softmax across the last dimesion\n",
        "        return embedded_x,k,q,v,w\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded_x = self.embedding(x)\n",
        "        k = self.key(embedded_x)\n",
        "        q = self.query(embedded_x)\n",
        "        v = self.value(embedded_x)\n",
        "        # Attention score\n",
        "        w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5   # Query * Keys / normalization\n",
        "        w = F.softmax(w, dim=-1)  # Do a softmax across the last dimesion\n",
        "        # Add weighted values\n",
        "        out = w @ v\n",
        "        return out"
      ],
      "metadata": {
        "id": "Ltdeo8tDKI8n"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    (1,\"Introduction to NLP\"),\n",
        "    (2,\"Basics of PyTorch\"),\n",
        "    (1,\"NLP Techniques for Text Classification\"),\n",
        "    (3,\"Named Entity Recognition with PyTorch\"),\n",
        "    (3,\"Sentiment Analysis using PyTorch\"),\n",
        "    (3,\"Machine Translation with PyTorch\"),\n",
        "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
        "    (1,\" Machine Translation with NLP \"),\n",
        "    (1,\" Named Entity vs Sentiment Analysis  NLP \"),\n",
        "    (3,\"he painted the car red\"),\n",
        "    (1,\"he painted the red car\")\n",
        "    ]"
      ],
      "metadata": {
        "id": "pztnAViuVHn8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")  # Get a basic English tokenizer"
      ],
      "metadata": {
        "id": "DXjpun-pVKtJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(data_iter):\n",
        "    \"\"\"Yield list of tokens in the dataset.\"\"\"\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])  # Set default index for unknown words"
      ],
      "metadata": {
        "id": "NSJP8fZUVOUM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_pipeline(x):\n",
        "    \"\"\"Converts a text string to a list of token indices.\"\"\"\n",
        "    return vocab(tokenizer(x))  # Tokenize the input and map each token to its index in the vocabulary"
      ],
      "metadata": {
        "id": "-8dhzZ97Vg_S"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)  # Total number of tokens in the vocabulary\n",
        "n_embd = 3  # Dimension of the embedding space\n",
        "\n",
        "# Create the attention head with the integrated embedding layer\n",
        "attention_head = Head()"
      ],
      "metadata": {
        "id": "KaRyu6xeVjFe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sentence to be tokenized and converted to indices\n",
        "my_tokens = 'he painted the car red'\n",
        "# Apply the text pipeline to the sentence to get token indices\n",
        "input_data = torch.tensor(text_pipeline(my_tokens), dtype=torch.long)\n",
        "\n",
        "# Print out the shape and the token indices tensor\n",
        "print(input_data.shape)\n",
        "print(input_data)"
      ],
      "metadata": {
        "id": "J4Xar-ALVoSo",
        "outputId": "35dd4954-7f28-436b-f8f2-8b8978e3ef3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5])\n",
            "tensor([12, 13, 15, 11, 14])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}