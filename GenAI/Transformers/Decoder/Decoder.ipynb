{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89FXBvnKo0Hl",
        "outputId": "847c827a-d0a4-4be0-e207-c357200a9817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"hello\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Causal Language Models"
      ],
      "metadata": {
        "id": "JD-k9_gco3xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT vs. ChatGPT\n",
        "\n",
        "GPT: General-purpose language model, used for text generation, translation, summarization, etc.\n",
        "\n",
        "ChatGPT: Fine-tuned GPT optimized for conversational AI, maintaining dialogue history for human-like conversations.\n",
        "Key Difference: GPT does one-time text generation, while ChatGPT is designed for interactive dialogue."
      ],
      "metadata": {
        "id": "pQtnA1AyrKjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.2.2\n",
        "!pip install torchtext==0.17.2\n",
        "!pip install portalocker==2.8.2\n",
        "!pip install torchdata==0.7.1\n",
        "!pip install pandas\n",
        "!pip install matplotlib==3.9.0 scikit-learn==1.5.0\n",
        "!pip install numpy==1.26.0\n",
        "!pip install transformers==4.40.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sHfuPz3o_86",
        "outputId": "9cfd6bc7-228b-4dc6-8ebc-425786add450"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2.2\n",
            "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2)\n",
            "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torchvision>=0.11, which is not installed.\n",
            "peft 0.14.0 requires transformers, which is not installed.\n",
            "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "timm 1.0.14 requires torchvision, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.19.3 torch-2.2.2 triton-2.2.0\n",
            "Collecting torchtext==0.17.2\n",
            "  Downloading torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.32.3)\n",
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchtext==0.17.2) (12.6.85)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2->torchtext==0.17.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2->torchtext==0.17.2) (1.3.0)\n",
            "Downloading torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.17.2\n",
            "Collecting portalocker==2.8.2\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
            "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n",
            "Collecting torchdata==0.7.1\n",
            "  Downloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.32.3)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata==0.7.1) (12.6.85)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata==0.7.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2->torchdata==0.7.1) (1.3.0)\n",
            "Downloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchdata\n",
            "Successfully installed torchdata-0.7.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting matplotlib==3.9.0\n",
            "  Downloading matplotlib-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scikit-learn==1.5.0\n",
            "  Downloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (4.55.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.9.0) (1.17.0)\n",
            "Downloading matplotlib-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn, matplotlib\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torchvision>=0.11, which is not installed.\n",
            "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.9.0 scikit-learn-1.5.0\n",
            "Collecting numpy==1.26.0\n",
            "  Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torchvision>=0.11, which is not installed.\n",
            "peft 0.14.0 requires transformers, which is not installed.\n",
            "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.0\n",
            "Collecting transformers==4.40.0\n",
            "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (1.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.0)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2024.12.14)\n",
            "Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torchdata: Enhances data loading and preprocessing functionalities for PyTorch, streamlining the workflow for machine learning models.\n",
        "\n",
        "portalocker: Provides a mechanism to lock files, ensuring that only one process can access a file at a time, useful for managing file resources in concurrent applications.\n",
        "\n",
        "torchtext: Offers utilities for text processing and datasets in PyTorch, simplifying the preparation of data for NLP tasks.\n",
        "\n",
        "matplotlib: A plotting library for creating static, interactive, and animated visualizations in Python, commonly used for data visualization and graphical plotting tasks."
      ],
      "metadata": {
        "id": "rWLHejoir5TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "aE85qI9byYIK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from torchtext.datasets import multi30k, Multi30k\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from typing import Iterable, List\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.datasets import IMDB,PennTreebank\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import time\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "gKA8H7UcrO8M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "train_iter, val_iter = IMDB()"
      ],
      "metadata": {
        "id": "SqVOSgjS6mR7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_itr=iter(train_iter)\n",
        "# retrieving the third first record\n",
        "next(data_itr)\n",
        "next(data_itr)\n",
        "next(data_itr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjgy7iRDFrHz",
        "outputId": "de311ef6-2882-4a30-930f-819868d93f6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQAYU1ebFun1",
        "outputId": "717f4101-16d5-4eae-cc23-51e1dc29a5b4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Preprocessing data\n",
        "The provided code is used for preprocessing text data, particularly for NLP tasks, with a focus on tokenization and vocabulary building.\n",
        "\n",
        "- **Special Symbols and Indices**: Initializes special tokens (`<unk>`, `<pad>`, and an empty string for EOS) with their corresponding indices (`0`, `1`, and `2`). These tokens are used for unknown words, padding, and end of sentence respectively.\n",
        "    - `UNK_IDX`: Index for unknown words.\n",
        "    - `PAD_IDX`: Index used for padding shorter sentences in a batch to ensure uniform length.\n",
        "    - `EOS_IDX`: Index representing the end of a sentence (though not explicitly used here as the EOS symbol is set to an empty string).\n",
        "\n",
        "- **`yield_tokens` Function**: A generator function that iterates through a dataset (`data_iter`), tokenizing each data sample using a `tokenizer` function, and yields one tokenized sample at a time.\n",
        "\n",
        "- **Vocabulary building**: Constructs a vocabulary from the tokenized dataset. The `build_vocab_from_iterator` function processes tokens generated by `yield_tokens`, includes special tokens (`special_symbols`) at the beginning of the vocabulary, and sets a minimum frequency (`min_freq=1`) for tokens to be included.\n",
        "\n",
        "- **Default index for unknown tokens**: Sets a default index for tokens not found in the vocabulary (`UNK_IDX`), ensuring that out-of-vocabulary words are handled as unknown tokens.\n",
        "\n",
        "- **`text_to_index` function**: Converts a given text into a sequence of indices based on the built vocabulary. This function is essential for transforming raw text into a numerical format that can be processed by machine learning models.\n",
        "\n",
        "- **`index_to_en` function**: Transforms a sequence of indices back into a readable string. It's useful for interpreting the outputs of models and converting numerical predictions back into text.\n",
        "\n",
        "- **Check functionality**: Demonstrates the use of `index_to_en` by converting a tensor of indices `[0,1,2]` back into their corresponding special symbols. This helps verify that the vocabulary and index conversion functions are working as expected.\n"
      ],
      "metadata": {
        "id": "vO0evZmIF1eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<|endoftext|>' ]"
      ],
      "metadata": {
        "id": "5-Mr10zgF01u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")"
      ],
      "metadata": {
        "id": "zhlKDYn7GE6z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(data_iter):\n",
        "\n",
        "    for _,data_sample in data_iter:\n",
        "        yield  tokenizer(data_sample)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=special_symbols, special_first=True)\n",
        "vocab.set_default_index(UNK_IDX)\n"
      ],
      "metadata": {
        "id": "cqIQU4tUGG7H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_index=lambda text: [vocab(token) for token in tokenizer(text)]\n",
        "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
      ],
      "metadata": {
        "id": "Fy_aqU75GJFH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check\n",
        "index_to_en(torch.tensor([0,1,2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rYbTPlY5GNHk",
        "outputId": "0ba81259-3b37-4307-a8be-3f8ba99354e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<unk> <pad> <|endoftext|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collate function\n",
        "In the context of our decoder model, we aim to create a collate function. This function takes a block of text as input and produces a modified block of text as output. The actual text transformation is achieved through the use of the `get_sample(block_size, text)` function. The **get_sample** function generates a random text sample(src_sequence) and its subsequent sequence(tgt_sequence) from a given text for language model training. It ensures the sample fits within the specified block size and adjusts for text shorter than the block size, returning both the source and target sequences for model input.\n"
      ],
      "metadata": {
        "id": "CmrXQ8IYGmBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample(block_size, text):\n",
        "    # Determine the length of the input text\n",
        "    sample_leg = len(text)\n",
        "    # Calculate the stopping point for randomly selecting a sample\n",
        "    # This ensures the selected sample doesn't exceed the text length\n",
        "    random_sample_stop = sample_leg - block_size\n",
        "\n",
        "\n",
        "    # Check if a random sample can be taken (if the text is longer than block_size)\n",
        "    if random_sample_stop >= 1:\n",
        "        # Randomly select a starting point for the sample\n",
        "        random_start = torch.randint(low=0, high=random_sample_stop, size=(1,)).item()\n",
        "        # Define the endpoint of the sample\n",
        "        stop = random_start + block_size\n",
        "\n",
        "        # Create the input and target sequences\n",
        "        src_sequence = text[random_start:stop]\n",
        "        tgt_sequence= text[random_start + 1:stop + 1]\n",
        "\n",
        "    # Handle the case where the text length is exactly equal or less the block size\n",
        "    elif random_sample_stop <= 0:\n",
        "        # Start from the beginning and use the entire text\n",
        "        random_start = 0\n",
        "        stop = sample_leg\n",
        "        src_sequence= text[random_start:stop]\n",
        "        tgt_sequence = text[random_start + 1:stop]\n",
        "        # Append an empty string to maintain sequence alignment\n",
        "        tgt_sequence.append( '<|endoftext|>')\n",
        "\n",
        "    return src_sequence, tgt_sequence"
      ],
      "metadata": {
        "id": "YffckmDVGk1p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=1\n",
        "\n",
        "batch_of_tokens=[]\n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "  _,text =next(iter(train_iter))\n",
        "  batch_of_tokens.append(tokenizer(text))"
      ],
      "metadata": {
        "id": "zg5j4dB-Iaz0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=batch_of_tokens[0][0:100]\n",
        "text[0:100]\n",
        "batch_of_tokens"
      ],
      "metadata": {
        "id": "3u1o1o3BIhj7",
        "outputId": "a4867449-0262-4fb3-8416-df5ec43b6eed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['i',\n",
              "  'rented',\n",
              "  'i',\n",
              "  'am',\n",
              "  'curious-yellow',\n",
              "  'from',\n",
              "  'my',\n",
              "  'video',\n",
              "  'store',\n",
              "  'because',\n",
              "  'of',\n",
              "  'all',\n",
              "  'the',\n",
              "  'controversy',\n",
              "  'that',\n",
              "  'surrounded',\n",
              "  'it',\n",
              "  'when',\n",
              "  'it',\n",
              "  'was',\n",
              "  'first',\n",
              "  'released',\n",
              "  'in',\n",
              "  '1967',\n",
              "  '.',\n",
              "  'i',\n",
              "  'also',\n",
              "  'heard',\n",
              "  'that',\n",
              "  'at',\n",
              "  'first',\n",
              "  'it',\n",
              "  'was',\n",
              "  'seized',\n",
              "  'by',\n",
              "  'u',\n",
              "  '.',\n",
              "  's',\n",
              "  '.',\n",
              "  'customs',\n",
              "  'if',\n",
              "  'it',\n",
              "  'ever',\n",
              "  'tried',\n",
              "  'to',\n",
              "  'enter',\n",
              "  'this',\n",
              "  'country',\n",
              "  ',',\n",
              "  'therefore',\n",
              "  'being',\n",
              "  'a',\n",
              "  'fan',\n",
              "  'of',\n",
              "  'films',\n",
              "  'considered',\n",
              "  'controversial',\n",
              "  'i',\n",
              "  'really',\n",
              "  'had',\n",
              "  'to',\n",
              "  'see',\n",
              "  'this',\n",
              "  'for',\n",
              "  'myself',\n",
              "  '.',\n",
              "  'the',\n",
              "  'plot',\n",
              "  'is',\n",
              "  'centered',\n",
              "  'around',\n",
              "  'a',\n",
              "  'young',\n",
              "  'swedish',\n",
              "  'drama',\n",
              "  'student',\n",
              "  'named',\n",
              "  'lena',\n",
              "  'who',\n",
              "  'wants',\n",
              "  'to',\n",
              "  'learn',\n",
              "  'everything',\n",
              "  'she',\n",
              "  'can',\n",
              "  'about',\n",
              "  'life',\n",
              "  '.',\n",
              "  'in',\n",
              "  'particular',\n",
              "  'she',\n",
              "  'wants',\n",
              "  'to',\n",
              "  'focus',\n",
              "  'her',\n",
              "  'attentions',\n",
              "  'to',\n",
              "  'making',\n",
              "  'some',\n",
              "  'sort',\n",
              "  'of',\n",
              "  'documentary',\n",
              "  'on',\n",
              "  'what',\n",
              "  'the',\n",
              "  'average',\n",
              "  'swede',\n",
              "  'thought',\n",
              "  'about',\n",
              "  'certain',\n",
              "  'political',\n",
              "  'issues',\n",
              "  'such',\n",
              "  'as',\n",
              "  'the',\n",
              "  'vietnam',\n",
              "  'war',\n",
              "  'and',\n",
              "  'race',\n",
              "  'issues',\n",
              "  'in',\n",
              "  'the',\n",
              "  'united',\n",
              "  'states',\n",
              "  '.',\n",
              "  'in',\n",
              "  'between',\n",
              "  'asking',\n",
              "  'politicians',\n",
              "  'and',\n",
              "  'ordinary',\n",
              "  'denizens',\n",
              "  'of',\n",
              "  'stockholm',\n",
              "  'about',\n",
              "  'their',\n",
              "  'opinions',\n",
              "  'on',\n",
              "  'politics',\n",
              "  ',',\n",
              "  'she',\n",
              "  'has',\n",
              "  'sex',\n",
              "  'with',\n",
              "  'her',\n",
              "  'drama',\n",
              "  'teacher',\n",
              "  ',',\n",
              "  'classmates',\n",
              "  ',',\n",
              "  'and',\n",
              "  'married',\n",
              "  'men',\n",
              "  '.',\n",
              "  'what',\n",
              "  'kills',\n",
              "  'me',\n",
              "  'about',\n",
              "  'i',\n",
              "  'am',\n",
              "  'curious-yellow',\n",
              "  'is',\n",
              "  'that',\n",
              "  '40',\n",
              "  'years',\n",
              "  'ago',\n",
              "  ',',\n",
              "  'this',\n",
              "  'was',\n",
              "  'considered',\n",
              "  'pornographic',\n",
              "  '.',\n",
              "  'really',\n",
              "  ',',\n",
              "  'the',\n",
              "  'sex',\n",
              "  'and',\n",
              "  'nudity',\n",
              "  'scenes',\n",
              "  'are',\n",
              "  'few',\n",
              "  'and',\n",
              "  'far',\n",
              "  'between',\n",
              "  ',',\n",
              "  'even',\n",
              "  'then',\n",
              "  'it',\n",
              "  \"'\",\n",
              "  's',\n",
              "  'not',\n",
              "  'shot',\n",
              "  'like',\n",
              "  'some',\n",
              "  'cheaply',\n",
              "  'made',\n",
              "  'porno',\n",
              "  '.',\n",
              "  'while',\n",
              "  'my',\n",
              "  'countrymen',\n",
              "  'mind',\n",
              "  'find',\n",
              "  'it',\n",
              "  'shocking',\n",
              "  ',',\n",
              "  'in',\n",
              "  'reality',\n",
              "  'sex',\n",
              "  'and',\n",
              "  'nudity',\n",
              "  'are',\n",
              "  'a',\n",
              "  'major',\n",
              "  'staple',\n",
              "  'in',\n",
              "  'swedish',\n",
              "  'cinema',\n",
              "  '.',\n",
              "  'even',\n",
              "  'ingmar',\n",
              "  'bergman',\n",
              "  ',',\n",
              "  'arguably',\n",
              "  'their',\n",
              "  'answer',\n",
              "  'to',\n",
              "  'good',\n",
              "  'old',\n",
              "  'boy',\n",
              "  'john',\n",
              "  'ford',\n",
              "  ',',\n",
              "  'had',\n",
              "  'sex',\n",
              "  'scenes',\n",
              "  'in',\n",
              "  'his',\n",
              "  'films',\n",
              "  '.',\n",
              "  'i',\n",
              "  'do',\n",
              "  'commend',\n",
              "  'the',\n",
              "  'filmmakers',\n",
              "  'for',\n",
              "  'the',\n",
              "  'fact',\n",
              "  'that',\n",
              "  'any',\n",
              "  'sex',\n",
              "  'shown',\n",
              "  'in',\n",
              "  'the',\n",
              "  'film',\n",
              "  'is',\n",
              "  'shown',\n",
              "  'for',\n",
              "  'artistic',\n",
              "  'purposes',\n",
              "  'rather',\n",
              "  'than',\n",
              "  'just',\n",
              "  'to',\n",
              "  'shock',\n",
              "  'people',\n",
              "  'and',\n",
              "  'make',\n",
              "  'money',\n",
              "  'to',\n",
              "  'be',\n",
              "  'shown',\n",
              "  'in',\n",
              "  'pornographic',\n",
              "  'theaters',\n",
              "  'in',\n",
              "  'america',\n",
              "  '.',\n",
              "  'i',\n",
              "  'am',\n",
              "  'curious-yellow',\n",
              "  'is',\n",
              "  'a',\n",
              "  'good',\n",
              "  'film',\n",
              "  'for',\n",
              "  'anyone',\n",
              "  'wanting',\n",
              "  'to',\n",
              "  'study',\n",
              "  'the',\n",
              "  'meat',\n",
              "  'and',\n",
              "  'potatoes',\n",
              "  '(',\n",
              "  'no',\n",
              "  'pun',\n",
              "  'intended',\n",
              "  ')',\n",
              "  'of',\n",
              "  'swedish',\n",
              "  'cinema',\n",
              "  '.',\n",
              "  'but',\n",
              "  'really',\n",
              "  ',',\n",
              "  'this',\n",
              "  'film',\n",
              "  'doesn',\n",
              "  \"'\",\n",
              "  't',\n",
              "  'have',\n",
              "  'much',\n",
              "  'of',\n",
              "  'a',\n",
              "  'plot',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the get_sample function with a block size of 100, where the output includes both the source sequence and the target sequence, with the target sequence being the source sequence shifted by one character, you can use the following code as an example:\n"
      ],
      "metadata": {
        "id": "HOQjDvjSIpDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size=10\n",
        "src_sequences, tgt_sequence=get_sample( block_size, text)"
      ],
      "metadata": {
        "id": "LJrWSLqdIlC8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"src: \",src_sequences)\n",
        "print(\"tgt: \",tgt_sequence)"
      ],
      "metadata": {
        "id": "PAV8b6EhItKG",
        "outputId": "cb450446-78fd-45b2-f956-4497b43a6305",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src:  ['everything', 'she', 'can', 'about', 'life', '.', 'in', 'particular', 'she', 'wants']\n",
            "tgt:  ['she', 'can', 'about', 'life', '.', 'in', 'particular', 'she', 'wants', 'to']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty lists to store source and target sequences\n",
        "src_batch, tgt_batch = [], []\n",
        "\n",
        "# Define the batch size\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# Loop to create batches of source and target sequences\n",
        "for i in range(BATCH_SIZE):\n",
        "    # Retrieve the next data point from the training iterator\n",
        "    _,text = next(iter(train_iter))\n",
        "\n",
        "    # Generate source and target sequences using the get_sample function\n",
        "    src_sequence_text, tgt_sequence_text = get_sample(block_size, tokenizer(text))\n",
        "\n",
        "    # Convert source and target sequences to tokenized vocabulary indices\n",
        "    src_sequence_indices = vocab(src_sequence_text)\n",
        "    tgt_sequence_indices = vocab(tgt_sequence_text)\n",
        "\n",
        "    # Convert the sequences to PyTorch tensors with dtype int64\n",
        "    src_sequence = torch.tensor(src_sequence_indices, dtype=torch.int64)\n",
        "    tgt_sequence = torch.tensor(tgt_sequence_indices, dtype=torch.int64)\n",
        "\n",
        "    # Append the source and target sequences to their respective batches\n",
        "    src_batch.append(src_sequence)\n",
        "    tgt_batch.append(tgt_sequence)\n",
        "\n",
        "    # Print the output for every 2nd sample (adjust as needed)\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(\"Source Sequence (Text):\", src_sequence_text)\n",
        "    print(\"Source Sequence (Indices):\", src_sequence_indices)\n",
        "    print(\"Source Sequence (Shape):\", src_sequence.shape)\n",
        "    print(\"Target Sequence (Text):\", tgt_sequence_text)\n",
        "    print(\"Target Sequence (Indices):\", tgt_sequence_indices)\n",
        "    print(\"Target Sequence (Shape):\", tgt_sequence.shape)"
      ],
      "metadata": {
        "id": "CI7W7G0hI3gE",
        "outputId": "e172e25a-b83d-4e7c-dc86-5da7b861a6c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0:\n",
            "Source Sequence (Text): ['about', 'life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her']\n",
            "Source Sequence (Indices): [51, 128, 4, 13, 827, 62, 490, 9, 1133, 48]\n",
            "Source Sequence (Shape): torch.Size([10])\n",
            "Target Sequence (Text): ['life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attentions']\n",
            "Target Sequence (Indices): [128, 4, 13, 827, 62, 490, 9, 1133, 48, 11860]\n",
            "Target Sequence (Shape): torch.Size([10])\n",
            "Sample 1:\n",
            "Source Sequence (Text): ['good', 'old', 'boy', 'john', 'ford', ',', 'had', 'sex', 'scenes', 'in']\n",
            "Source Sequence (Indices): [58, 177, 436, 305, 1722, 5, 76, 403, 146, 13]\n",
            "Source Sequence (Shape): torch.Size([10])\n",
            "Target Sequence (Text): ['old', 'boy', 'john', 'ford', ',', 'had', 'sex', 'scenes', 'in', 'his']\n",
            "Target Sequence (Indices): [177, 436, 305, 1722, 5, 76, 403, 146, 13, 33]\n",
            "Target Sequence (Shape): torch.Size([10])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}