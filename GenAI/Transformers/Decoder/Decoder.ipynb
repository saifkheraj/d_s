{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "89FXBvnKo0Hl",
        "outputId": "4d30a138-0407-4b98-e706-75017e2b6996",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"hello\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Causal Language Models"
      ],
      "metadata": {
        "id": "JD-k9_gco3xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT vs. ChatGPT\n",
        "\n",
        "GPT: General-purpose language model, used for text generation, translation, summarization, etc.\n",
        "\n",
        "ChatGPT: Fine-tuned GPT optimized for conversational AI, maintaining dialogue history for human-like conversations.\n",
        "Key Difference: GPT does one-time text generation, while ChatGPT is designed for interactive dialogue."
      ],
      "metadata": {
        "id": "pQtnA1AyrKjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.2.2\n",
        "!pip install torchtext==0.17.2\n",
        "!pip install portalocker==2.8.2\n",
        "!pip install torchdata==0.7.1\n",
        "!pip install pandas\n",
        "!pip install matplotlib==3.9.0 scikit-learn==1.5.0\n",
        "!pip install numpy==1.26.0\n",
        "!pip install transformers==4.40.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sHfuPz3o_86",
        "outputId": "52d539ec-a72c-4a4a-bf9c-f57571928a1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Requirement already satisfied: torchtext==0.17.2 in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.32.3)\n",
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (1.26.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchtext==0.17.2) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2->torchtext==0.17.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2->torchtext==0.17.2) (1.3.0)\n",
            "Requirement already satisfied: portalocker==2.8.2 in /usr/local/lib/python3.11/dist-packages (2.8.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.11/dist-packages (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.32.3)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata==0.7.1) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata==0.7.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2->torchdata==0.7.1) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: matplotlib==3.9.0 in /usr/local/lib/python3.11/dist-packages (3.9.0)\n",
            "Requirement already satisfied: scikit-learn==1.5.0 in /usr/local/lib/python3.11/dist-packages (1.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.9.0) (1.17.0)\n",
            "Requirement already satisfied: numpy==1.26.0 in /usr/local/lib/python3.11/dist-packages (1.26.0)\n",
            "Requirement already satisfied: transformers==4.40.0 in /usr/local/lib/python3.11/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (1.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torchdata: Enhances data loading and preprocessing functionalities for PyTorch, streamlining the workflow for machine learning models.\n",
        "\n",
        "portalocker: Provides a mechanism to lock files, ensuring that only one process can access a file at a time, useful for managing file resources in concurrent applications.\n",
        "\n",
        "torchtext: Offers utilities for text processing and datasets in PyTorch, simplifying the preparation of data for NLP tasks.\n",
        "\n",
        "matplotlib: A plotting library for creating static, interactive, and animated visualizations in Python, commonly used for data visualization and graphical plotting tasks."
      ],
      "metadata": {
        "id": "rWLHejoir5TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "aE85qI9byYIK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from torchtext.datasets import multi30k, Multi30k\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from typing import Iterable, List\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.datasets import IMDB,PennTreebank\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import time\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "gKA8H7UcrO8M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "train_iter, val_iter = IMDB()"
      ],
      "metadata": {
        "id": "SqVOSgjS6mR7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_itr=iter(train_iter)\n",
        "# retrieving the third first record\n",
        "next(data_itr)\n",
        "next(data_itr)\n",
        "next(data_itr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjgy7iRDFrHz",
        "outputId": "0c023802-2724-4b3a-e50f-683c0f40dabc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQAYU1ebFun1",
        "outputId": "92e36771-3716-4d26-c5ae-ce74fae50d49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Preprocessing data\n",
        "The provided code is used for preprocessing text data, particularly for NLP tasks, with a focus on tokenization and vocabulary building.\n",
        "\n",
        "- **Special Symbols and Indices**: Initializes special tokens (`<unk>`, `<pad>`, and an empty string for EOS) with their corresponding indices (`0`, `1`, and `2`). These tokens are used for unknown words, padding, and end of sentence respectively.\n",
        "    - `UNK_IDX`: Index for unknown words.\n",
        "    - `PAD_IDX`: Index used for padding shorter sentences in a batch to ensure uniform length.\n",
        "    - `EOS_IDX`: Index representing the end of a sentence (though not explicitly used here as the EOS symbol is set to an empty string).\n",
        "\n",
        "- **`yield_tokens` Function**: A generator function that iterates through a dataset (`data_iter`), tokenizing each data sample using a `tokenizer` function, and yields one tokenized sample at a time.\n",
        "\n",
        "- **Vocabulary building**: Constructs a vocabulary from the tokenized dataset. The `build_vocab_from_iterator` function processes tokens generated by `yield_tokens`, includes special tokens (`special_symbols`) at the beginning of the vocabulary, and sets a minimum frequency (`min_freq=1`) for tokens to be included.\n",
        "\n",
        "- **Default index for unknown tokens**: Sets a default index for tokens not found in the vocabulary (`UNK_IDX`), ensuring that out-of-vocabulary words are handled as unknown tokens.\n",
        "\n",
        "- **`text_to_index` function**: Converts a given text into a sequence of indices based on the built vocabulary. This function is essential for transforming raw text into a numerical format that can be processed by machine learning models.\n",
        "\n",
        "- **`index_to_en` function**: Transforms a sequence of indices back into a readable string. It's useful for interpreting the outputs of models and converting numerical predictions back into text.\n",
        "\n",
        "- **Check functionality**: Demonstrates the use of `index_to_en` by converting a tensor of indices `[0,1,2]` back into their corresponding special symbols. This helps verify that the vocabulary and index conversion functions are working as expected.\n"
      ],
      "metadata": {
        "id": "vO0evZmIF1eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<|endoftext|>' ]"
      ],
      "metadata": {
        "id": "5-Mr10zgF01u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")"
      ],
      "metadata": {
        "id": "zhlKDYn7GE6z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(data_iter):\n",
        "\n",
        "    for _,data_sample in data_iter:\n",
        "        yield  tokenizer(data_sample)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=special_symbols, special_first=True)\n",
        "vocab.set_default_index(UNK_IDX)\n"
      ],
      "metadata": {
        "id": "cqIQU4tUGG7H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_index=lambda text: [vocab(token) for token in tokenizer(text)]\n",
        "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
      ],
      "metadata": {
        "id": "Fy_aqU75GJFH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check\n",
        "index_to_en(torch.tensor([0,1,2,3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rYbTPlY5GNHk",
        "outputId": "42645515-f757-482f-81f1-28480a2de358"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<unk> <pad> <|endoftext|> .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collate function\n",
        "In the context of our decoder model, we aim to create a collate function. This function takes a block of text as input and produces a modified block of text as output. The actual text transformation is achieved through the use of the `get_sample(block_size, text)` function. The **get_sample** function generates a random text sample(src_sequence) and its subsequent sequence(tgt_sequence) from a given text for language model training. It ensures the sample fits within the specified block size and adjusts for text shorter than the block size, returning both the source and target sequences for model input.\n"
      ],
      "metadata": {
        "id": "CmrXQ8IYGmBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The source (src_sequence) and target (tgt_sequence) sequences are offset by one position because this function is preparing data for training language models, particularly for next-character or next-token prediction.\n",
        "\n",
        "Reason for One-Character Shift:\n",
        "Supervised Learning Setup:\n",
        "\n",
        "The model is trained to predict the next character (or token) in the sequence given the current character."
      ],
      "metadata": {
        "id": "V6kLHXUWWA5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample(block_size, text):\n",
        "    # Determine the length of the input text\n",
        "    sample_leg = len(text)\n",
        "    # Calculate the stopping point for randomly selecting a sample\n",
        "    # This ensures the selected sample doesn't exceed the text length\n",
        "    random_sample_stop = sample_leg - block_size\n",
        "\n",
        "\n",
        "    # Check if a random sample can be taken (if the text is longer than block_size)\n",
        "    if random_sample_stop >= 1:\n",
        "        # Randomly select a starting point for the sample\n",
        "        random_start = torch.randint(low=0, high=random_sample_stop, size=(1,)).item()\n",
        "        # Define the endpoint of the sample\n",
        "        stop = random_start + block_size\n",
        "\n",
        "        # Create the input and target sequences\n",
        "        src_sequence = text[random_start:stop]\n",
        "        tgt_sequence= text[random_start + 1:stop + 1]\n",
        "\n",
        "    # Handle the case where the text length is exactly equal or less the block size\n",
        "    elif random_sample_stop <= 0:\n",
        "        # Start from the beginning and use the entire text\n",
        "        random_start = 0\n",
        "        stop = sample_leg\n",
        "        src_sequence= text[random_start:stop]\n",
        "        tgt_sequence = text[random_start + 1:stop]\n",
        "        # Append an empty string to maintain sequence alignment\n",
        "        tgt_sequence.append( '<|endoftext|>')\n",
        "\n",
        "    return src_sequence, tgt_sequence"
      ],
      "metadata": {
        "id": "YffckmDVGk1p"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=1\n",
        "\n",
        "batch_of_tokens=[]\n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "  _,text =next(iter(train_iter))\n",
        "  batch_of_tokens.append(tokenizer(text))"
      ],
      "metadata": {
        "id": "zg5j4dB-Iaz0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=batch_of_tokens[0][0:100]\n",
        "text[0:100]\n",
        "batch_of_tokens"
      ],
      "metadata": {
        "id": "3u1o1o3BIhj7",
        "outputId": "8ae7e0d9-bce6-42a9-a25f-3c43f839a328",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['i',\n",
              "  'rented',\n",
              "  'i',\n",
              "  'am',\n",
              "  'curious-yellow',\n",
              "  'from',\n",
              "  'my',\n",
              "  'video',\n",
              "  'store',\n",
              "  'because',\n",
              "  'of',\n",
              "  'all',\n",
              "  'the',\n",
              "  'controversy',\n",
              "  'that',\n",
              "  'surrounded',\n",
              "  'it',\n",
              "  'when',\n",
              "  'it',\n",
              "  'was',\n",
              "  'first',\n",
              "  'released',\n",
              "  'in',\n",
              "  '1967',\n",
              "  '.',\n",
              "  'i',\n",
              "  'also',\n",
              "  'heard',\n",
              "  'that',\n",
              "  'at',\n",
              "  'first',\n",
              "  'it',\n",
              "  'was',\n",
              "  'seized',\n",
              "  'by',\n",
              "  'u',\n",
              "  '.',\n",
              "  's',\n",
              "  '.',\n",
              "  'customs',\n",
              "  'if',\n",
              "  'it',\n",
              "  'ever',\n",
              "  'tried',\n",
              "  'to',\n",
              "  'enter',\n",
              "  'this',\n",
              "  'country',\n",
              "  ',',\n",
              "  'therefore',\n",
              "  'being',\n",
              "  'a',\n",
              "  'fan',\n",
              "  'of',\n",
              "  'films',\n",
              "  'considered',\n",
              "  'controversial',\n",
              "  'i',\n",
              "  'really',\n",
              "  'had',\n",
              "  'to',\n",
              "  'see',\n",
              "  'this',\n",
              "  'for',\n",
              "  'myself',\n",
              "  '.',\n",
              "  'the',\n",
              "  'plot',\n",
              "  'is',\n",
              "  'centered',\n",
              "  'around',\n",
              "  'a',\n",
              "  'young',\n",
              "  'swedish',\n",
              "  'drama',\n",
              "  'student',\n",
              "  'named',\n",
              "  'lena',\n",
              "  'who',\n",
              "  'wants',\n",
              "  'to',\n",
              "  'learn',\n",
              "  'everything',\n",
              "  'she',\n",
              "  'can',\n",
              "  'about',\n",
              "  'life',\n",
              "  '.',\n",
              "  'in',\n",
              "  'particular',\n",
              "  'she',\n",
              "  'wants',\n",
              "  'to',\n",
              "  'focus',\n",
              "  'her',\n",
              "  'attentions',\n",
              "  'to',\n",
              "  'making',\n",
              "  'some',\n",
              "  'sort',\n",
              "  'of',\n",
              "  'documentary',\n",
              "  'on',\n",
              "  'what',\n",
              "  'the',\n",
              "  'average',\n",
              "  'swede',\n",
              "  'thought',\n",
              "  'about',\n",
              "  'certain',\n",
              "  'political',\n",
              "  'issues',\n",
              "  'such',\n",
              "  'as',\n",
              "  'the',\n",
              "  'vietnam',\n",
              "  'war',\n",
              "  'and',\n",
              "  'race',\n",
              "  'issues',\n",
              "  'in',\n",
              "  'the',\n",
              "  'united',\n",
              "  'states',\n",
              "  '.',\n",
              "  'in',\n",
              "  'between',\n",
              "  'asking',\n",
              "  'politicians',\n",
              "  'and',\n",
              "  'ordinary',\n",
              "  'denizens',\n",
              "  'of',\n",
              "  'stockholm',\n",
              "  'about',\n",
              "  'their',\n",
              "  'opinions',\n",
              "  'on',\n",
              "  'politics',\n",
              "  ',',\n",
              "  'she',\n",
              "  'has',\n",
              "  'sex',\n",
              "  'with',\n",
              "  'her',\n",
              "  'drama',\n",
              "  'teacher',\n",
              "  ',',\n",
              "  'classmates',\n",
              "  ',',\n",
              "  'and',\n",
              "  'married',\n",
              "  'men',\n",
              "  '.',\n",
              "  'what',\n",
              "  'kills',\n",
              "  'me',\n",
              "  'about',\n",
              "  'i',\n",
              "  'am',\n",
              "  'curious-yellow',\n",
              "  'is',\n",
              "  'that',\n",
              "  '40',\n",
              "  'years',\n",
              "  'ago',\n",
              "  ',',\n",
              "  'this',\n",
              "  'was',\n",
              "  'considered',\n",
              "  'pornographic',\n",
              "  '.',\n",
              "  'really',\n",
              "  ',',\n",
              "  'the',\n",
              "  'sex',\n",
              "  'and',\n",
              "  'nudity',\n",
              "  'scenes',\n",
              "  'are',\n",
              "  'few',\n",
              "  'and',\n",
              "  'far',\n",
              "  'between',\n",
              "  ',',\n",
              "  'even',\n",
              "  'then',\n",
              "  'it',\n",
              "  \"'\",\n",
              "  's',\n",
              "  'not',\n",
              "  'shot',\n",
              "  'like',\n",
              "  'some',\n",
              "  'cheaply',\n",
              "  'made',\n",
              "  'porno',\n",
              "  '.',\n",
              "  'while',\n",
              "  'my',\n",
              "  'countrymen',\n",
              "  'mind',\n",
              "  'find',\n",
              "  'it',\n",
              "  'shocking',\n",
              "  ',',\n",
              "  'in',\n",
              "  'reality',\n",
              "  'sex',\n",
              "  'and',\n",
              "  'nudity',\n",
              "  'are',\n",
              "  'a',\n",
              "  'major',\n",
              "  'staple',\n",
              "  'in',\n",
              "  'swedish',\n",
              "  'cinema',\n",
              "  '.',\n",
              "  'even',\n",
              "  'ingmar',\n",
              "  'bergman',\n",
              "  ',',\n",
              "  'arguably',\n",
              "  'their',\n",
              "  'answer',\n",
              "  'to',\n",
              "  'good',\n",
              "  'old',\n",
              "  'boy',\n",
              "  'john',\n",
              "  'ford',\n",
              "  ',',\n",
              "  'had',\n",
              "  'sex',\n",
              "  'scenes',\n",
              "  'in',\n",
              "  'his',\n",
              "  'films',\n",
              "  '.',\n",
              "  'i',\n",
              "  'do',\n",
              "  'commend',\n",
              "  'the',\n",
              "  'filmmakers',\n",
              "  'for',\n",
              "  'the',\n",
              "  'fact',\n",
              "  'that',\n",
              "  'any',\n",
              "  'sex',\n",
              "  'shown',\n",
              "  'in',\n",
              "  'the',\n",
              "  'film',\n",
              "  'is',\n",
              "  'shown',\n",
              "  'for',\n",
              "  'artistic',\n",
              "  'purposes',\n",
              "  'rather',\n",
              "  'than',\n",
              "  'just',\n",
              "  'to',\n",
              "  'shock',\n",
              "  'people',\n",
              "  'and',\n",
              "  'make',\n",
              "  'money',\n",
              "  'to',\n",
              "  'be',\n",
              "  'shown',\n",
              "  'in',\n",
              "  'pornographic',\n",
              "  'theaters',\n",
              "  'in',\n",
              "  'america',\n",
              "  '.',\n",
              "  'i',\n",
              "  'am',\n",
              "  'curious-yellow',\n",
              "  'is',\n",
              "  'a',\n",
              "  'good',\n",
              "  'film',\n",
              "  'for',\n",
              "  'anyone',\n",
              "  'wanting',\n",
              "  'to',\n",
              "  'study',\n",
              "  'the',\n",
              "  'meat',\n",
              "  'and',\n",
              "  'potatoes',\n",
              "  '(',\n",
              "  'no',\n",
              "  'pun',\n",
              "  'intended',\n",
              "  ')',\n",
              "  'of',\n",
              "  'swedish',\n",
              "  'cinema',\n",
              "  '.',\n",
              "  'but',\n",
              "  'really',\n",
              "  ',',\n",
              "  'this',\n",
              "  'film',\n",
              "  'doesn',\n",
              "  \"'\",\n",
              "  't',\n",
              "  'have',\n",
              "  'much',\n",
              "  'of',\n",
              "  'a',\n",
              "  'plot',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the get_sample function with a block size of 100, where the output includes both the source sequence and the target sequence, with the target sequence being the source sequence shifted by one character, you can use the following code as an example:\n"
      ],
      "metadata": {
        "id": "HOQjDvjSIpDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size=10\n",
        "src_sequences, tgt_sequence=get_sample( block_size, text)"
      ],
      "metadata": {
        "id": "LJrWSLqdIlC8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"src: \",src_sequences)\n",
        "print(\"tgt: \",tgt_sequence)"
      ],
      "metadata": {
        "id": "PAV8b6EhItKG",
        "outputId": "565cd0f9-74c9-4655-f636-a34ca4010c01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src:  ['about', 'life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her']\n",
            "tgt:  ['life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attentions']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty lists to store source and target sequences\n",
        "src_batch, tgt_batch = [], []\n",
        "\n",
        "# Define the batch size\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# Loop to create batches of source and target sequences\n",
        "for i in range(BATCH_SIZE):\n",
        "    # Retrieve the next data point from the training iterator\n",
        "    _,text = next(iter(train_iter))\n",
        "\n",
        "    # Generate source and target sequences using the get_sample function\n",
        "    src_sequence_text, tgt_sequence_text = get_sample(block_size, tokenizer(text))\n",
        "\n",
        "    # Convert source and target sequences to tokenized vocabulary indices\n",
        "    src_sequence_indices = vocab(src_sequence_text)\n",
        "    tgt_sequence_indices = vocab(tgt_sequence_text)\n",
        "\n",
        "    # Convert the sequences to PyTorch tensors with dtype int64\n",
        "    src_sequence = torch.tensor(src_sequence_indices, dtype=torch.int64)\n",
        "    tgt_sequence = torch.tensor(tgt_sequence_indices, dtype=torch.int64)\n",
        "\n",
        "    # Append the source and target sequences to their respective batches\n",
        "    src_batch.append(src_sequence)\n",
        "    tgt_batch.append(tgt_sequence)\n",
        "\n",
        "    # Print the output for every 2nd sample (adjust as needed)\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(\"Source Sequence (Text):\", src_sequence_text)\n",
        "    print(\"Source Sequence (Indices):\", src_sequence_indices)\n",
        "    print(\"Source Sequence (Shape):\", src_sequence.shape)\n",
        "    print(\"Target Sequence (Text):\", tgt_sequence_text)\n",
        "    print(\"Target Sequence (Indices):\", tgt_sequence_indices)\n",
        "    print(\"Target Sequence (Shape):\", tgt_sequence.shape)"
      ],
      "metadata": {
        "id": "CI7W7G0hI3gE",
        "outputId": "5f626bf7-9ac1-4053-b193-141f8b0c6696",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0:\n",
            "Source Sequence (Text): ['issues', 'in', 'the', 'united', 'states', '.', 'in', 'between', 'asking', 'politicians']\n",
            "Source Sequence (Indices): [1462, 14, 4, 2671, 1768, 3, 14, 259, 1743, 7457]\n",
            "Source Sequence (Shape): torch.Size([10])\n",
            "Target Sequence (Text): ['in', 'the', 'united', 'states', '.', 'in', 'between', 'asking', 'politicians', 'and']\n",
            "Target Sequence (Indices): [14, 4, 2671, 1768, 3, 14, 259, 1743, 7457, 7]\n",
            "Target Sequence (Shape): torch.Size([10])\n",
            "Sample 1:\n",
            "Source Sequence (Text): ['heard', 'that', 'at', 'first', 'it', 'was', 'seized', 'by', 'u', '.']\n",
            "Source Sequence (Indices): [564, 16, 38, 98, 12, 18, 17608, 46, 1466, 3]\n",
            "Source Sequence (Shape): torch.Size([10])\n",
            "Target Sequence (Text): ['that', 'at', 'first', 'it', 'was', 'seized', 'by', 'u', '.', 's']\n",
            "Target Sequence (Indices): [16, 38, 98, 12, 18, 17608, 46, 1466, 3, 17]\n",
            "Target Sequence (Shape): torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The collate_batch function prepares batches of source and target sequences for training by processing each text sample in a given batch. It generates source and target sequences using the get_sample function with a specified block size, converts these sequences to indices using a vocabulary, and transforms them into PyTorch tensors. The sequences are then padded to ensure uniform length across the batch. Finally, it returns the padded source and target batches, ready for training on the specified device (DEVICE)."
      ],
      "metadata": {
        "id": "dAr6Qeu8UwX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BLOCK_SIZE=30\n",
        "def collate_batch(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for _,_textt in batch:\n",
        "      src_sequence,tgt_sequence=get_sample(BLOCK_SIZE,tokenizer(_textt))\n",
        "      src_sequence=vocab(src_sequence)\n",
        "      tgt_sequence=vocab(tgt_sequence)\n",
        "      src_sequence= torch.tensor(src_sequence, dtype=torch.int64)\n",
        "      tgt_sequence = torch.tensor(tgt_sequence, dtype=torch.int64)\n",
        "      src_batch.append(src_sequence)\n",
        "      tgt_batch.append(tgt_sequence)\n",
        "\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "\n",
        "    return src_batch.to(DEVICE), tgt_batch.to(DEVICE)"
      ],
      "metadata": {
        "id": "ta7NkQlcUyhd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code sets up data loaders for the training, validation, and testing sets using the DataLoader class, with each set utilizing a custom collate_batch function for batch processing. The data loaders handle batches of size 1 for simplicity and shuffle the data for randomized access. After initializing the training data loader, it fetches the first batch of source (src) and target (tgt) sequences. It then iterates over each token in the source sequence, converts them back to text using the index_to_en function, and prints the resulting sentences, demonstrating how to access and display preprocessed data ready for model training."
      ],
      "metadata": {
        "id": "6GgndhA4VAra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=1\n",
        "dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_dataloader= DataLoader(val_iter , batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "6b51PEcEU5r7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=iter(dataloader)\n",
        "for sample in range(10):\n",
        "  src,trt=next(dataset)\n",
        "  print(\"sample\",sample)\n",
        "  print(\"sorce:\",index_to_en(src))\n",
        "  print(\"\\n\")\n",
        "  print(\"target:\",index_to_en(trt))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "XX6A3EHEVDzM",
        "outputId": "a9e8b7e1-80d8-4804-fecf-2febca179fbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample 0\n",
            "sorce: that people considering this movie read my comment and decide against it . i ' m all for brutality and shock , but the overall unrealism and truly awful acting\n",
            "\n",
            "\n",
            "target: people considering this movie read my comment and decide against it . i ' m all for brutality and shock , but the overall unrealism and truly awful acting makes\n",
            "\n",
            "\n",
            "sample 1\n",
            "sorce: . i loved s1 , i liked season 2 , season 3 was ok , and loved the final one . yay , there is a spin-off show ! i\n",
            "\n",
            "\n",
            "target: i loved s1 , i liked season 2 , season 3 was ok , and loved the final one . yay , there is a spin-off show ! i didn\n",
            "\n",
            "\n",
            "sample 2\n",
            "sorce: have santa on the payroll . satan ' s dance routine is hilarious . pitch . . . he is so useless . the cheese factor in of this movie\n",
            "\n",
            "\n",
            "target: santa on the payroll . satan ' s dance routine is hilarious . pitch . . . he is so useless . the cheese factor in of this movie is\n",
            "\n",
            "\n",
            "sample 3\n",
            "sorce: , fast and furious 2 also stunk , but i ' d rather see this than ff2 . ) if you have a fetish for harrison ford or that other\n",
            "\n",
            "\n",
            "target: fast and furious 2 also stunk , but i ' d rather see this than ff2 . ) if you have a fetish for harrison ford or that other young\n",
            "\n",
            "\n",
            "sample 4\n",
            "sorce: murders in which the bodies were stolen ! so , she follows some clues all the way to the doorstep of lugosi . lugosi ' s home is complete with\n",
            "\n",
            "\n",
            "target: in which the bodies were stolen ! so , she follows some clues all the way to the doorstep of lugosi . lugosi ' s home is complete with his\n",
            "\n",
            "\n",
            "sample 5\n",
            "sorce: be served with a side-order of gershwin ' s marmalade . clearly the idea , or hope , was to make an unintelligible elizabethan exercise palatable for modern audiences by\n",
            "\n",
            "\n",
            "target: served with a side-order of gershwin ' s marmalade . clearly the idea , or hope , was to make an unintelligible elizabethan exercise palatable for modern audiences by administering\n",
            "\n",
            "\n",
            "sample 6\n",
            "sorce: a dolph spoiler since the scripts are so absurd to begin with a chase scene with a handicapped kid carrying a pistol versus a guy on a harley with a\n",
            "\n",
            "\n",
            "target: dolph spoiler since the scripts are so absurd to begin with a chase scene with a handicapped kid carrying a pistol versus a guy on a harley with a sub-machine\n",
            "\n",
            "\n",
            "sample 7\n",
            "sorce: agonizing to watch . i ' m sorry but i do not see the ' fun ' in this . just the thrill of pointing the many mistakes and stupid\n",
            "\n",
            "\n",
            "target: to watch . i ' m sorry but i do not see the ' fun ' in this . just the thrill of pointing the many mistakes and stupid one-liners\n",
            "\n",
            "\n",
            "sample 8\n",
            "sorce: telling them how much the movie sucks . this film is too slow , this movie is too boring , and this movie is too talky . which wouldn '\n",
            "\n",
            "\n",
            "target: them how much the movie sucks . this film is too slow , this movie is too boring , and this movie is too talky . which wouldn ' t\n",
            "\n",
            "\n",
            "sample 9\n",
            "sorce: i assume this is why , anyway ) , but it is disorienting and really makes the film look shoddier than it had to look . anyway , i '\n",
            "\n",
            "\n",
            "target: assume this is why , anyway ) , but it is disorienting and really makes the film look shoddier than it had to look . anyway , i ' ve\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for  src,trt in dataset:\n",
        "    print(trt.shape)\n",
        "    print(src.shape)\n",
        "    print(index_to_en(src[0,:]))\n",
        "    print(index_to_en(trt[0,:]))\n",
        "    break"
      ],
      "metadata": {
        "id": "a9MwTkh8WLhR",
        "outputId": "7e102e6b-6826-4495-8e4c-8e2ada278782",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 1])\n",
            "torch.Size([30, 1])\n",
            "are\n",
            "in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"source:\",index_to_en(src))\n",
        "print(\"target:\",index_to_en(trt))"
      ],
      "metadata": {
        "id": "i-4Ev6gwWODn",
        "outputId": "cc08734e-f5cf-4506-af4d-2eb3ff218bad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source: are in essence unlikeable , mostly unsympathetic , and frequently cruel to one another . it changes the very nature of certain characters- isabelle , for instance , in the\n",
            "target: in essence unlikeable , mostly unsympathetic , and frequently cruel to one another . it changes the very nature of certain characters- isabelle , for instance , in the novel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masking\n",
        "\n",
        "In transformers, masking is crucial for ensuring certain positions are not attended to. The function generate_square_subsequent_mask produces an upper triangular matrix, which ensures that during decoding, a token can't attend to future tokens of target.\n",
        "\n",
        "How This Works in Transformers\n",
        "\n",
        "Assume we are generating a sequence:\n",
        "\n",
        "Input: [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
        "\n",
        "At each timestep:\n",
        "\n",
        "Token \"A\" can only attend to itself.\n",
        "\n",
        "Token \"B\" can attend to \"A\" but not \"C\", \"D\", \"E\".\n",
        "\n",
        "Token \"C\" can attend to \"A, B\" but not \"D, E\", and so on.\n",
        "\n",
        "This prevents the model from seeing future tokens when generating output during autoregressive decoding."
      ],
      "metadata": {
        "id": "DgLlmjIiWfUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function creates an upper triangular mask for transformer models to prevent a token from attending to future tokens in decoder self-attention.\n",
        "def generate_square_subsequent_mask(sz,device=DEVICE):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask"
      ],
      "metadata": {
        "id": "iTOefFxGWZor"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding Mask (src_padding_mask):\n",
        "\n",
        "Masks out padding tokens in the input, ensuring that the model doesn't attend to these irrelevant tokens.\n",
        "This is crucial for sequences of different lengths, where padding is added to equalize sequence lengths."
      ],
      "metadata": {
        "id": "rKdIQJFpXhea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(src,device=DEVICE):\n",
        "    src_seq_len = src.shape[0]\n",
        "    src_mask = generate_square_subsequent_mask(src_seq_len)\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask,src_padding_mask"
      ],
      "metadata": {
        "id": "HDlvayIkXBQY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace first four tokens with PAD token so we can also check how pad tokens are masked using padding_mask\n",
        "src[0:4]=PAD_IDX"
      ],
      "metadata": {
        "id": "ADEhAMeUXV62"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask,padding_mask = create_mask(src)\n",
        "src"
      ],
      "metadata": {
        "id": "lkvPfSRnXkUO",
        "outputId": "91c7a023-ad33-4028-ef8f-c3e407099005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    1],\n",
              "        [    1],\n",
              "        [    1],\n",
              "        [    1],\n",
              "        [    5],\n",
              "        [  642],\n",
              "        [ 4755],\n",
              "        [    5],\n",
              "        [    7],\n",
              "        [ 3119],\n",
              "        [ 2381],\n",
              "        [   10],\n",
              "        [   37],\n",
              "        [  163],\n",
              "        [    3],\n",
              "        [   12],\n",
              "        [ 1433],\n",
              "        [    4],\n",
              "        [   75],\n",
              "        [ 1130],\n",
              "        [    9],\n",
              "        [  907],\n",
              "        [20601],\n",
              "        [ 6781],\n",
              "        [    5],\n",
              "        [   20],\n",
              "        [ 1642],\n",
              "        [    5],\n",
              "        [   14],\n",
              "        [    4]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional encoding\n",
        "\n",
        "The Transformer model doesn't have built-in knowledge of the order of tokens in the sequence. To give the model this information, positional encodings are added to the embeddings of the tokens. These encodings have a fixed pattern based on their position in the sequence.\n",
        "\n",
        "GPT uses trainable positional encodings. Unlike fixed positional encodings (such as sinusoidal encodings used in the original Transformer paper), trainable positional encodings are learned during the model training process.\n",
        "\n",
        "Trainable positional encodings are implemented as a set of learnable parameters, one for each position in the input sequence. These parameters have the same dimensionality as the token embeddings. During training, the model updates the positional encoding parameters along with the other model parameters to capture the positional information more effectively.\n",
        "\n",
        "The use of trainable positional encodings in GPT allows the model to learn more flexible and task-specific positional representations, potentially improving its performance on various natural language processing tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "3HQa-mBbXx3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add positional information to the input tokens\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
      ],
      "metadata": {
        "id": "FGZyk1IHX5fc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ],
      "metadata": {
        "id": "6OzRy9yMX85F"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token embedding\n",
        "\n",
        "Token embedding, also known as word embedding or word representation, is a way to convert words or tokens from a text corpus into numerical vectors in a continuous vector space. Each unique word or token in the corpus is assigned a fixed-length vector where the numerical values represent various linguistic properties of the word, such as its meaning, context, or relationships with other words.\n",
        "\n",
        "The TokenEmbedding class below converts numerical tokens into embeddings:"
      ],
      "metadata": {
        "id": "LoFLRMStYGj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom GPT model architecture\n",
        "\n",
        "The `CustomGPTModel` class defines a transformer-based model architecture for generative pre-trained models. This model aims to generate text and perform various NLP tasks. Below is an explanation of the main components of the class:\n",
        "\n",
        "- **Initialization (`__init__`)**: The constructor takes several parameters including `embed_size`, `vocab_size`, `num_heads`, `num_layers`, `max_seq_len`, and `dropout`. It initializes the embedding layer, positional encoding, transformer encoder layers, and a linear layer (`lm_head`) for generating logits over the vocabulary.\n",
        "\n",
        "- **Weight initialization (`init_weights`)**: This method initializes the weights of the model for better training convergence. The Xavier uniform initialization is used, which is a common practice for initializing weights in deep learning.\n",
        "\n",
        "- **Decoder (`decoder`)**: Although named `decoder`, this method currently functions as the forward pass through the transformer encoder layers, followed by the generation of logits for the language modeling task. It handles the addition of positional encodings to the embeddings and applies a mask if necessary.\n",
        "\n",
        "- **Forward pass (`forward`)**: This method is similar to the `decoder` method and defines the forward computation of the model. It processes the input through embedding layers, positional encoding, transformer encoder layers, and produces the final output using the `lm_head`.\n",
        "\n",
        "- **Mask generation**: Both `decoder` and `forward` methods contain logic to generate a square causal mask if no source mask is provided. This mask ensures that the prediction for a position does not depend on the future tokens in the sequence, which is important for the autoregressive nature of GPT models.\n",
        "\n",
        "- **Commented out decoder**: A section of the code is commented out, suggesting an initial design where a transformer decoder layer was considered. However, the final implementation uses only encoder layers, which is a common simplification for models focusing on language modeling and generation.\n",
        "\n",
        "This class effectively encapsulates the necessary components to create a GPT-like model, allowing for training on language modeling tasks and text generation applications.\n"
      ],
      "metadata": {
        "id": "E-4FaRZ_YNYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomGPTModel(nn.Module):\n",
        "    def __init__(self, embed_size,vocab_size, num_heads, num_layers, max_seq_len=500,dropout=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.init_weights()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size, dropout=dropout)\n",
        "\n",
        "        print( embed_size )\n",
        "\n",
        "\n",
        "        # Remaining layers are part of the TransformerDecoder\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
        "        self.embed_size = embed_size\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def init_weights(self):\n",
        "      for p in self.parameters():\n",
        "          if p.dim() > 1:\n",
        "              nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def create_mask(src,device=DEVICE):\n",
        "        src_seq_len = src.shape[0]\n",
        "        src_mask = nn.Transformer.generate_square_subsequent_mask(src_seq_len)\n",
        "        src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "        return src_mask,src_padding_mask\n",
        "\n",
        "    def decoder(self, x,src_mask):\n",
        "        seq_length = x.size(0)\n",
        "\n",
        "        # Add positional embeddings to the input embeddings\n",
        "        x = self.embed(x)* math.sqrt(self.embed_size)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        if src_mask is None:\n",
        "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "            \"\"\"\n",
        "            src_mask, src_padding_mask = create_mask(x)\n",
        "\n",
        "        output = self.transformer_encoder(x, src_mask)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "    def forward(self,x,src_mask=None,key_padding_mask=None):\n",
        "\n",
        "        seq_length = x.size(0)\n",
        "\n",
        "        # Add positional embeddings to the input embeddings\n",
        "        x = self.embed(x)* math.sqrt(self.embed_size) #src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "\n",
        "        if src_mask is None:\n",
        "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "            \"\"\"\n",
        "            src_mask, src_padding_mask = create_mask(x)\n",
        "\n",
        "        output = self.transformer_encoder(x, src_mask,key_padding_mask)\n",
        "        x = self.lm_head(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Wojqo15YYJA9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens in each layer?\n",
        "\n",
        "1️⃣ Multi-Head Self-Attention: Understands relationships between tokens.\n",
        "\n",
        "2️⃣ Layer Normalization: Stabilizes training.\n",
        "\n",
        "3️⃣ Feed-Forward Network (FFN): Adds additional transformation capability.\n",
        "\n",
        "4️⃣ Residual Connections: Helps with training deep models.\n",
        "\n",
        "5️⃣ Dropout: Prevents overfitting.\n",
        "\n",
        "Each encoder layer processes the input sequentially, refining token representations before passing them to the next layer."
      ],
      "metadata": {
        "id": "tcdLTGHoZYdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
        "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
        "dropout = 0.2  # dropout probability\n",
        "\n",
        "model = CustomGPTModel(embed_size=emsize, num_heads=nhead, num_layers=nlayers, vocab_size=ntokens,dropout=dropout).to(DEVICE)"
      ],
      "metadata": {
        "id": "xcWsYuN0ZKgp",
        "outputId": "a3b5e158-3308-419b-fce8-341b87c2e212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_prompt(prompt, block_size=BLOCK_SIZE):\n",
        "    # Handle None prompt\n",
        "    while prompt is None:\n",
        "        prompt = input(\"Sorry, prompt cannot be empty. Please enter a valid prompt: \")\n",
        "\n",
        "    tokens = tokenizer(prompt)\n",
        "    number_of_tokens = len(tokens)\n",
        "\n",
        "    # Handle long prompts\n",
        "    if number_of_tokens > block_size:\n",
        "        tokens = tokens[-block_size:]  # Keep last block_size characters\n",
        "\n",
        "    prompt_indices = vocab(tokens)\n",
        "    prompt_encoded = torch.tensor(prompt_indices, dtype=torch.int64).reshape(-1, 1)\n",
        "    return prompt_encoded"
      ],
      "metadata": {
        "id": "BWfIATgXZ3vu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index_to_en(encode_prompt(\"This is a prompt to get model generate next words.\" ) ))"
      ],
      "metadata": {
        "id": "aCQf3jAKZ8aJ",
        "outputId": "dbd77a8c-1797-4cf4-a7f4-1ed52f130812",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a prompt to get model generate next words .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_encoded=encode_prompt(\"This is a prompt to get model generate next words.\").to(DEVICE)\n",
        "prompt_encoded"
      ],
      "metadata": {
        "id": "bywvdwTAZ_FA",
        "outputId": "7368be8c-ac29-4ada-a966-09eceb4e632d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   15],\n",
              "        [   11],\n",
              "        [    6],\n",
              "        [33700],\n",
              "        [   10],\n",
              "        [   86],\n",
              "        [ 2076],\n",
              "        [ 5673],\n",
              "        [  388],\n",
              "        [  665],\n",
              "        [    3]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model.decoder(prompt_encoded,src_mask=None).to(DEVICE)"
      ],
      "metadata": {
        "id": "k2TeCkM2aEO_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits"
      ],
      "metadata": {
        "id": "HDxBQZmJaHlL",
        "outputId": "68919ccc-3e4b-4bd3-a16c-e0166ac6366d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 16.8370,   5.2111,   1.3572,  ...,  -1.0039,  12.0848,  -4.9258]],\n",
              "\n",
              "        [[  1.1735,   0.5255,   8.3361,  ...,  -1.9887,  12.9945,  -4.9213]],\n",
              "\n",
              "        [[-10.3119,  -2.5426,  10.0007,  ...,  17.0549,   5.8150,   1.4574]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-10.3391,  -1.6030,  16.2347,  ...,   4.2710,   2.7438, -14.5902]],\n",
              "\n",
              "        [[ 15.4599,  -7.4686,  16.8885,  ...,  -8.1977,  -0.1991, -14.0434]],\n",
              "\n",
              "        [[ -8.1488, -11.7837,   1.5635,  ...,   7.5134,   9.2038, -12.0769]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = logits.transpose(0, 1)\n",
        "logits.shape"
      ],
      "metadata": {
        "id": "hFLPqeeXafsC",
        "outputId": "c9059136-83d0-4a06-e0df-b93cd94a178e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 11, 68813])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "seq_len = 11 → The input sequence has 11 tokens.\n",
        "\n",
        "batch_size = 1 → Only one sequence is being processed.\n",
        "\n",
        "vocab_size = 68813 → The vocabulary contains 68,813 unique tokens, meaning the model predicts a probability distribution over this many possible tokens at each position."
      ],
      "metadata": {
        "id": "RcT3ws6xbCpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logit_preiction =logits[:,-1]\n",
        "logit_preiction.shape"
      ],
      "metadata": {
        "id": "BL3Wvl4maw99",
        "outputId": "439560fa-6668-467e-9dae-bd99b2075c99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 68813])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " _, next_word_index = torch.max(logit_preiction, dim=1)\n",
        " next_word_index"
      ],
      "metadata": {
        "id": "Pl3CysN8azAb",
        "outputId": "70acb72c-e61b-4a6e-a46c-17086c10fde1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([8756])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoregressive text generation\n",
        "\n",
        "In decoder models, we simply append the output to the input to generate the next response. We stop this process when we encounter the end-of-sequence tag <|endoftext|> or if the input becomes too large. We will implement it as a function later in this notebook.\n"
      ],
      "metadata": {
        "id": "2tUhHqd7bPon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"this is the beginning of\""
      ],
      "metadata": {
        "id": "gHjD2_WpbFmG"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_encoded = encode_prompt(prompt).to(DEVICE)\n",
        "print(\"Device for prompt_encoded:\", prompt_encoded.shape)"
      ],
      "metadata": {
        "id": "3jjmW8aobS4M",
        "outputId": "6cdff0c7-143e-4481-f9d0-885e1886859c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device for prompt_encoded: torch.Size([5, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens=10"
      ],
      "metadata": {
        "id": "mrqpZZeebXdA"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(max_new_tokens):\n",
        "    logits = model.decoder(prompt_encoded,src_mask=None)\n",
        "    logits = logits.transpose(0, 1)\n",
        "    print(\" \")\n",
        "    print(f\"Shape of logits at step {i}: {logits.shape}\")\n",
        "\n",
        "    logit_preiction = logits[:, -1]\n",
        "    print(f\"Shape of logit_prediction at step {i}: {logit_preiction.shape}\")\n",
        "\n",
        "    next_token_encoded = torch.argmax(logit_preiction, dim=-1).reshape(-1, 1)\n",
        "    print(f\"Shape of next_token_encoded at step {i}: {next_token_encoded.shape}\")\n",
        "\n",
        "    prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0).to(DEVICE)\n",
        "    print(f\"Sequence for step {i}: {[index_to_en(j) for j in prompt_encoded]}\")\n",
        "    print(f\"Shape of prompt_encoded after concatenation at step {i}: {prompt_encoded.shape}\")"
      ],
      "metadata": {
        "id": "D9Tz0h0lbaWo",
        "outputId": "bb738387-cbdb-4122-e3bd-e27e0a8a0d0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "Shape of logits at step 0: torch.Size([1, 5, 68813])\n",
            "Shape of logit_prediction at step 0: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 0: torch.Size([1, 1])\n",
            "Sequence for step 0: ['this', 'is', 'the', 'beginning', 'of', 'raising']\n",
            "Shape of prompt_encoded after concatenation at step 0: torch.Size([6, 1])\n",
            " \n",
            "Shape of logits at step 1: torch.Size([1, 6, 68813])\n",
            "Shape of logit_prediction at step 1: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 1: torch.Size([1, 1])\n",
            "Sequence for step 1: ['this', 'is', 'the', 'beginning', 'of', 'raising', 'onna']\n",
            "Shape of prompt_encoded after concatenation at step 1: torch.Size([7, 1])\n",
            " \n",
            "Shape of logits at step 2: torch.Size([1, 7, 68813])\n",
            "Shape of logit_prediction at step 2: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 2: torch.Size([1, 1])\n",
            "Sequence for step 2: ['this', 'is', 'the', 'beginning', 'of', 'raising', 'onna', 'plants']\n",
            "Shape of prompt_encoded after concatenation at step 2: torch.Size([8, 1])\n",
            " \n",
            "Shape of logits at step 3: torch.Size([1, 8, 68813])\n",
            "Shape of logit_prediction at step 3: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 3: torch.Size([1, 1])\n",
            "Sequence for step 3: ['this', 'is', 'the', 'beginning', 'of', 'raising', 'onna', 'plants', 'vegan']\n",
            "Shape of prompt_encoded after concatenation at step 3: torch.Size([9, 1])\n",
            " \n",
            "Shape of logits at step 4: torch.Size([1, 9, 68813])\n",
            "Shape of logit_prediction at step 4: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 4: torch.Size([1, 1])\n",
            "Sequence for step 4: ['this', 'is', 'the', 'beginning', 'of', 'raising', 'onna', 'plants', 'vegan', 'near-death']\n",
            "Shape of prompt_encoded after concatenation at step 4: torch.Size([10, 1])\n",
            " \n",
            "Shape of logits at step 5: torch.Size([1, 10, 68813])\n",
            "Shape of logit_prediction at step 5: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 5: torch.Size([1, 1])\n",
            "Sequence for step 5: ['this', 'is', 'the', 'beginning', 'of', 'raising', 'onna', 'plants', 'vegan', 'near-death', 'folklore']\n",
            "Shape of prompt_encoded after concatenation at step 5: torch.Size([11, 1])\n",
            " \n",
            "Shape of logits at step 6: torch.Size([1, 11, 68813])\n",
            "Shape of logit_prediction at step 6: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 6: torch.Size([1, 1])\n",
            "Sequence for step 6: ['this', 'is', 'the', 'beginning', 'of', 'raising', 'onna', 'plants', 'vegan', 'near-death', 'folklore', 'gilding']\n",
            "Shape of prompt_encoded after concatenation at step 6: torch.Size([12, 1])\n",
            " \n",
            "Shape of logits at step 7: torch.Size([1, 12, 68813])\n",
            "Shape of logit_prediction at step 7: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 7: torch.Size([1, 1])\n",
            "Sequence for step 7: ['this', 'is', 'the', 'beginning', 'of', 'raising', 'onna', 'plants', 'vegan', 'near-death', 'folklore', 'gilding', 'christ/judas']\n",
            "Shape of prompt_encoded after concatenation at step 7: torch.Size([13, 1])\n",
            " \n",
            "Shape of logits at step 8: torch.Size([1, 13, 68813])\n",
            "Shape of logit_prediction at step 8: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 8: torch.Size([1, 1])\n",
            "Sequence for step 8: ['this', 'is', 'the', 'beginning', 'of', 'raising', 'onna', 'plants', 'vegan', 'near-death', 'folklore', 'gilding', 'christ/judas', 'extremly']\n",
            "Shape of prompt_encoded after concatenation at step 8: torch.Size([14, 1])\n",
            " \n",
            "Shape of logits at step 9: torch.Size([1, 14, 68813])\n",
            "Shape of logit_prediction at step 9: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 9: torch.Size([1, 1])\n",
            "Sequence for step 9: ['this', 'is', 'the', 'beginning', 'of', 'raising', 'onna', 'plants', 'vegan', 'near-death', 'folklore', 'gilding', 'christ/judas', 'extremly', 'offices']\n",
            "Shape of prompt_encoded after concatenation at step 9: torch.Size([15, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<|endoftext|>' ]\n",
        "BLOCK_SIZE"
      ],
      "metadata": {
        "id": "fOdjjS9obeH9",
        "outputId": "e1f53097-433f-4128-a2df-67d72f49c8bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#auto-regressive Language Model text generation\n",
        "def generate(model, prompt=None, max_new_tokens=500, block_size=BLOCK_SIZE, vocab=vocab, tokenizer=tokenizer):\n",
        "    # Move model to the specified device (e.g., GPU or CPU)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # Encode the input prompt using the provided encode_prompt function\n",
        "    prompt_encoded = encode_prompt(prompt).to(DEVICE)\n",
        "    tokens = []\n",
        "\n",
        "    # Generate new tokens up to max_new_tokens\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Decode the encoded prompt using the model's decoder\n",
        "        logits = model(prompt_encoded,src_mask=None,key_padding_mask=None)\n",
        "\n",
        "        # Transpose the logits to bring the sequence length to the first dimension\n",
        "        logits = logits.transpose(0, 1)\n",
        "\n",
        "        # Select the logits of the last token in the sequence\n",
        "        logit_prediction = logits[:, -1]\n",
        "\n",
        "        # Choose the most probable next token from the logits(greedy decoding)\n",
        "        next_token_encoded = torch.argmax(logit_prediction, dim=-1).reshape(-1, 1)\n",
        "\n",
        "        # If the next token is the end-of-sequence (EOS) token, stop generation\n",
        "        if next_token_encoded.item() == EOS_IDX:\n",
        "            break\n",
        "\n",
        "        # Append the next token to the prompt_encoded and keep only the last 'block_size' tokens\n",
        "        prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0)[-block_size:]\n",
        "\n",
        "        # Convert the next token index to a token string using the vocabulary\n",
        "        # Move the tensor back to CPU for vocab lookup if needed\n",
        "        token_id = next_token_encoded.to('cpu').item()\n",
        "        tokens.append(vocab.get_itos()[token_id])\n",
        "\n",
        "    # Join the generated tokens into a single string and return\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "1phiBF5XbhYz"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model,prompt=\"this is the beginning of\",max_new_tokens=30,vocab=vocab,tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "cpvIuZGgbjXO",
        "outputId": "000e4faf-7a7b-412a-c381-2d6ddc7b56e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'squabbling self-congratulatory manu dubiel marilu 30 action-films voyna integration sextet disarmed texas almost-real-time organ trade-mark mills swiped condominium suse $177 clichés-only stumped manservant horatio ¨jurassik base midnight- grandchildren nicolas big-screen'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoding the differences: Training vs. inference\n",
        "\n",
        "The key difference between the training and inference stages lies in the inputs to the decoder. During training, the decoder benefits from exposure to the ground truth--receiving the exact target sequence tokens incrementally through a technique known as \"teacher forcing.\" This approach is in stark contrast to some other neural network architectures that rely on the network's previous predictions as inputs during training. Once training concludes, the datasets used resemble those employed in more conventional neural network models, providing a familiar foundation for comparison and evaluation.\n",
        "\n",
        "To start the training, first create a Cross Entropy Loss object. The loss will not consider PAD tokens.\n"
      ],
      "metadata": {
        "id": "CwurNgBxbmgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "lmEa8RB5bsrv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src,tgt=next(iter(dataloader))\n",
        "\n",
        "mask,padding_mask = create_mask(src)"
      ],
      "metadata": {
        "id": "Equ3ArC0bvDF"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model(src,src_mask=mask,key_padding_mask=padding_mask)\n",
        "print(logits.shape)"
      ],
      "metadata": {
        "id": "0tZG2-s3bw13",
        "outputId": "c4e470cd-f6e4-4c6d-d9b8-3321eb4b44e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 1, 68813])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"output shape\",logits.shape)\n",
        "print(\"source shape \",src)"
      ],
      "metadata": {
        "id": "2o4janZBbyA8",
        "outputId": "471aca6c-f8f9-408a-f28a-4aaeaf12475b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output shape torch.Size([30, 1, 68813])\n",
            "source shape  tensor([[    5],\n",
            "        [   26],\n",
            "        [  120],\n",
            "        [    5],\n",
            "        [   16],\n",
            "        [  399],\n",
            "        [   18],\n",
            "        [  391],\n",
            "        [  222],\n",
            "        [    3],\n",
            "        [   14],\n",
            "        [    4],\n",
            "        [ 3425],\n",
            "        [  208],\n",
            "        [  255],\n",
            "        [   15],\n",
            "        [ 3394],\n",
            "        [   19],\n",
            "        [   18],\n",
            "        [   96],\n",
            "        [    5],\n",
            "        [  500],\n",
            "        [  182],\n",
            "        [   37],\n",
            "        [    9],\n",
            "        [    4],\n",
            "        [10937],\n",
            "        [ 3877],\n",
            "        [   16],\n",
            "        [ 4350]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tgt\n",
        "print(tgt.shape)\n",
        "\n",
        "print(logits.reshape(-1, logits.shape[-1]).shape)\n",
        "print(tgt.reshape(-1).shape)\n",
        "\n",
        "loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1))\n",
        "print(loss.item())\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for src,tgt in eval_data:\n",
        "            tgt = tgt.to(DEVICE)\n",
        "            #seq_len = src.size(0)\n",
        "            logits = model(src,src_mask=None,key_padding_mask=None)\n",
        "            total_loss +=  loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1)).item()\n",
        "    return total_loss / (len(list(eval_data)) - 1)\n",
        "\n",
        "evaluate(model,val_dataloader)"
      ],
      "metadata": {
        "id": "R3fY0lvub8T8",
        "outputId": "7a2059e4-dd0f-4f6c-d356-d1bf6e6d1f0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 1])\n",
            "torch.Size([30, 68813])\n",
            "torch.Size([30])\n",
            "38.579898834228516\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35.51883515831356"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "Incorporating the previously outlined steps, we proceed to train the model. Apart from these specific procedures, the overall training process conforms to the conventional methods employed in neural network training.\n",
        "\n",
        "**Please be aware that training the model using CPUs can be a time-consuming process. If you don't have access to GPUs, you can jump to  \"loading the saved model\" and proceed with loading the pre-trained model using the provided code in the subsequent section `Loading the Saved Model`. We have trained the model for 30 epochs and saved it for your convenience.**\n",
        "\n",
        "The `train` function is defined to fine-tune the `CustomGPTModel` on a given training dataset. It is structured as follows:\n",
        "\n",
        "- **Optimizer**: Initializes an ADAM optimizer.\n",
        "\n",
        "Within the `train` function:\n",
        "\n",
        "- The model is set to train mode, which enables dropout and batch normalization layers.\n",
        "- A loop iterates over the training data, which is loaded in batches. For each batch:\n",
        "    - The source (`src`) and target (`tgt`) sequences are extracted.\n",
        "    - The model performs a forward pass to get logits.\n",
        "    - The logits are reshaped for loss calculation.\n",
        "    - The loss is computed using `loss_fn`, which likely refers to a loss function such as cross-entropy that measures the difference between the predicted logits and the target sequences.\n",
        "- Gradient clipping is applied to prevent exploding gradients, which is common in training deep neural networks.\n",
        "- The optimizer updates the model parameters based on the computed gradients.\n",
        "\n",
        "Logging occurs every `10000` steps, or when reaching a specific batch (batch `42060` is hardcoded as an example). During logging:\n",
        "\n",
        "- The average loss and the perplexity (a measure of how well the probability model predicts a sample) are calculated and printed, providing insights into the model's performance.\n",
        "- The elapsed time per batch since the last log interval is measured and reported, giving an indication of training efficiency.\n",
        "\n"
      ],
      "metadata": {
        "id": "clPYY34kb_mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10000, gamma=0.9)\n",
        "\n",
        "def train(model: nn.Module,train_data) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 10000\n",
        "    start_time = time.time()\n",
        "\n",
        "    num_batches = len(list(train_data)) // block_size\n",
        "    for batch,srctgt in enumerate(train_data):\n",
        "        src= srctgt[0]\n",
        "        tgt= srctgt[1]\n",
        "        logits = model(src,src_mask=None)\n",
        "        logits_flat = logits.reshape(-1, logits.shape[-1])\n",
        "        loss = loss_fn(logits_flat, tgt.reshape(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (batch % log_interval == 0 and batch > 0) or batch==42060:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            #cur_loss = total_loss / log_interval\n",
        "            cur_loss = total_loss / batch\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch//block_size:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            start_time = time.time()\n",
        "\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "HO571nL3cCyZ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 30\n",
        "Train_losses= []\n",
        "Val_losses = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train_loss = train(model,dataloader)\n",
        "    val_loss = evaluate(model, val_dataloader)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    Train_losses.append(train_loss)\n",
        "    Val_losses.append(val_loss)\n",
        "\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "        f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'model_best_val_loss.pt')"
      ],
      "metadata": {
        "id": "Nd_OmRxacGvd",
        "outputId": "77736832-1f26-4568-9f7b-9188e580a877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-1aa3ed4db38b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mval_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-e3d8cc5be932>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of epochs (assuming the lengths of train_losses and val_losses are equal)\n",
        "num_epochs = len(Train_losses)\n",
        "\n",
        "# Create a figure and a set of subplots\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot the training losses\n",
        "ax.plot(range(num_epochs), Train_losses, label='Training Loss', color='blue')\n",
        "\n",
        "# Plot the validation losses\n",
        "ax.plot(range(num_epochs), Val_losses, label='Validation Loss', color='orange')\n",
        "\n",
        "# Set the x-axis label\n",
        "ax.set_xlabel('Epoch')\n",
        "\n",
        "# Set the y-axis label\n",
        "ax.set_ylabel('Loss')\n",
        "\n",
        "# Set the title of the plot\n",
        "ax.set_title('Training and Validation Losses')\n",
        "\n",
        "# Add a legend to the plot\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDuCAUQ6cIeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading GPT2 model from HuggingFace\n",
        "Let's now load the GPT2 model from HuggingFace to check how it performs at text generation:\n"
      ],
      "metadata": {
        "id": "0XRvOKDCcLw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and model\n",
        "tokenizer1 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Define the input prompt\n",
        "#input_text = \"Once upon a time in a faraway land,\"\n",
        "input_text = \"the movie was\"\n",
        "\n",
        "# Tokenize the input text and prepare the input for the model\n",
        "input_ids = tokenizer1.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text using the model\n",
        "# Set the desired length of the generated text (max_length),\n",
        "# and other generation parameters like temperature, top_k, and top_p\n",
        "max_length = 15\n",
        "temperature = 0.7\n",
        "top_k = 50\n",
        "top_p = 0.95\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=max_length,\n",
        "    temperature=temperature,\n",
        "    top_k=top_k,\n",
        "    top_p=top_p,\n",
        "    pad_token_id=tokenizer1.eos_token_id,\n",
        ")\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer1.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the input prompt and the generated text\n",
        "print(f\"Input: {input_text}\")\n",
        "print(f\"Generated Text: {generated_text}\")"
      ],
      "metadata": {
        "id": "KpEnk38QcM3v",
        "outputId": "e94c1dbf-04f2-4505-fd80-293fcc63a200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276,
          "referenced_widgets": [
            "db82d4c8aaf7475f92b753aa5016d408",
            "7f49f0dca9264098867187598cfbe4f0",
            "7a222402e36341138bfcf40530fa3699",
            "c5c105d39f7b430489372153eeba49f1",
            "5fe2cf653b6c4cc39d7ad94ad7dc3bdc",
            "dce6cc3b85e14a71b3252145fa01cd30",
            "3abbff565c5148a0a6f1a05c499920bc",
            "9df0feeea2164139b9dbf610ca306059",
            "052d46c94a3844da9b4bc22c0ac70607",
            "71a6e8c6e6a846789b911cce2ecee140",
            "023953a88edd48c0b29735acf4849e15",
            "b4db28fc80df4104a9846e2b40fe9217",
            "74f4af05a6f5412dbc467678f1faec20",
            "dd4b9a2eff464e9186e73533bf2401cf",
            "cff7f80d1895401e8520b1361582bb67",
            "d0d75cff788c4ef9a943aa76e591e32c",
            "186268acc1b046438cb9e6c01eecfddb",
            "d1271f4c87924aada3e4f316e4bed522",
            "d5044e80074e40f993c3c63823274b47",
            "7b784cbe9f7d44a6bcbe77ce4eb24fd5",
            "00d61b7e78b84a29a315c2da325e2b38",
            "0c5b2e1acd904f019f4f088306331591",
            "1d2ca86d46a747fcb5832411c7e51a49",
            "390182c7ee4f44a88357f329d05ff028",
            "bf729c05ca464b93869f4117bfc023f2",
            "dea309e888c1440bb70c674d9172c24a",
            "b9134cee714a4eec808b97c35486970a",
            "5bfb273f65f64b879c16e980bdda226d",
            "7fe82e7b8060464f96673f7fa473102d",
            "f5a46db35e1f426dbba90507bbda27b3",
            "9cb2bf3500d24860956309e1c279ab60",
            "2c38a4d1f8ff4b3498f05df8cecc4557",
            "7cdcb10e9273421286913c01bac0458d",
            "10c27fb4546445769541fba44af2b4ae",
            "750aacd9a1e7460fbce5313e59d23adb",
            "d8051bb9ad7045a0aecfb88d28630318",
            "cb5caa27ff894ed6aa7ea0d78d86e463",
            "c9bb9da11e6448ceb767c3b54180dc99",
            "9201af1398b04d8e9e9d7ccb9f343c91",
            "898c7336de7f46a2b3e8d9058de9595f",
            "ab1f3a500cc946ebb815bf2a13713174",
            "27ce1017d76b481fa0a2d88d5448ef78",
            "f5f11f068fab49a58ca7c5e66013c8c0",
            "7a194ecf68f743c0b9b9fb8bd49addf9",
            "f0f9822e7f264b8d80247f40e35a1a7c",
            "3bea6153fc494d008a60e7e9bb6f23ce",
            "a8634ebfb34f4153957130c82b074731",
            "35dde6a299094675bb85b7b7697d1012",
            "f8fbd4dc0ae246ad965e2631599b5ba6",
            "a7672d15eccd46eca5a369a783c69de8",
            "7c160b61a590455fae29db5d30b16b78",
            "4676df983ff24be29e26ba7805c52cce",
            "1b3ddc2071f14b9eba23d4eae9a9be25",
            "efffb75dbf254896b41ee087b4e1e8c2",
            "91b4e43a7d9446e286b5191eccb135a3",
            "33d801022019449caf76ce5d0ff6f6e3",
            "6a5c64cee7404ea0988387f006368296",
            "ab88d15e712147ad97c1ce3e7d3796e6",
            "31a0273793d84756ac2d048c230f3ec3",
            "168836d1147247e19e224352646db2e6",
            "9074c0584ea64dfda10bcece65e8b1da",
            "29c4c3e5facd419290f62441019bc8ac",
            "3685aa3f93344c539e57818f14cbb645",
            "1076d828c8a24e33b7e898c3c3269135",
            "4556692ee646458b896e620da398c779",
            "05c0fed010d94d1e9533109fe5f86600",
            "66b45ba9e49f4db4941da15164d2c0b1",
            "f6f0fc1a772c474cad737d34993786d7",
            "6ad42b7d71df417dbdf15a41ad333adf",
            "0ca15fa4812c4dd4866ad04e8cec4966",
            "789445562b0c42d2baf20164960442d8",
            "c75f47acc6424ab48528926857e39200",
            "b84b25281dab49b8b1b849259b32fdb4",
            "e3cfe0ff1f8c4be69b0425b4cbfc04a5",
            "a61a0868bdbc4f5ba02246c92a18d189",
            "fec9d56fe8124c2192bdb657364f45bf",
            "033e2212cee7471397597c6a1fcfd291"
          ]
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db82d4c8aaf7475f92b753aa5016d408"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4db28fc80df4104a9846e2b40fe9217"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d2ca86d46a747fcb5832411c7e51a49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10c27fb4546445769541fba44af2b4ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0f9822e7f264b8d80247f40e35a1a7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33d801022019449caf76ce5d0ff6f6e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66b45ba9e49f4db4941da15164d2c0b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: the movie was\n",
            "Generated Text: the movie was a bit of a disappointment, but it was a great movie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "1️⃣ Encoder vs. Decoder in Transformers\n",
        "\n",
        "The Transformer model, introduced in the research paper \"Attention Is All You Need\", has two main components:\n",
        "\n",
        "Encoder: Used in models like BERT, processes the entire input at once.\n",
        "Decoder: Used in models like GPT, generates text one token at a time.\n",
        "\n",
        "2️⃣ Key Difference Between Encoder and Decoder\n",
        "\n",
        "The decoder is similar to the encoder in structure, but it has an extra component:\n",
        "\n",
        "Masked Multi-Head Self-Attention.\n",
        "\n",
        "This masking ensures that the model cannot see future tokens when generating text.\n",
        "\n",
        "Without masking, the model could \"peek\" at the future words, which would break the idea of autoregressive text generation.\n",
        "\n",
        "3️⃣ What is Masked Multi-Head Self-Attention?\n",
        "\n",
        "Multi-Head Attention is a mechanism that allows the model to focus on different parts of the input when predicting the next word.\n",
        "\n",
        "Masking ensures causality:\n",
        "At each step, the model can only see the past and present tokens, but not future tokens.\n",
        "\n",
        "This is done by adding -∞ to the softmax layer, forcing attention weights for future tokens to be zero.\n",
        "\n",
        "This prevents data leakage, ensuring that text is generated one token at a time.\n",
        "🔹 Example of Masking: If the input is \"The movie was amazing\", the model should only attend to \"The\", \"movie\", and \"was\" when predicting the next word. It should not see \"amazing\" before generating it.\n",
        "\n",
        "4️⃣ How the Decoder Works\n",
        "\n",
        "The decoder follows these steps:\n",
        "\n",
        "1️⃣ Takes an input query (Q):\n",
        "\n",
        "If it's used without an encoder (like in GPT), the Q comes from the decoder itself.\n",
        "\n",
        "If it's used with an encoder (like in translation models), Q comes from the decoder, but the Key (K) and Value (V) come from the encoder.\n",
        "\n",
        "2️⃣ Processes Input with Multi-Head Attention:\n",
        "\n",
        "First, it applies Masked Self-Attention to process previously generated words.\n",
        "Then, it applies Encoder-Decoder Attention (if applicable, like in translation).\n",
        "\n",
        "3️⃣ Performs Layer Normalization & Feed-Forward Processing:\n",
        "\n",
        "Normalizes the outputs and applies a feed-forward network for better representation.\n",
        "\n",
        "4️⃣ Predicts the Next Token:\n",
        "\n",
        "The final output is a probability distribution over the vocabulary (e.g., 50,000 words).\n",
        "\n",
        "The most likely token is selected using methods like greedy search, nucleus sampling, or beam search.\n",
        "\n",
        "5️⃣ Decoder-Only Models (GPT Family)\n",
        "\n",
        "Models like GPT-2, GPT-3, GPT-4, Gemini LM, etc. only use the decoder part.\n",
        "These models are called Autoregressive Models because they generate text one token at a time.\n",
        "\n",
        "Unlike BERT, which encodes all input at once, GPT predicts one token at a time and feeds it back as input.\n",
        "\n",
        "🔹 Example of Autoregressive Generation:\n",
        "\n",
        "Step 1: Start with \"The movie was\"\n",
        "\n",
        "Step 2: GPT predicts \"amazing\"\n",
        "\n",
        "Step 3: Now input becomes \"The movie was amazing\"\n",
        "\n",
        "Step 4: GPT predicts \"!\", making the final output \"The movie was amazing!\"\n",
        "\n",
        "The process repeats until the desired length is reached.\n",
        "\n",
        "6️⃣ Embeddings & Positional Encoding\n",
        "Before feeding text into the decoder, the model converts words into embeddings.\n",
        "Positional encoding is added so that the model understands word order.\n",
        "Uses sinusoidal functions (sine and cosine) to encode position information.\n",
        "\n",
        "7️⃣ How the Model Chooses Words\n",
        "\n",
        "After predicting probabilities for all words in the vocabulary, it selects words using different strategies:\n",
        "\n",
        "Method\tDescription\n",
        "\n",
        "Greedy Search\tPicks the word with the highest probability at each step (can be repetitive).\n",
        "\n",
        "Beam Search\tExplores multiple possibilities and selects the best sequence.\n",
        "\n",
        "Top-K Sampling\tPicks from the top K most probable words (adds randomness).\n",
        "\n",
        "Top-P Sampling (Nucleus Sampling)\tChooses words from the smallest set whose probabilities sum to at least p (e.g., 95%).\n",
        "\n",
        "# How Text Generation Works Step-by-Step\n",
        "\n",
        "1️⃣ Start with an input sentence: \"The movie was\".\n",
        "\n",
        "2️⃣ Tokenize and embed it.\n",
        "\n",
        "3️⃣ Pass through masked self-attention in the decoder.\n",
        "\n",
        "4️⃣ Predict the next token (e.g., \"amazing\").\n",
        "\n",
        "5️⃣ Add the predicted token back into input (\"The movie was amazing\").\n",
        "\n",
        "6️⃣ Repeat steps 3-5 until reaching the desired text length.\n",
        "\n",
        "🔹 This loop continues, generating text token by token.\n",
        "\n"
      ],
      "metadata": {
        "id": "JGDT7IlmeHsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1️⃣ What is Top-K Sampling?\n",
        "\n",
        "🔹 \"Keep the K most probable words and ignore the rest\"\n",
        "\n",
        "Instead of picking from all words, it only considers the top K most probable words at each step.\n",
        "\n",
        "Example of Top-K Sampling\n",
        "\n",
        "Imagine GPT-2 is generating text and needs to predict the next word after:\n",
        "📝 \"The food was\"\n",
        "\n",
        "The model gives probabilities for the next word:\n",
        "\n",
        "Word\tProbability\n",
        "\n",
        "delicious\t40%\n",
        "\n",
        "amazing\t30%\n",
        "\n",
        "terrible\t15%\n",
        "\n",
        "overpriced\t10%\n",
        "\n",
        "blue\t5%\n",
        "\n",
        "dog\t0.1%\n",
        "\n",
        "spaceship\t0.01%\n",
        "\n",
        "🔹 Top-K with K=3 → Keep only the top 3 words: ✅ \"delicious\", \"amazing\",\n",
        "\"terrible\"\n",
        "\n",
        "❌ \"overpriced\", \"blue\", \"dog\", \"spaceship\" (discarded)\n",
        "\n",
        "👉 The model picks randomly among the top 3 words, making the text more diverse.\n",
        "\n",
        "\n",
        " 2️⃣ What is Top-P (Nucleus Sampling)?\n",
        "🔹 \"Keep the smallest set of words whose probabilities sum to at least P\"\n",
        "\n",
        "Instead of choosing a fixed K, this method dynamically selects the top words until their combined probability reaches a threshold (e.g., 0.95 or 95%).\n",
        "Example of Top-P Sampling (P = 0.95)\n",
        "\n",
        "Using the same scenario (\"The food was\"), here are the probabilities of different words:\n",
        "\n",
        "Word\tProbability\tCumulative Probability\n",
        "\n",
        "delicious\t40%\t40%\n",
        "\n",
        "amazing\t30%\t70%\n",
        "\n",
        "terrible\t15%\t85%\n",
        "\n",
        "overpriced\t10%\t95% ✅ (Stop here)\n",
        "\n",
        "blue\t5%\t100% ❌ (Ignored)\n",
        "\n",
        "🔹 Top-P with P=0.95 → Keep words until total probability reaches 95%:\n",
        "\n",
        "✅ \"delicious\", \"amazing\", \"terrible\", \"overpriced\"\n",
        "\n",
        "❌ \"blue\", \"dog\", \"spaceship\" (discarded)\n",
        "\n",
        "👉 Unlike Top-K, the number of words considered is not fixed!\n",
        "👉 It varies depending on the probability distribution of the words.\n",
        "\n",
        "\n",
        "\n",
        "Which method is better?\n",
        "\n",
        "✅ Use Top-K when you want consistent word selection (e.g., K=50).\n",
        "\n",
        "✅ Use Top-P when you want a flexible, probability-based approach (e.g., P=0.9)."
      ],
      "metadata": {
        "id": "CWhmZXq1fgFV"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db82d4c8aaf7475f92b753aa5016d408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f49f0dca9264098867187598cfbe4f0",
              "IPY_MODEL_7a222402e36341138bfcf40530fa3699",
              "IPY_MODEL_c5c105d39f7b430489372153eeba49f1"
            ],
            "layout": "IPY_MODEL_5fe2cf653b6c4cc39d7ad94ad7dc3bdc"
          }
        },
        "7f49f0dca9264098867187598cfbe4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dce6cc3b85e14a71b3252145fa01cd30",
            "placeholder": "​",
            "style": "IPY_MODEL_3abbff565c5148a0a6f1a05c499920bc",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7a222402e36341138bfcf40530fa3699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9df0feeea2164139b9dbf610ca306059",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_052d46c94a3844da9b4bc22c0ac70607",
            "value": 26
          }
        },
        "c5c105d39f7b430489372153eeba49f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a6e8c6e6a846789b911cce2ecee140",
            "placeholder": "​",
            "style": "IPY_MODEL_023953a88edd48c0b29735acf4849e15",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.68kB/s]"
          }
        },
        "5fe2cf653b6c4cc39d7ad94ad7dc3bdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dce6cc3b85e14a71b3252145fa01cd30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3abbff565c5148a0a6f1a05c499920bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9df0feeea2164139b9dbf610ca306059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "052d46c94a3844da9b4bc22c0ac70607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71a6e8c6e6a846789b911cce2ecee140": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "023953a88edd48c0b29735acf4849e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4db28fc80df4104a9846e2b40fe9217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74f4af05a6f5412dbc467678f1faec20",
              "IPY_MODEL_dd4b9a2eff464e9186e73533bf2401cf",
              "IPY_MODEL_cff7f80d1895401e8520b1361582bb67"
            ],
            "layout": "IPY_MODEL_d0d75cff788c4ef9a943aa76e591e32c"
          }
        },
        "74f4af05a6f5412dbc467678f1faec20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_186268acc1b046438cb9e6c01eecfddb",
            "placeholder": "​",
            "style": "IPY_MODEL_d1271f4c87924aada3e4f316e4bed522",
            "value": "vocab.json: 100%"
          }
        },
        "dd4b9a2eff464e9186e73533bf2401cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5044e80074e40f993c3c63823274b47",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b784cbe9f7d44a6bcbe77ce4eb24fd5",
            "value": 1042301
          }
        },
        "cff7f80d1895401e8520b1361582bb67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00d61b7e78b84a29a315c2da325e2b38",
            "placeholder": "​",
            "style": "IPY_MODEL_0c5b2e1acd904f019f4f088306331591",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 14.3MB/s]"
          }
        },
        "d0d75cff788c4ef9a943aa76e591e32c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "186268acc1b046438cb9e6c01eecfddb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1271f4c87924aada3e4f316e4bed522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5044e80074e40f993c3c63823274b47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b784cbe9f7d44a6bcbe77ce4eb24fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00d61b7e78b84a29a315c2da325e2b38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c5b2e1acd904f019f4f088306331591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d2ca86d46a747fcb5832411c7e51a49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_390182c7ee4f44a88357f329d05ff028",
              "IPY_MODEL_bf729c05ca464b93869f4117bfc023f2",
              "IPY_MODEL_dea309e888c1440bb70c674d9172c24a"
            ],
            "layout": "IPY_MODEL_b9134cee714a4eec808b97c35486970a"
          }
        },
        "390182c7ee4f44a88357f329d05ff028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bfb273f65f64b879c16e980bdda226d",
            "placeholder": "​",
            "style": "IPY_MODEL_7fe82e7b8060464f96673f7fa473102d",
            "value": "merges.txt: 100%"
          }
        },
        "bf729c05ca464b93869f4117bfc023f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5a46db35e1f426dbba90507bbda27b3",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cb2bf3500d24860956309e1c279ab60",
            "value": 456318
          }
        },
        "dea309e888c1440bb70c674d9172c24a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c38a4d1f8ff4b3498f05df8cecc4557",
            "placeholder": "​",
            "style": "IPY_MODEL_7cdcb10e9273421286913c01bac0458d",
            "value": " 456k/456k [00:00&lt;00:00, 4.63MB/s]"
          }
        },
        "b9134cee714a4eec808b97c35486970a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bfb273f65f64b879c16e980bdda226d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fe82e7b8060464f96673f7fa473102d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5a46db35e1f426dbba90507bbda27b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cb2bf3500d24860956309e1c279ab60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c38a4d1f8ff4b3498f05df8cecc4557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cdcb10e9273421286913c01bac0458d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10c27fb4546445769541fba44af2b4ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_750aacd9a1e7460fbce5313e59d23adb",
              "IPY_MODEL_d8051bb9ad7045a0aecfb88d28630318",
              "IPY_MODEL_cb5caa27ff894ed6aa7ea0d78d86e463"
            ],
            "layout": "IPY_MODEL_c9bb9da11e6448ceb767c3b54180dc99"
          }
        },
        "750aacd9a1e7460fbce5313e59d23adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9201af1398b04d8e9e9d7ccb9f343c91",
            "placeholder": "​",
            "style": "IPY_MODEL_898c7336de7f46a2b3e8d9058de9595f",
            "value": "tokenizer.json: 100%"
          }
        },
        "d8051bb9ad7045a0aecfb88d28630318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab1f3a500cc946ebb815bf2a13713174",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27ce1017d76b481fa0a2d88d5448ef78",
            "value": 1355256
          }
        },
        "cb5caa27ff894ed6aa7ea0d78d86e463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5f11f068fab49a58ca7c5e66013c8c0",
            "placeholder": "​",
            "style": "IPY_MODEL_7a194ecf68f743c0b9b9fb8bd49addf9",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 21.7MB/s]"
          }
        },
        "c9bb9da11e6448ceb767c3b54180dc99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9201af1398b04d8e9e9d7ccb9f343c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "898c7336de7f46a2b3e8d9058de9595f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab1f3a500cc946ebb815bf2a13713174": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27ce1017d76b481fa0a2d88d5448ef78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5f11f068fab49a58ca7c5e66013c8c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a194ecf68f743c0b9b9fb8bd49addf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0f9822e7f264b8d80247f40e35a1a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bea6153fc494d008a60e7e9bb6f23ce",
              "IPY_MODEL_a8634ebfb34f4153957130c82b074731",
              "IPY_MODEL_35dde6a299094675bb85b7b7697d1012"
            ],
            "layout": "IPY_MODEL_f8fbd4dc0ae246ad965e2631599b5ba6"
          }
        },
        "3bea6153fc494d008a60e7e9bb6f23ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7672d15eccd46eca5a369a783c69de8",
            "placeholder": "​",
            "style": "IPY_MODEL_7c160b61a590455fae29db5d30b16b78",
            "value": "config.json: 100%"
          }
        },
        "a8634ebfb34f4153957130c82b074731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4676df983ff24be29e26ba7805c52cce",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b3ddc2071f14b9eba23d4eae9a9be25",
            "value": 665
          }
        },
        "35dde6a299094675bb85b7b7697d1012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efffb75dbf254896b41ee087b4e1e8c2",
            "placeholder": "​",
            "style": "IPY_MODEL_91b4e43a7d9446e286b5191eccb135a3",
            "value": " 665/665 [00:00&lt;00:00, 38.5kB/s]"
          }
        },
        "f8fbd4dc0ae246ad965e2631599b5ba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7672d15eccd46eca5a369a783c69de8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c160b61a590455fae29db5d30b16b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4676df983ff24be29e26ba7805c52cce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b3ddc2071f14b9eba23d4eae9a9be25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "efffb75dbf254896b41ee087b4e1e8c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91b4e43a7d9446e286b5191eccb135a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33d801022019449caf76ce5d0ff6f6e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a5c64cee7404ea0988387f006368296",
              "IPY_MODEL_ab88d15e712147ad97c1ce3e7d3796e6",
              "IPY_MODEL_31a0273793d84756ac2d048c230f3ec3"
            ],
            "layout": "IPY_MODEL_168836d1147247e19e224352646db2e6"
          }
        },
        "6a5c64cee7404ea0988387f006368296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9074c0584ea64dfda10bcece65e8b1da",
            "placeholder": "​",
            "style": "IPY_MODEL_29c4c3e5facd419290f62441019bc8ac",
            "value": "model.safetensors: 100%"
          }
        },
        "ab88d15e712147ad97c1ce3e7d3796e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3685aa3f93344c539e57818f14cbb645",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1076d828c8a24e33b7e898c3c3269135",
            "value": 548105171
          }
        },
        "31a0273793d84756ac2d048c230f3ec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4556692ee646458b896e620da398c779",
            "placeholder": "​",
            "style": "IPY_MODEL_05c0fed010d94d1e9533109fe5f86600",
            "value": " 548M/548M [00:08&lt;00:00, 153MB/s]"
          }
        },
        "168836d1147247e19e224352646db2e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9074c0584ea64dfda10bcece65e8b1da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29c4c3e5facd419290f62441019bc8ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3685aa3f93344c539e57818f14cbb645": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1076d828c8a24e33b7e898c3c3269135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4556692ee646458b896e620da398c779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05c0fed010d94d1e9533109fe5f86600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66b45ba9e49f4db4941da15164d2c0b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6f0fc1a772c474cad737d34993786d7",
              "IPY_MODEL_6ad42b7d71df417dbdf15a41ad333adf",
              "IPY_MODEL_0ca15fa4812c4dd4866ad04e8cec4966"
            ],
            "layout": "IPY_MODEL_789445562b0c42d2baf20164960442d8"
          }
        },
        "f6f0fc1a772c474cad737d34993786d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c75f47acc6424ab48528926857e39200",
            "placeholder": "​",
            "style": "IPY_MODEL_b84b25281dab49b8b1b849259b32fdb4",
            "value": "generation_config.json: 100%"
          }
        },
        "6ad42b7d71df417dbdf15a41ad333adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3cfe0ff1f8c4be69b0425b4cbfc04a5",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a61a0868bdbc4f5ba02246c92a18d189",
            "value": 124
          }
        },
        "0ca15fa4812c4dd4866ad04e8cec4966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec9d56fe8124c2192bdb657364f45bf",
            "placeholder": "​",
            "style": "IPY_MODEL_033e2212cee7471397597c6a1fcfd291",
            "value": " 124/124 [00:00&lt;00:00, 6.66kB/s]"
          }
        },
        "789445562b0c42d2baf20164960442d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c75f47acc6424ab48528926857e39200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b84b25281dab49b8b1b849259b32fdb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3cfe0ff1f8c4be69b0425b4cbfc04a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a61a0868bdbc4f5ba02246c92a18d189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fec9d56fe8124c2192bdb657364f45bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "033e2212cee7471397597c6a1fcfd291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}