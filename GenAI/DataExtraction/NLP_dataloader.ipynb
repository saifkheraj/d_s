{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THyksMuqlKi4"
   },
   "source": [
    "### Hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bMbcQpjlKi5",
    "outputId": "c41c9fd7-2e8e-4e93-c70a-e51fc12e8826"
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.0.1 torchtext==0.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Nu6ILNjTY4B",
    "outputId": "17a7abc8-933a-40d1-b4a3-98aa9c80ba0d"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imXeI0KPnMY0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
    "import torchtext\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkG4ZDqUl8nd",
    "outputId": "54d18169-c102-4db4-de3a-c0acdd459c2a"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mMkQAqonVWG"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "A data set in PyTorch is an object that represents a collection of data samples. Each data sample consists of one or more input features and their  target labels.\n",
    "\n",
    "### Dataloader\n",
    "- For efficiently loading and batching data from a data set.\n",
    "- Data loaders parameters, includes data set to load from, batch size (determining how many samples per batch), shuffle (whether to shuffle the data for each epoch), and more. Data loaders also provide an iterator interface, making it easy to iterate over batches of data during training.\n",
    "- In PyTorch, not all data sets are iterators, but all data loaders are.\n",
    "- Purpose is to convert input data and labels into batches of tensors with the same shape for deep learning models to interpret.\n",
    "- It can be used for tasks such as tokenizing, sequencing, converting your samples to the same size, and transforming your data into tensors that your model can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97vMy1wH-SGD"
   },
   "source": [
    "We can use custom data set called CustomDataset. This data set inherits from the `torch.utils.data.Dataset` class and is initialized with a list of sentences. This comprises of two essential methods:\n",
    "\n",
    "- __init__(self, sentences): Initializes the data set with a list of sentences.\n",
    "- __getitem__(self, idx): Retrieves an item (in this case, a sentence) at a specific index, idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grlxnBBK9ruS"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "\n",
    "#The name in parentheses (Dataset) specifies that the new class CustomDataset is inheriting all the properties and methods of the Dataset class.\n",
    "#This is Pythonâ€™s standard way of defining inheritance.\n",
    "#Even though Dataset provides a general framework, the CustomDataset class overrides its behavior by implementing its own versions of __len__ and __getitem__ methods. These methods are customized to handle the sentences data.\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "\n",
    "# Create an instance of your custom dataset\n",
    "custom_dataset = CustomDataset(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tJy4tEfJ-zlJ",
    "outputId": "eed550fe-e389-4eb1-8db8-e0077be6dffa"
   },
   "outputs": [],
   "source": [
    "custom_dataset.__getitem__(1) ## second sentence in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXQBs283_Gp0",
    "outputId": "307805af-3210-4d7f-b97c-2768cefd2718"
   },
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 2\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2TjocOLBwUv"
   },
   "outputs": [],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNcpc5ckCVIn"
   },
   "source": [
    "Next step is to convert these sentences into tensors. Let's see how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8mjGAe8HCY27",
    "outputId": "ce2d3d84-5b6a-49f3-a20d-8aaee51c1afc"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "\n",
    "# Define a custom data set\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, vocab):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.sentences[idx])\n",
    "        # Convert tokens to tensor indices using vocab\n",
    "        tensor_indices = [self.vocab[token] for token in tokens]\n",
    "        return torch.tensor(tensor_indices)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, sentences))\n",
    "\n",
    "# Create an instance of your custom data set\n",
    "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
    "\n",
    "print(\"Custom Dataset Length:\", len(custom_dataset))\n",
    "print(\"Sample Items:\")\n",
    "for i in range(6):\n",
    "    sample_item = custom_dataset[i]\n",
    "    print(f\"Item {i + 1}: {sample_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "UaYuvt-GCyFL",
    "outputId": "0b07419c-2174-4371-af07-cd356584e8a3"
   },
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 2\n",
    "\n",
    "# Create a data loader\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through the data loader\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8_DyJaJC_2T"
   },
   "source": [
    "This error arises because the tensor batches have unequal lengths. The data loader is using the default collate_function, which requires tensors to have equal lengths. We can define our own collate_function and pass the data into it to establish your own rules. Typically, to address the issue of unequal tensor lengths,  employ data padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ld7CPK51DGVZ"
   },
   "outputs": [],
   "source": [
    "# Create a custom collate function\n",
    "def collate_fn(batch):\n",
    "    # Pad sequences within the batch to have equal lengths\n",
    "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    return padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zDsaynXDYFO",
    "outputId": "2227f353-e654-4097-9a03-1dfa4752d529"
   },
   "outputs": [],
   "source": [
    "# Create a data loader with the custom collate function with batch_first=True,\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Iterate through the data loader\n",
    "for batch in dataloader:\n",
    "    for row in batch:\n",
    "        for idx in row:\n",
    "            words = [vocab.get_itos()[idx] for idx in row]\n",
    "        print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSOdvRnlDbNE",
    "outputId": "1ad60761-e7c1-435f-80d9-72108f1e9884"
   },
   "outputs": [],
   "source": [
    "# Iterate through the data loader with batch_first = TRUE\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    print(\"Length of sequences in the batch:\",batch.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egdCniYoEhyV"
   },
   "source": [
    "each batch has a fixed size for all the sequences within the batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POrGvc4bEetD"
   },
   "source": [
    "\n",
    "We also have the option to utilize the collate function for tasks such as tokenization, converting tokenized indices, and transforming the result into a tensor. It's important to note that the original data set remains untouched by these transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ojj10RoCEi8e"
   },
   "outputs": [],
   "source": [
    "# Define a custom data set\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkc2Zd5oD2-K"
   },
   "outputs": [],
   "source": [
    "custom_dataset=CustomDataset(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "UmWXGvp0FJDR",
    "outputId": "07b14824-a1d7-46d9-c28d-aafbd249a852"
   },
   "outputs": [],
   "source": [
    "custom_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wch1KebDFLMX"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Tokenize each sample in the batch using the specified tokenizer\n",
    "    tensor_batch = []\n",
    "    for sample in batch:\n",
    "        tokens = tokenizer(sample)\n",
    "        # Convert tokens to vocabulary indices and create a tensor for each sample\n",
    "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "\n",
    "    # Pad sequences within the batch to have equal lengths using pad_sequence\n",
    "    # batch_first=True ensures that the tensors have shape (batch_size, max_sequence_length)\n",
    "    padded_batch = pad_sequence(tensor_batch, batch_first=True)\n",
    "\n",
    "    # Return the padded batch\n",
    "    return padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGYO1IsIFV00"
   },
   "outputs": [],
   "source": [
    "# Create a data loader for the custom dataset\n",
    "dataloader = DataLoader(\n",
    "    dataset=custom_dataset,   # Custom PyTorch Dataset containing your data\n",
    "    batch_size=batch_size,     # Number of samples in each mini-batch\n",
    "    shuffle=True,              # Shuffle the data at the beginning of each epoch\n",
    "    collate_fn=collate_fn      # Custom collate function for processing batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmJhovCxFZWV",
    "outputId": "4f6dd713-0f47-4f19-acc0-2e92a13b0371"
   },
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    print(\"shape of sample\",len(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQu6zgPQFg69"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUjJwlDVQhwZ"
   },
   "source": [
    "a data loader with a collate function that processes batches of French text (provided below). Sort the data set on sequences length. Then tokenize, numericalize and pad the sequences. Sorting the sequences will minimize the number of <PAD>tokens added to the sequences, which enhances the model's performance. Prepare the data in batches of size 4 and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWlJZPLLao64"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example French corpus\n",
    "corpus = [\n",
    "    \"Ceci est une phrase.\",\n",
    "    \"C'est un autre exemple de phrase.\",\n",
    "    \"Voici une troisiÃ¨me phrase.\",\n",
    "    \"Il fait beau aujourd'hui.\",\n",
    "    \"J'aime beaucoup la cuisine franÃ§aise.\",\n",
    "    \"Quel est ton plat prÃ©fÃ©rÃ© ?\",\n",
    "    \"Je t'adore.\",\n",
    "    \"Bon appÃ©tit !\",\n",
    "    \"Je suis en train d'apprendre le franÃ§ais.\",\n",
    "    \"Nous devons partir tÃ´t demain matin.\",\n",
    "    \"Je suis heureux.\",\n",
    "    \"Le film Ã©tait vraiment captivant !\",\n",
    "    \"Je suis lÃ .\",\n",
    "    \"Je ne sais pas.\",\n",
    "    \"Je suis fatiguÃ© aprÃ¨s une longue journÃ©e de travail.\",\n",
    "    \"Est-ce que tu as des projets pour le week-end ?\",\n",
    "    \"Je vais chez le mÃ©decin cet aprÃ¨s-midi.\",\n",
    "    \"La musique adoucit les mÅ“urs.\",\n",
    "    \"Je dois acheter du pain et du lait.\",\n",
    "    \"Il y a beaucoup de monde dans cette ville.\",\n",
    "    \"Merci beaucoup !\",\n",
    "    \"Au revoir !\",\n",
    "    \"Je suis ravi de vous rencontrer enfin !\",\n",
    "    \"Les vacances sont toujours trop courtes.\",\n",
    "    \"Je suis en retard.\",\n",
    "    \"FÃ©licitations pour ton nouveau travail !\",\n",
    "    \"Je suis dÃ©solÃ©, je ne peux pas venir Ã  la rÃ©union.\",\n",
    "    \"Ã€ quelle heure est le prochain train ?\",\n",
    "    \"Bonjour !\",\n",
    "    \"C'est gÃ©nial !\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IPFXsN9FRMtR",
    "outputId": "944dc1e2-8393-4ad6-ab03-a92caf041b96"
   },
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the Camembert tokenizer\n",
    "camembert_tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "# Define a function to tokenize and convert tokens to IDs\n",
    "def tokenize(sentence):\n",
    "    return camembert_tokenizer.encode(sentence, add_special_tokens=False)\n",
    "\n",
    "\n",
    "# Define a collate function for padding\n",
    "def collate_fn_fr(batch):\n",
    "    tensor_batch = []\n",
    "    for sample in batch:\n",
    "        tokens = tokenize(sample)\n",
    "        tensor_batch.append(torch.tensor(tokens))\n",
    "    padded_batch = pad_sequence(tensor_batch, batch_first=True)\n",
    "    return padded_batch\n",
    "\n",
    "# Sort sentences based on their length\n",
    "sorted_data = sorted(corpus, key=lambda x: len(tokenize(x)))\n",
    "\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(sorted_data, batch_size=2, shuffle=False, collate_fn=collate_fn_fr)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in dataloader:\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAI2wWonbz1J",
    "outputId": "3696820b-acd5-4e46-f535-d8871907a558"
   },
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDQiIHmSdZon"
   },
   "source": [
    "### Special symbols\n",
    "In a typical NLP  context, the tokens `['<unk>', '<pad>', '<bos>', '<eos>']` have specific meanings:\n",
    "\n",
    "1. `<unk>`: This token represents \"unknown\" or \"out-of-vocabulary\" words. It is used when a word in the input text is not found in the vocabulary or when dealing with words that are rare or unseen during training. When the model encounters an unknown word, it replaces it with the `<unk>` token.\n",
    "\n",
    "2. `<pad>`: This token represents padding. In sequences of text data, such as sentences or documents, sequences may have different lengths. To create batches of data with uniform dimensions, shorter sequences are often padded with this `<pad>` token to match the length of the longest sequence in the batch.\n",
    "\n",
    "3. `<bos>`: This token represents the \"beginning of sequence.\" It is used to indicate the start of a sentence or sequence of tokens. It helps the model understand the beginning of a text sequence.\n",
    "\n",
    "4. `<eos>`: This token represents the \"end of sequence.\" It is used to indicate the end of a sentence or sequence of tokens. It helps the model recognize the end of a text sequence.\n",
    "\n",
    " Define special symbols and indices\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
