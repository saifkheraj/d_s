{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nW7Raqn14r5",
        "outputId": "2eab0298-42ec-4158-b7d5-ba9c2b97a498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Collecting torchtext==0.15.2\n",
            "  Using cached torchtext-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (2.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Collecting torchdata==0.6.1 (from torchtext==0.15.2)\n",
            "  Using cached torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext==0.15.2) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext==0.15.2) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext==0.15.2) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext==0.15.2) (18.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchtext==0.15.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchtext==0.15.2) (1.3.0)\n",
            "Using cached torchtext-0.15.2-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "Using cached torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "Installing collected packages: torchdata, torchtext\n",
            "Successfully installed torchdata-0.6.1 torchtext-0.15.2\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scikit-learn\n",
        "!pip install torch==2.0.1\n",
        "!pip install torchtext==0.15.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from transformers import BertTokenizer\n",
        "from transformers import XLNetTokenizer\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BWJ1TOv2axH",
        "outputId": "98f1053c-7735-4af2-c36e-eb9249706520"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M5s7oJiU3jkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three types of tokenizer:\n",
        "- word based,\n",
        "- character based,\n",
        "- sub word based.\n",
        "\n",
        "There are different rules for word-based tokenizers, such as splitting on spaces or splitting on punctuation. Each  assigns a specific ID to the split word. Here we will use nltk's  word_tokenize\n"
      ],
      "metadata": {
        "id": "br4y-Rgj3kbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Based using NLTK and spacy"
      ],
      "metadata": {
        "id": "2ndPxMH65ocL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a sample sentence for word tokenization.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inIzKw_n4LOR",
        "outputId": "5cfab2a3-547a-41f4-eddf-66efe65e215d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', 'for', 'word', 'tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kPzJqpn4WnL",
        "outputId": "a93fb89b-f63e-4b42-bfed-1e14efd458a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GpqkDplx5FKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This uses 'spaCy' tokenizer\n",
        "\n",
        "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are. dogs are loyal\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Making a list of the tokens and priting the list\n",
        "token_list = [token.text for token in doc]\n",
        "print(\"Tokens:\", token_list)\n",
        "\n",
        "# Showing token details\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEjhAnKF4gF6",
        "outputId": "d3957ab2-b02f-438d-fc20-6ff6e8d2bc45"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.', 'dogs', 'are', 'loyal']\n",
            "I PRON nsubj\n",
            "could AUX aux\n",
            "n't PART neg\n",
            "help VERB ROOT\n",
            "the DET det\n",
            "dog NOUN dobj\n",
            ". PUNCT punct\n",
            "Ca AUX aux\n",
            "n't PART neg\n",
            "you PRON nsubj\n",
            "do VERB ROOT\n",
            "it PRON dobj\n",
            "? PUNCT punct\n",
            "Do AUX aux\n",
            "n't PART neg\n",
            "be AUX ROOT\n",
            "afraid ADJ acomp\n",
            "if SCONJ mark\n",
            "you PRON nsubj\n",
            "are AUX advcl\n",
            ". PUNCT punct\n",
            "dogs NOUN nsubj\n",
            "are AUX ROOT\n",
            "loyal ADJ acomp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I PRON nsubj: \"I\" is a pronoun (PRON) and is the nominal subject (nsubj) of the sentence.\n",
        "- help VERB ROOT: \"help\" is a verb (VERB) and is the root action (ROOT) of the sentence.\n",
        "- afraid ADJ acomp: \"afraid\" is an adjective (ADJ) and is an adjectival complement (acomp) which gives more information about a state or quality related to the verb.\n"
      ],
      "metadata": {
        "id": "Ed9je6a45IOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem with this algorithm is that words with similar meanings will be assigned different IDs, resulting in them being treated as entirely separate words with distinct meanings. For example, $Unicorns$ is the plural form of $Unicorn$, but a word-based tokenizer would tokenize them as two separate words, potentially causing the model to miss their semantic relationship.\n",
        "\n"
      ],
      "metadata": {
        "id": "tn5TfPyI6N9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Unicorns are real. I saw a unicorn yesterday.\"\n",
        "token = word_tokenize(text)\n",
        "print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5LM8FDJ6Se9",
        "outputId": "abbb1352-efeb-4541-90f2-68a7fd974675"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Unicorns', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Languages generally have a large number of words, the vocabularies based on them will always be extensive. However, the number of characters in a language is always fewer compared to the number of words. Next, we will explore character-based tokenizers.\n"
      ],
      "metadata": {
        "id": "CDwDcZyw6gjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Character Based Tokenization"
      ],
      "metadata": {
        "id": "RgIiwsya944n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "it is important to note that character-based tokenization has its limitations. Single characters may not convey the same information as entire words\n",
        "\n"
      ],
      "metadata": {
        "id": "092PdOyS8lWi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18Nd6GIy14r8"
      },
      "source": [
        "## Subword-based tokenizer\n",
        "Techniques such as SentencePiece, or WordPiece are commonly used for subword tokenization. These methods learn subword units from a given text corpus, identifying common prefixes, suffixes, and root words as subword tokens based on their frequency of occurrence.  For instance, 'Unhappiness' is split into 'un' and 'happiness,' both of which can appear as stand-alone subwords. When we combine these individual subwords, they form 'unhappiness,' which retains its meaningful context."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wordpiece"
      ],
      "metadata": {
        "id": "Uesii9wq_znp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.tokenize(\"Unhappiness is not good for health and let us see how tokenization works in this case\")"
      ],
      "metadata": {
        "id": "QPRJ0wUZ-j35",
        "outputId": "6febd1c2-b4ee-4dbe-b303-55b509e2f089",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['un',\n",
              " '##ha',\n",
              " '##pp',\n",
              " '##iness',\n",
              " 'is',\n",
              " 'not',\n",
              " 'good',\n",
              " 'for',\n",
              " 'health',\n",
              " 'and',\n",
              " 'let',\n",
              " 'us',\n",
              " 'see',\n",
              " 'how',\n",
              " 'token',\n",
              " '##ization',\n",
              " 'works',\n",
              " 'in',\n",
              " 'this',\n",
              " 'case']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'token', '##ization': \"Tokenization\" is broken into two tokens. \"Token\" is a whole word, and \"##ization\" is a part of the original word.\n",
        "\n",
        " The \"##\" indicates that \"ization\" should be connected back to \"token\" when detokenizing (transforming tokens back to words)."
      ],
      "metadata": {
        "id": "1nK5i5Ds_AEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Piece\n"
      ],
      "metadata": {
        "id": "2TCeSUZf_uUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentencePiece is a tool that takes text, divides it into smaller, more manageable parts, assigns IDs to these segments, and ensures that it does so consistently"
      ],
      "metadata": {
        "id": "8wJyqcvmAbG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "tokenizer.tokenize(\"IBM taught me tokenization. I am practising IBM labs\")"
      ],
      "metadata": {
        "id": "0VUvno3G-_a4",
        "outputId": "ff34dd63-3626-44b0-c659-332e735c22e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁IBM',\n",
              " '▁taught',\n",
              " '▁me',\n",
              " '▁token',\n",
              " 'ization',\n",
              " '.',\n",
              " '▁I',\n",
              " '▁am',\n",
              " '▁pr',\n",
              " 'act',\n",
              " 'ising',\n",
              " '▁IBM',\n",
              " '▁labs']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_ indicates space"
      ],
      "metadata": {
        "id": "PEbxfp6EAilN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization with Pytorch"
      ],
      "metadata": {
        "id": "qzf2RPSTNnO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    (1,\"Introduction to NLP\"),\n",
        "    (2,\"Basics of PyTorch\"),\n",
        "    (1,\"NLP Techniques for Text Classification\"),\n",
        "    (3,\"Named Entity Recognition with PyTorch\"),\n",
        "    (3,\"Sentiment Analysis using PyTorch\"),\n",
        "    (3,\"Machine Translation with PyTorch\"),\n",
        "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
        "    (1,\" Machine Translation with NLP \"),\n",
        "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]"
      ],
      "metadata": {
        "id": "_bxOrjG1M-yQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokenizer(dataset[0][1])"
      ],
      "metadata": {
        "id": "jkmqTSQgNmIP",
        "outputId": "31554953-531b-4df6-feb7-4ecad385b887",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['introduction', 'to', 'nlp']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hriVhGunNz1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "build_vocab_from_iterator function, when applied to a list of tokens, assigns a unique index to each token based on its position in the vocabulary. These indices serve as a way to represent the tokens in a numerical format that can be easily processed by machine learning models.\n",
        "\n",
        "\n",
        "dataset is an iterable. Therefore, you use a generator function yield_tokens to apply the tokenizer. The purpose of the generator function yield_tokens is to yield tokenized texts one at a time.\n"
      ],
      "metadata": {
        "id": "p8xcW5RoOjGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(data_iter):\n",
        "    for  _,text in data_iter:\n",
        "        yield tokenizer(text)"
      ],
      "metadata": {
        "id": "N04x8GL2Owyf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_iterator = yield_tokens(dataset)  ### used for iterating dataset"
      ],
      "metadata": {
        "id": "npqILk8fOzk9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(my_iterator)"
      ],
      "metadata": {
        "id": "sS_Mh5diO04v",
        "outputId": "3961e089-d13d-4da8-dd61-8c00fa5f654c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['introduction', 'to', 'nlp']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "8x2brpkgPYHI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenized_sentence_and_indices(iterator):\n",
        "    tokenized_sentence = next(iterator)  # Get the next tokenized sentence\n",
        "    token_indices = [vocab[token] for token in tokenized_sentence]  # Get token indices\n",
        "    return tokenized_sentence, token_indices\n",
        "\n",
        "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
        "next(my_iterator)\n",
        "\n",
        "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
        "print(\"Token Indices:\", token_indices)"
      ],
      "metadata": {
        "id": "GUq5qTELPjp5",
        "outputId": "fdaa894e-ca68-4406-9cbe-f26712d8fb32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentence: ['basics', 'of', 'pytorch']\n",
            "Token Indices: [11, 15, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = [\"IBM taught me tokenization\",\n",
        "         \"Special tokenizers are ready and they will blow your mind\",\n",
        "         \"just saying hi!\"]\n",
        "\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "tokens = []\n",
        "max_length = 0\n",
        "\n",
        "for line in lines:\n",
        "    tokenized_line = tokenizer_en(line)\n",
        "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
        "    tokens.append(tokenized_line)\n",
        "    max_length = max(max_length, len(tokenized_line))\n",
        "\n",
        "for i in range(len(tokens)):\n",
        "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
        "\n",
        "print(\"Lines after adding special tokens:\\n\", tokens)\n",
        "\n",
        "# Build vocabulary without unk_init\n",
        "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Vocabulary and Token Ids\n",
        "print(\"Vocabulary:\", vocab.get_itos())\n",
        "print(\"Token IDs for 'tokenization':\", vocab.get_stoi())"
      ],
      "metadata": {
        "id": "mXz0ik2QP6PE",
        "outputId": "b40a16ad-6591-4f0c-d158-081b919c7817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lines after adding special tokens:\n",
            " [['<bos>', 'IBM', 'taught', 'me', 'tokenization', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'Special', 'tokenizers', 'are', 'ready', 'and', 'they', 'will', 'blow', 'your', 'mind', '<eos>'], ['<bos>', 'just', 'saying', 'hi', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
            "Vocabulary: ['<unk>', '<pad>', '<bos>', '<eos>', '!', 'IBM', 'Special', 'and', 'are', 'blow', 'hi', 'just', 'me', 'mind', 'ready', 'saying', 'taught', 'they', 'tokenization', 'tokenizers', 'will', 'your']\n",
            "Token IDs for 'tokenization': {'will': 20, 'tokenizers': 19, 'tokenization': 18, 'taught': 16, 'your': 21, 'saying': 15, '<unk>': 0, 'and': 7, 'hi': 10, '<pad>': 1, '<bos>': 2, 'they': 17, '<eos>': 3, '!': 4, 'ready': 14, 'IBM': 5, 'are': 8, 'Special': 6, 'mind': 13, 'me': 12, 'blow': 9, 'just': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FnT_SJJgTsgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down the output:\n",
        "1. **Special Tokens**:\n",
        "- Token: \"`<unk>`\", Index: 0: `<unk>` stands for \"unknown\" and represents words that were not seen during vocabulary building, usually during inference on new text.\n",
        "- Token: \"`<pad>`\", Index: 1: `<pad>` is a \"padding\" token used to make sequences of words the same length when batching them together.\n",
        "- Token: \"`<bos>`\", Index: 2: `<bos>` is an acronym for \"beginning of sequence\" and is used to denote the start of a text sequence.\n",
        "- Token: \"`<eos>`\", Index: 3: `<eos>` is an acronym for \"end of sequence\" and is used to denote the end of a text sequence.\n",
        "\n",
        "2. **Word Tokens**:\n",
        "The rest of the tokens are words or punctuation extracted from the provided sentences, each assigned a unique index:\n",
        "- Token: \"IBM\", Index: 5\n",
        "- Token: \"taught\", Index: 16\n",
        "- Token: \"me\", Index: 12\n",
        "    ... and so on.\n",
        "    \n",
        "3. **Vocabulary**:\n",
        "It denotes the total number of tokens in the sentences upon which vocabulary is built.\n",
        "    \n",
        "4. **Token IDs for 'tokenization'**:\n",
        "It represents the token IDs assigned in the vocab where a number represents its presence in the sentence.\n"
      ],
      "metadata": {
        "id": "5-_XzfieUOP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Going through the world of tokenization has been like walking through a huge maze made of words, symbols, and meanings. Each turn shows a bit more about the cool ways computers learn to understand our language. And while I'm still finding my way through it, the journey’s been enlightening and, honestly, a bunch of fun.\n",
        "Eager to see where this learning path takes me next!\"\n",
        "\"\"\"\n",
        "\n",
        "# Counting and displaying tokens and their frequency\n",
        "from collections import Counter\n",
        "def show_frequencies(tokens, method_name):\n",
        "    print(f\"{method_name} Token Frequencies: {dict(Counter(tokens))}\\n\")"
      ],
      "metadata": {
        "id": "ecNuqFWDUQS0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from transformers import BertTokenizer, XLNetTokenizer\n",
        "from datetime import datetime\n",
        "\n",
        "# NLTK Tokenization\n",
        "start_time = datetime.now()\n",
        "nltk_tokens = nltk.word_tokenize(text)\n",
        "nltk_time = datetime.now() - start_time\n",
        "\n",
        "# SpaCy Tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "start_time = datetime.now()\n",
        "spacy_tokens = [token.text for token in nlp(text)]\n",
        "spacy_time = datetime.now() - start_time\n",
        "\n",
        "# BertTokenizer Tokenization\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "start_time = datetime.now()\n",
        "bert_tokens = bert_tokenizer.tokenize(text)\n",
        "bert_time = datetime.now() - start_time\n",
        "\n",
        "# XLNetTokenizer Tokenization\n",
        "xlnet_tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "start_time = datetime.now()\n",
        "xlnet_tokens = xlnet_tokenizer.tokenize(text)\n",
        "xlnet_time = datetime.now() - start_time\n",
        "\n",
        "# Display tokens, time taken for each tokenizer, and token frequencies\n",
        "print(f\"NLTK Tokens: {nltk_tokens}\\nTime Taken: {nltk_time} seconds\\n\")\n",
        "show_frequencies(nltk_tokens, \"NLTK\")\n",
        "\n",
        "print(f\"SpaCy Tokens: {spacy_tokens}\\nTime Taken: {spacy_time} seconds\\n\")\n",
        "show_frequencies(spacy_tokens, \"SpaCy\")\n",
        "\n",
        "print(f\"Bert Tokens: {bert_tokens}\\nTime Taken: {bert_time} seconds\\n\")\n",
        "show_frequencies(bert_tokens, \"Bert\")\n",
        "\n",
        "print(f\"XLNet Tokens: {xlnet_tokens}\\nTime Taken: {xlnet_time} seconds\\n\")\n",
        "show_frequencies(xlnet_tokens, \"XLNet\")"
      ],
      "metadata": {
        "id": "Jk_NQouRUtBd",
        "outputId": "57050f97-7e53-46b3-a02c-5f9669ed783d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Tokens: ['Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', \"''\"]\n",
            "Time Taken: 0:00:00.002816 seconds\n",
            "\n",
            "NLTK Token Frequencies: {'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, \"''\": 1}\n",
            "\n",
            "SpaCy Tokens: ['\\n', 'Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’s', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', '\\n', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"', '\\n']\n",
            "Time Taken: 0:00:00.048883 seconds\n",
            "\n",
            "SpaCy Token Frequencies: {'\\n': 3, 'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’s': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
            "\n",
            "Bert Tokens: ['going', 'through', 'the', 'world', 'of', 'token', '##ization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'and', 'while', 'i', \"'\", 'm', 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'en', '##light', '##ening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"']\n",
            "Time Taken: 0:00:00.001844 seconds\n",
            "\n",
            "Bert Token Frequencies: {'going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'token': 1, '##ization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 3, 'meanings': 1, '.': 3, 'each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'while': 1, 'i': 1, \"'\": 1, 'm': 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'en': 1, '##light': 1, '##ening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
            "\n",
            "XLNet Tokens: ['▁Going', '▁through', '▁the', '▁world', '▁of', '▁token', 'ization', '▁has', '▁been', '▁like', '▁walking', '▁through', '▁a', '▁huge', '▁maze', '▁made', '▁of', '▁words', ',', '▁symbols', ',', '▁and', '▁meaning', 's', '.', '▁Each', '▁turn', '▁shows', '▁a', '▁bit', '▁more', '▁about', '▁the', '▁cool', '▁ways', '▁computers', '▁learn', '▁to', '▁understand', '▁our', '▁language', '.', '▁And', '▁while', '▁I', \"'\", 'm', '▁still', '▁finding', '▁my', '▁way', '▁through', '▁it', ',', '▁the', '▁journey', '’', 's', '▁been', '▁enlighten', 'ing', '▁and', ',', '▁honestly', ',', '▁a', '▁bunch', '▁of', '▁fun', '.', '▁E', 'ager', '▁to', '▁see', '▁where', '▁this', '▁learning', '▁path', '▁takes', '▁me', '▁next', '!', '\"']\n",
            "Time Taken: 0:00:00.000764 seconds\n",
            "\n",
            "XLNet Token Frequencies: {'▁Going': 1, '▁through': 3, '▁the': 3, '▁world': 1, '▁of': 3, '▁token': 1, 'ization': 1, '▁has': 1, '▁been': 2, '▁like': 1, '▁walking': 1, '▁a': 3, '▁huge': 1, '▁maze': 1, '▁made': 1, '▁words': 1, ',': 5, '▁symbols': 1, '▁and': 2, '▁meaning': 1, 's': 2, '.': 3, '▁Each': 1, '▁turn': 1, '▁shows': 1, '▁bit': 1, '▁more': 1, '▁about': 1, '▁cool': 1, '▁ways': 1, '▁computers': 1, '▁learn': 1, '▁to': 2, '▁understand': 1, '▁our': 1, '▁language': 1, '▁And': 1, '▁while': 1, '▁I': 1, \"'\": 1, 'm': 1, '▁still': 1, '▁finding': 1, '▁my': 1, '▁way': 1, '▁it': 1, '▁journey': 1, '’': 1, '▁enlighten': 1, 'ing': 1, '▁honestly': 1, '▁bunch': 1, '▁fun': 1, '▁E': 1, 'ager': 1, '▁see': 1, '▁where': 1, '▁this': 1, '▁learning': 1, '▁path': 1, '▁takes': 1, '▁me': 1, '▁next': 1, '!': 1, '\"': 1}\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}