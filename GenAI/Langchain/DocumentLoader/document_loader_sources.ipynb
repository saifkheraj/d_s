{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Documents Using LangChain for Different Sources\n"
      ],
      "metadata": {
        "id": "3cNA9Jmu2emo"
      },
      "id": "3cNA9Jmu2emo"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jq"
      ],
      "metadata": {
        "id": "DP1wLDifJvjk",
        "outputId": "f759dbe2-6749-4e48-b14f-1fa33e4cfbf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DP1wLDifJvjk",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jq\n",
            "  Downloading jq-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Downloading jq-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (746 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m746.6/746.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jq\n",
            "Successfully installed jq-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2acaa5db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2acaa5db",
        "outputId": "b493857f-beac-4846-94af-75ae215782e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community==0.2.1\n",
            "  Downloading langchain_community-0.2.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.1) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.1) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.1) (3.11.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.2.1)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.0 (from langchain-community==0.2.1)\n",
            "  Downloading langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain-community==0.2.1)\n",
            "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-community==0.2.1)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2,>=1 (from langchain-community==0.2.1)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m727.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.1) (2.32.3)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain-community==0.2.1)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.1)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.1)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.0->langchain-community==0.2.1)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community==0.2.1) (2.11.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.1) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.1) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.1) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.1) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.2.1) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.1) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.1) (0.4.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.1)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (1.3.1)\n",
            "Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain-0.2.17-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: tenacity, numpy, mypy-extensions, marshmallow, typing-inspect, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "\u001b[33m  WARNING: The script f2py is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script langsmith is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-0.2.17 langchain-community-0.2.1 langchain-core-0.2.43 langchain-text-splitters-0.2.4 langsmith-0.1.147 marshmallow-3.26.1 mypy-extensions-1.1.0 numpy-1.26.4 tenacity-8.5.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --user \"langchain-community==0.2.1\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I0nocCjYFwEK"
      },
      "id": "I0nocCjYFwEK"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-vs_nIb2rjo",
        "outputId": "47863286-e099-4bff-e14c-468ebe87af47"
      },
      "id": "N-vs_nIb2rjo",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymupdf"
      ],
      "metadata": {
        "id": "RamyDYnBG61I",
        "outputId": "51f91d33-d2bc-4c23-8faf-24a8f47ad57f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RamyDYnBG61I",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unstructured"
      ],
      "metadata": {
        "id": "2G_rqgKrI0bx",
        "outputId": "44c8a861-0a6b-45f0-a54f-d8af9d58b9fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2G_rqgKrI0bx",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.17.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.4)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dataclasses-json in /root/.local/lib/python3.11/site-packages (from unstructured) (0.6.7)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /root/.local/lib/python3.11/site-packages (from unstructured) (1.26.4)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.2)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.35.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /root/.local/lib/python3.11/site-packages (from dataclasses-json->unstructured) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /root/.local/lib/python3.11/site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (2024.11.6)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (2025.4.26)\n",
            "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (2.11.4)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (5.5.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.4.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /root/.local/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Downloading unstructured-0.17.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.35.0-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.1/192.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=aead8f2743292fec9de5c51a03a1c56624fde0119f88b15b85e00f439e5cd6ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, olefile, langdetect, emoji, backoff, aiofiles, python-oxmsg, unstructured-client, unstructured\n",
            "Successfully installed aiofiles-24.1.0 backoff-2.2.1 emoji-2.14.1 filetype-1.2.0 langdetect-1.0.9 olefile-0.47 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.13.0 unstructured-0.17.2 unstructured-client-0.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Each client provides their data in different formats: some in PDFs, others in Word documents, CSV files, or even HTML webpages. Manually loading and parsing each document type is not only time-consuming but also prone to errors. Your goal is to streamline this process, making it efficient and error-free.\n",
        "\n",
        "To achieve this, you'll use LangChain’s powerful document loaders. These loaders allow you to read and convert various file formats into a unified document structure that can be easily processed. For example, you'll load client policy documents from text files, financial reports from PDFs, marketing strategies from Word documents, and product reviews from JSON files. By the end of this lab, you will have a robust pipeline that can handle any new file formats clients might send, saving you valuable time and effort.\n",
        "\n",
        " - Understand how to use `TextLoader` to load text files.\n",
        " - Learn how to load PDFs using `PyPDFLoader` and `PyMuPDFLoader`.\n",
        " - Use `UnstructuredMarkdownLoader` to load Markdown files.\n",
        " - Load JSON files with `JSONLoader` using jq schemas.\n",
        " - Process CSV files with `CSVLoader` and `UnstructuredCSVLoader`.\n",
        " - Load Webpage content using `WebBaseLoader`.\n",
        " - Load Word documents using `Docx2txtLoader`.\n",
        " - Utilize `UnstructuredFileLoader` for various file types.\n",
        "\n"
      ],
      "metadata": {
        "id": "mbRedxwr2j5h"
      },
      "id": "mbRedxwr2j5h"
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also use this section to suppress warnings generated by your code:\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from pprint import pprint\n",
        "import json\n",
        "from pathlib import Path\n",
        "import nltk\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain_community.document_loaders import JSONLoader\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain_community.document_loaders.csv_loader import UnstructuredCSVLoader\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.document_loaders import Docx2txtLoader\n",
        "from langchain_community.document_loaders import UnstructuredFileLoader\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eLTU89-2st9",
        "outputId": "ca15a384-d838-4c3b-dcea-e4a75fb2f6ef"
      },
      "id": "7eLTU89-2st9",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-h_JYZYqOxZ",
        "outputId": "faba1acb-b79d-4bdd-84e1-5b8b91911444"
      },
      "id": "G-h_JYZYqOxZ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-25 20:10:16--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6363 (6.2K) [text/plain]\n",
            "Saving to: ‘new-Policies.txt’\n",
            "\n",
            "new-Policies.txt    100%[===================>]   6.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-25 20:10:17 (2.31 GB/s) - ‘new-Policies.txt’ saved [6363/6363]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will use the `TextLoader` class to load the file.\n"
      ],
      "metadata": {
        "id": "3sle9boLEYmQ"
      },
      "id": "3sle9boLEYmQ"
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(\"new-Policies.txt\")\n",
        "loader"
      ],
      "metadata": {
        "id": "Q76m2h1vEY9N",
        "outputId": "ec2d1d3d-4b0d-4eb7-e2c3-bd1e0e089d63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Q76m2h1vEY9N",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.document_loaders.text.TextLoader at 0x7b71d3840750>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "QEfN3HaLEdIZ"
      },
      "id": "QEfN3HaLEdIZ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's present the entire data (document) here.\n",
        "\n",
        "This is a `document` object that includes `page_content` and `metadata` attributes.\n"
      ],
      "metadata": {
        "id": "2ZbrNZD_Ehik"
      },
      "id": "2ZbrNZD_Ehik"
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "CSUl2jkKEe5M",
        "outputId": "940d1555-ff70-4d19-fdc1-3f39987d0fca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CSUl2jkKEe5M",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'new-Policies.txt'}, page_content=\"1. Code of Conduct\\n\\nOur Code of Conduct establishes the core values and ethical standards that all members of our organization must adhere to. We are committed to fostering a workplace characterized by integrity, respect, and accountability.\\n\\nIntegrity: We commit to the highest ethical standards by being honest and transparent in all our dealings, whether with colleagues, clients, or the community. We protect sensitive information and avoid conflicts of interest.\\n\\nRespect: We value diversity and every individual's contribution. Discrimination, harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where differences are respected, and everyone is treated with dignity.\\n\\nAccountability: We are responsible for our actions and decisions, complying with all relevant laws and regulations. We aim for continuous improvement and report any breaches of this code, supporting investigations into such matters.\\n\\nSafety: We prioritize the safety of our employees, clients, and the community. We encourage a culture of safety, including reporting any unsafe practices or conditions.\\n\\nEnvironmental Responsibility: We strive to reduce our environmental impact and promote sustainable practices.\\n\\nThis Code of Conduct is the cornerstone of our organizational culture. We expect every employee to uphold these principles and act as role models, ensuring our reputation for ethical conduct, integrity, and social responsibility.\\n\\n2. Recruitment Policy\\n\\nOur Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified and diverse candidates into our organization. The success of our company depends on the talent, skills, and commitment of our employees.\\n\\nEqual Opportunity: We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively support diversity and inclusion.\\n\\nTransparency: We maintain a transparent recruitment process. Job vacancies are advertised both internally and externally when appropriate. Job descriptions and requirements are clear and accurately reflect the role.\\n\\nSelection Criteria: We base our selection on qualifications, experience, and skills relevant to the role. Our interviews and assessments are objective, and decisions are made impartially.\\n\\nData Privacy: We are dedicated to protecting candidates' personal information and comply with all applicable data protection laws.\\n\\nFeedback: Candidates receive timely and constructive feedback on their applications and interview performance.\\n\\nOnboarding: New hires receive thorough onboarding to help them integrate effectively, including an overview of our culture, policies, and expectations.\\n\\nEmployee Referrals: We welcome employee referrals as they help build a strong and engaged team.\\n\\nThis policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\\n\\n\\n3. Internet and Email Policy\\n\\nOur Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\\n\\nAcceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\\n\\nSecurity: Protect your login credentials and avoid sharing passwords. Be cautious with email attachments and links from unknown sources, and promptly report any unusual online activity or potential security threats.\\n\\nConfidentiality: Use email for confidential information, trade secrets, and sensitive customer data only with encryption. Be careful when discussing company matters on public platforms or social media.\\n\\nHarassment and Inappropriate Content: Internet and email must not be used for harassment, discrimination, or the distribution of offensive content. Always communicate respectfully and sensitively online.\\n\\nCompliance: Adhere to all relevant laws and regulations concerning internet and email use, including copyright and data protection laws.\\n\\nMonitoring: The company reserves the right to monitor internet and email usage for security and compliance purposes.\\n\\nConsequences: Violations of this policy may lead to disciplinary action, including potential termination.\\n\\nThis policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\\n\\n4. Mobile Phone Policy\\n\\nOur Mobile Phone Policy defines standards for responsible use of mobile devices within the organization to ensure alignment with company values and legal requirements.\\n\\nAcceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is allowed if it does not disrupt work responsibilities.\\n\\nSecurity: Secure your mobile device and credentials. Be cautious with app downloads and links from unknown sources, and report any security issues promptly.\\n\\nConfidentiality: Avoid sharing sensitive company information via unsecured messaging apps or emails. Exercise caution when discussing company matters in public.\\n\\nCost Management: Personal use of mobile phones should be separate from company accounts, and any personal charges on company-issued phones must be reimbursed.\\n\\nCompliance: Comply with all relevant laws and regulations concerning mobile phone usage, including data protection and privacy laws.\\n\\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\\n\\nConsequences: Non-compliance with this policy may result in disciplinary actions, including potential loss of mobile phone privileges.\\n\\nThis policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.\\n\")]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(data[0].page_content[:1000])"
      ],
      "metadata": {
        "id": "1BqhRNZcFbDB",
        "outputId": "3b1e7231-9039-415b-92d6-48fb22908d4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1BqhRNZcFbDB",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('1. Code of Conduct\\n'\n",
            " '\\n'\n",
            " 'Our Code of Conduct establishes the core values and ethical standards that '\n",
            " 'all members of our organization must adhere to. We are committed to '\n",
            " 'fostering a workplace characterized by integrity, respect, and '\n",
            " 'accountability.\\n'\n",
            " '\\n'\n",
            " 'Integrity: We commit to the highest ethical standards by being honest and '\n",
            " 'transparent in all our dealings, whether with colleagues, clients, or the '\n",
            " 'community. We protect sensitive information and avoid conflicts of '\n",
            " 'interest.\\n'\n",
            " '\\n'\n",
            " \"Respect: We value diversity and every individual's contribution. \"\n",
            " 'Discrimination, harassment, or any form of disrespect is not tolerated. We '\n",
            " 'promote an inclusive environment where differences are respected, and '\n",
            " 'everyone is treated with dignity.\\n'\n",
            " '\\n'\n",
            " 'Accountability: We are responsible for our actions and decisions, complying '\n",
            " 'with all relevant laws and regulations. We aim for continuous improvement '\n",
            " 'and report any breaches of this code, supporting investigations into such '\n",
            " 'matters.\\n'\n",
            " '\\n'\n",
            " 'Safety: We prioritize the safety of our employees, c')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load from PDF files\n",
        "\n",
        "Sometimes, we may have files in PDF format that we want to load for processing.\n",
        "\n",
        "LangChain provides several classes for loading PDFs. Here, we introduce two classes: `PyPDFLoader` and `PyMuPDFLoader`.\n",
        "\n",
        "#### PyPDFLoader\n",
        "\n",
        "Load the PDF using `PyPDFLoader` into an array of documents, where each document contains the page content and metadata with the page number.\n"
      ],
      "metadata": {
        "id": "lnglNzd3FhDc"
      },
      "id": "lnglNzd3FhDc"
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf\"\n",
        "\n",
        "loader = PyPDFLoader(pdf_url)\n",
        "\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "80zzpdLxFkZQ"
      },
      "id": "80zzpdLxFkZQ",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pages[0])"
      ],
      "metadata": {
        "id": "j6uWTn26Gt7g",
        "outputId": "b310efe6-b797-4bce-caf7-0d1ebfd5a73c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "j6uWTn26Gt7g",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='LAB: L ARGE -SCALE ALIGNMENT FOR CHATBOTS\n",
            "MIT-IBM Watson AI Lab and IBM Research\n",
            "Shivchander Sudalairaj∗\n",
            "Abhishek Bhandwaldar∗\n",
            "Aldo Pareja∗\n",
            "Kai Xu\n",
            "David D. Cox\n",
            "Akash Srivastava∗,†\n",
            "*Equal Contribution, †Corresponding Author\n",
            "ABSTRACT\n",
            "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
            "ology designed to overcome the scalability challenges in the instruction-tuning\n",
            "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
            "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
            "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
            "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
            "itive performance across several benchmarks compared to models trained with\n",
            "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
            "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
            "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
            "step forward in the efficient training of LLMs for a wide range of applications.\n",
            "1 I NTRODUCTION\n",
            "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
            "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
            "marization . This has been made possible, in large part, by the introduction of the transformer\n",
            "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
            "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
            "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
            "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
            "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
            "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
            "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
            "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
            "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
            "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
            "article in 2 lines:{News article}) and the model is trained to maximize the likelihood of the pro-\n",
            "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
            "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
            "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
            "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
            "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
            "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
            "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
            "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
            "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
            "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
            "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
            "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
            "1\n",
            "arXiv:2403.01081v3  [cs.CL]  29 Apr 2024' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## display first 3 pages\n",
        "for p,page in enumerate(pages[0:3]):\n",
        "    print(f\"page number {p+1}\")\n",
        "    print(page)"
      ],
      "metadata": {
        "id": "uZ3FA41yGuzG",
        "outputId": "50322e0c-921e-4b61-85d2-a3e6111de12b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uZ3FA41yGuzG",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page number 1\n",
            "page_content='LAB: L ARGE -SCALE ALIGNMENT FOR CHATBOTS\n",
            "MIT-IBM Watson AI Lab and IBM Research\n",
            "Shivchander Sudalairaj∗\n",
            "Abhishek Bhandwaldar∗\n",
            "Aldo Pareja∗\n",
            "Kai Xu\n",
            "David D. Cox\n",
            "Akash Srivastava∗,†\n",
            "*Equal Contribution, †Corresponding Author\n",
            "ABSTRACT\n",
            "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
            "ology designed to overcome the scalability challenges in the instruction-tuning\n",
            "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
            "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
            "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
            "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
            "itive performance across several benchmarks compared to models trained with\n",
            "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
            "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
            "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
            "step forward in the efficient training of LLMs for a wide range of applications.\n",
            "1 I NTRODUCTION\n",
            "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
            "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
            "marization . This has been made possible, in large part, by the introduction of the transformer\n",
            "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
            "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
            "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
            "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
            "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
            "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
            "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
            "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
            "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
            "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
            "article in 2 lines:{News article}) and the model is trained to maximize the likelihood of the pro-\n",
            "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
            "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
            "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
            "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
            "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
            "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
            "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
            "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
            "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
            "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
            "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
            "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
            "1\n",
            "arXiv:2403.01081v3  [cs.CL]  29 Apr 2024' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0}\n",
            "page number 2\n",
            "page_content='training captures enough of the distribution of language and knowledge, such that a small amount of\n",
            "supervised training can “unlock” or shape latent abilities related to the ultimate desired instruction-\n",
            "following behavior of the model. However, unlike the unstructured data that is abundantly available\n",
            "in the public domain, high-quality, human-generated task-specific instruction data is costly to pro-\n",
            "cure, even via crowd-sourcing, and human-generated instruction data is typically closely guarded\n",
            "by model builders, even for ostensibly “open” model-building efforts. In this work, we address\n",
            "the challenges associated with scaling of the alignment-tuning phase and propose a new method\n",
            "called LAB: Large-scale Alignment for chatBots. The LAB method consists of two components:\n",
            "(i) a taxonomy-guided synthetic data generation method and quality assurance process that yields a\n",
            "highly diverse and high-quality instruction dataset, without resorting to the use of proprietary LLMs\n",
            "like GPT-4 or substantial human curation, and (ii) a novel multi-phase training framework and un-\n",
            "conventional tuning regime that allows for adding new knowledge and instruction-following abilities\n",
            "into pre-trained LLMs without suffering from catastrophic forgetting. Our findings show that LAB-\n",
            "trained models can perform competitively with proprietary and open-source models that use human\n",
            "annotations and/or synthetic data generated using GPT-4 on a number of benchmarks.\n",
            "2 R ELATED WORK\n",
            "Existing methods for instruction tuning typically either rely on humans for generating high-quality\n",
            "datasets, or use synthetic data generation using a large teacher model. OpenAI (Ouyang et al.,\n",
            "2022) arguably set the standard for model alignment from human data, employing human annota-\n",
            "tors to gather data for supervised fine tuning (SFT) and reinforcement learning with human feed-\n",
            "back (RLHF) training. Collecting human-generated data for these steps is complex undertaking; the\n",
            "selection of annotators requires a rigorous multi-stage screening process aimed at achieving high\n",
            "inter-annotator agreement, and collecting even modest amounts data (by LLM standards) requires\n",
            "the coordination of large groups of annotators. The creators of the LLaMA 2 model series (Touvron\n",
            "et al., 2023) followed a similar recipe, collecting tens of thousands of human-generated instruction\n",
            "samples, and approximately 1 million human-annotated binary comparisons for reward modeling.\n",
            "Not only are such approaches expensive and time consuming, but they can also potentially limit\n",
            "agility in exploring the space of instructions and capabilities the model is trained to perform. Alter-\n",
            "natives to this approach, such as transforming existing human datasets into instructions via templat-\n",
            "ing (Wei et al.) can be more cost effective, but face limitations in the naturalness and length of the\n",
            "responses used for training.\n",
            "More recently, training with synthetic data generated from LLMs has emerged as an alternative to\n",
            "purely human-data-based approaches. Wang et al. (2023) introduced Self-Instruct, which leverages\n",
            "a small number of handwritten human seed instructions as input to bootstrapping process to generate\n",
            "a large number of samples using an LLM’s own generation abilities. Taori et al. (2023) built upon\n",
            "Self-Instruct, using a larger teacher model to generate synthetic data to train a smaller student model,\n",
            "and incorporating principles in the generation prompt to promote diversity in the generated instruc-\n",
            "tion data. Xu et al. (2023) introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes\n",
            "iteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee\n",
            "et al. (2023), Mitra et al. (2023) present a synthetic data generation approach to enhance task di-\n",
            "versity and scalability, alongside a progressive training framework aimed at improving the model’s\n",
            "reasoning ability and response style to match teacher models. This is achieved by generating rich' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 1}\n",
            "page number 3\n",
            "page_content='versity and scalability, alongside a progressive training framework aimed at improving the model’s\n",
            "reasoning ability and response style to match teacher models. This is achieved by generating rich\n",
            "reasoning signals in the generated answer and progressively training on datasets of varying difficulty\n",
            "in incremental phases.\n",
            "Similar to LAB, concurrent work, GLAN (Li et al., 2024), employs a semi-automatic approach to\n",
            "synthetic data generation that uses a human-curated taxonomy to generate instruction tuning data\n",
            "from a teacher model. However, as explained in section 3.2.2, unlike LAB, GLAN cannot be used\n",
            "to generate synthetic data from domains that are not captured in the teacher model’s support. As\n",
            "such, while LAB uses the open-source Mixtral model as the teacher, like many other synthetic\n",
            "data generation approaches, GLAN has to rely on a large proprietary model (GPT-4). This poses\n",
            "complicated questions about the usability of generated data (especially for commercial purposes)\n",
            "since the terms of use of proprietary models typically forbid using the model to improve other\n",
            "models.\n",
            "2' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PyMuPDFLoader\n",
        "\n",
        "`PyMuPDFLoader` is the fastest of the PDF parsing options. It provides detailed metadata about the PDF and its pages, and returns one document per page.\n"
      ],
      "metadata": {
        "id": "E_jblv3iG1c-"
      },
      "id": "E_jblv3iG1c-"
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyMuPDFLoader(pdf_url)\n",
        "loader"
      ],
      "metadata": {
        "id": "kM5L24CuGyxQ",
        "outputId": "aa82c085-39ed-452d-d27f-1d34dd762958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kM5L24CuGyxQ",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.document_loaders.pdf.PyMuPDFLoader at 0x7b7215b36190>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()\n",
        "print(data[0])"
      ],
      "metadata": {
        "id": "Xw2upi8NHsSo",
        "outputId": "5635b119-3621-4f55-c29d-35e6879a004c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Xw2upi8NHsSo",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='LAB: LARGE-SCALE ALIGNMENT FOR CHATBOTS\n",
            "MIT-IBM Watson AI Lab and IBM Research\n",
            "Shivchander Sudalairaj∗\n",
            "Abhishek Bhandwaldar∗\n",
            "Aldo Pareja∗\n",
            "Kai Xu\n",
            "David D. Cox\n",
            "Akash Srivastava∗,†\n",
            "*Equal Contribution, †Corresponding Author\n",
            "ABSTRACT\n",
            "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
            "ology designed to overcome the scalability challenges in the instruction-tuning\n",
            "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
            "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
            "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
            "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
            "itive performance across several benchmarks compared to models trained with\n",
            "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
            "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
            "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
            "step forward in the efficient training of LLMs for a wide range of applications.\n",
            "1\n",
            "INTRODUCTION\n",
            "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
            "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
            "marization . This has been made possible, in large part, by the introduction of the transformer\n",
            "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
            "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
            "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
            "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
            "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
            "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
            "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
            "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
            "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
            "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
            "article in 2 lines: {News article}) and the model is trained to maximize the likelihood of the pro-\n",
            "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
            "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
            "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
            "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
            "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
            "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
            "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
            "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
            "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
            "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
            "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
            "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
            "1\n",
            "arXiv:2403.01081v3  [cs.CL]  29 Apr 2024\n",
            "' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0, 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240501000524Z', 'modDate': 'D:20240501000524Z', 'trapped': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `metadata` attribute reveals that `PyMuPDFLoader` provides more detailed metadata information than `PyPDFLoader`.\n"
      ],
      "metadata": {
        "id": "Hizu1DCSIlim"
      },
      "id": "Hizu1DCSIlim"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load from Markdown files\n",
        "\n",
        "Sometimes, our file source might be in Markdown format.\n",
        "\n",
        "LangChain provides the `UnstructuredMarkdownLoader` to load content from Markdown files.\n"
      ],
      "metadata": {
        "id": "261Yg7F4IokZ"
      },
      "id": "261Yg7F4IokZ"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eMSP5vJjj9yOfAacLZRWsg/markdown-sample.md'"
      ],
      "metadata": {
        "id": "XAn0kEV4ImO0",
        "outputId": "233ce739-4127-4a19-dcad-332f604ebe47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XAn0kEV4ImO0",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-25 20:29:20--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eMSP5vJjj9yOfAacLZRWsg/markdown-sample.md\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3398 (3.3K) [text/markdown]\n",
            "Saving to: ‘markdown-sample.md’\n",
            "\n",
            "markdown-sample.md  100%[===================>]   3.32K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-25 20:29:21 (1.36 GB/s) - ‘markdown-sample.md’ saved [3398/3398]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_path = \"markdown-sample.md\"\n",
        "loader = UnstructuredMarkdownLoader(markdown_path)\n",
        "loader\n",
        "\n",
        "data = loader.load()\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "MIqqLmLTIt-A",
        "outputId": "617e3a30-2b0a-4911-bb0d-337653c2cc54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MIqqLmLTIt-A",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'markdown-sample.md'}, page_content='An h1 header\\n\\nParagraphs are separated by a blank line.\\n\\n2nd paragraph. Italic, bold, and monospace. Itemized lists look like:\\n\\nthis one\\n\\nthat one\\n\\nthe other one\\n\\nNote that --- not considering the asterisk --- the actual text content starts at 4-columns in.\\n\\nBlock quotes are written like so.\\n\\nThey can span multiple paragraphs, if you like.\\n\\nUse 3 dashes for an em-dash. Use 2 dashes for ranges (ex., \"it\\'s all in chapters 12--14\"). Three dots ... will be converted to an ellipsis. Unicode is supported. ☺\\n\\nAn h2 header\\n\\nHere\\'s a numbered list:\\n\\nfirst item\\n\\nsecond item\\n\\nthird item\\n\\nNote again how the actual text starts at 4 columns in (4 characters from the left side). Here\\'s a code sample:\\n\\n# Let me re-iterate ...\\nfor i in 1 .. 10 { do-something(i) }\\n\\nAs you probably guessed, indented 4 spaces. By the way, instead of indenting the block, you can use delimited blocks, if you like:\\n\\n~~~ define foobar() { print \"Welcome to flavor country!\"; } ~~~\\n\\n(which makes copying & pasting easier). You can optionally mark the delimited block for Pandoc to syntax highlight it:\\n\\n~~~python import time\\n\\nQuick, count to ten!\\n\\nfor i in range(10): # (but not too quick) time.sleep(0.5) print i ~~~\\n\\nAn h3 header\\n\\nNow a nested list:\\n\\nFirst, get these ingredients:\\n\\ncarrots\\n\\ncelery\\n\\nlentils\\n\\nBoil some water.\\n\\nDump everything in the pot and follow this algorithm:\\n\\nfind wooden spoon\\nuncover pot\\nstir\\ncover pot\\nbalance wooden spoon precariously on pot handle\\nwait 10 minutes\\ngoto first step (or shut off burner when done)\\n\\nDo not bump wooden spoon or it will fall.\\n\\nNotice again how text always lines up on 4-space indents (including that last line which continues item 3 above).\\n\\nHere\\'s a link to a website, to a local doc, and to a section heading in the current doc. Here\\'s a footnote [^1].\\n\\n[^1]: Footnote text goes here.\\n\\nTables can look like this:\\n\\nsize material color\\n\\n9 leather brown 10 hemp canvas natural 11 glass transparent\\n\\nTable: Shoes, their sizes, and what they\\'re made of\\n\\n(The above is the caption for the table.) Pandoc also supports multi-line tables:\\n\\nkeyword text\\n\\nred Sunsets, apples, and other red or reddish things.\\n\\ngreen Leaves, grass, frogs and other things it\\'s not easy being.\\n\\nA horizontal rule follows.\\n\\nHere\\'s a definition list:\\n\\napples : Good for making applesauce. oranges : Citrus! tomatoes : There\\'s no \"e\" in tomatoe.\\n\\nAgain, text is indented 4 spaces. (Put a blank line between each term/definition pair to spread things out more.)\\n\\nHere\\'s a \"line block\":\\n\\n| Line one | Line too | Line tree\\n\\nand images can be specified like so:\\n\\nexample image\\n\\nInline math equations go in like so: $\\\\omega = d\\\\phi / dt$. Display math should get its own line and be put in in double-dollarsigns:\\n\\n$$I = \\\\int \\\\rho R^{2} dV$$\\n\\nAnd note that you can backslash-escape any punctuation characters which you wish to be displayed literally, ex.: `foo`, *bar*, etc.')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load from JSON files\n",
        "\n",
        "The JSONLoader uses a specified [jq schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files. It uses the jq python package, which we've installed before.\n"
      ],
      "metadata": {
        "id": "RVbGlf7fJcv6"
      },
      "id": "RVbGlf7fJcv6"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hAmzVJeOUAMHzmhUHNdAUg/facebook-chat.json'"
      ],
      "metadata": {
        "id": "csugInEzJac-",
        "outputId": "242a3534-5c8d-4d1e-b1d2-154cc591d150",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "csugInEzJac-",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-25 20:32:51--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hAmzVJeOUAMHzmhUHNdAUg/facebook-chat.json\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2167 (2.1K) [application/json]\n",
            "Saving to: ‘facebook-chat.json’\n",
            "\n",
            "facebook-chat.json  100%[===================>]   2.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-25 20:32:51 (735 MB/s) - ‘facebook-chat.json’ saved [2167/2167]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path='facebook-chat.json'\n",
        "data = json.loads(Path(file_path).read_text())\n",
        "pprint(data)"
      ],
      "metadata": {
        "id": "2rQ0fQtnJgqk",
        "outputId": "2f7d842f-5ccd-4a4f-b0fe-ff6fffb6bc56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2rQ0fQtnJgqk",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'},\n",
            " 'is_still_participant': True,\n",
            " 'joinable_mode': {'link': '', 'mode': 1},\n",
            " 'magic_words': [],\n",
            " 'messages': [{'content': 'Bye!',\n",
            "               'sender_name': 'User 2',\n",
            "               'timestamp_ms': 1675597571851},\n",
            "              {'content': 'Oh no worries! Bye',\n",
            "               'sender_name': 'User 1',\n",
            "               'timestamp_ms': 1675597435669},\n",
            "              {'content': 'No Im sorry it was my mistake, the blue one is not '\n",
            "                          'for sale',\n",
            "               'sender_name': 'User 2',\n",
            "               'timestamp_ms': 1675596277579},\n",
            "              {'content': 'I thought you were selling the blue one!',\n",
            "               'sender_name': 'User 1',\n",
            "               'timestamp_ms': 1675595140251},\n",
            "              {'content': 'Im not interested in this bag. Im interested in the '\n",
            "                          'blue one!',\n",
            "               'sender_name': 'User 1',\n",
            "               'timestamp_ms': 1675595109305},\n",
            "              {'content': 'Here is $129',\n",
            "               'sender_name': 'User 2',\n",
            "               'timestamp_ms': 1675595068468},\n",
            "              {'photos': [{'creation_timestamp': 1675595059,\n",
            "                           'uri': 'url_of_some_picture.jpg'}],\n",
            "               'sender_name': 'User 2',\n",
            "               'timestamp_ms': 1675595060730},\n",
            "              {'content': 'Online is at least $100',\n",
            "               'sender_name': 'User 2',\n",
            "               'timestamp_ms': 1675595045152},\n",
            "              {'content': 'How much do you want?',\n",
            "               'sender_name': 'User 1',\n",
            "               'timestamp_ms': 1675594799696},\n",
            "              {'content': 'Goodmorning! $50 is too low.',\n",
            "               'sender_name': 'User 2',\n",
            "               'timestamp_ms': 1675577876645},\n",
            "              {'content': 'Hi! Im interested in your bag. Im offering $50. Let '\n",
            "                          'me know if you are interested. Thanks!',\n",
            "               'sender_name': 'User 1',\n",
            "               'timestamp_ms': 1675549022673}],\n",
            " 'participants': [{'name': 'User 1'}, {'name': 'User 2'}],\n",
            " 'thread_path': 'inbox/User 1 and User 2 chat',\n",
            " 'title': 'User 1 and User 2 chat'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use `JSONLoader` to load data from the JSON file. However, JSON files can have various attribute-value pairs. If we want to load a specific attribute and its value, we need to set an appropriate `jq schema`.\n",
        "\n",
        "So for example, if we want to load the `content` from the JSON file, we need to set `jq_schema='.messages[].content'`.\n"
      ],
      "metadata": {
        "id": "a0dUXPiZJpe_"
      },
      "id": "a0dUXPiZJpe_"
    },
    {
      "cell_type": "code",
      "source": [
        "loader = JSONLoader(\n",
        "    file_path=file_path,\n",
        "    jq_schema='.messages[].content',\n",
        "    text_content=False)\n",
        "\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "iKj5nUBxJr6u",
        "outputId": "cad8c89d-8984-48fd-f1c9-f868ffa07d72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "id": "iKj5nUBxJr6u",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'JSONLoader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-57d96f074919>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m loader = JSONLoader(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mjq_schema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.messages[].content'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     text_content=False)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'JSONLoader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load from CSV files\n",
        "CSV files are a common format for storing tabular data. The `CSVLoader` provides a convenient way to read and process this data.\n"
      ],
      "metadata": {
        "id": "HWEpdEDkKXJO"
      },
      "id": "HWEpdEDkKXJO"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IygVG_j0M87BM4Z0zFsBMA/mlb-teams-2012.csv'\n",
        "loader = CSVLoader(file_path='mlb-teams-2012.csv')\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "-gSoqaInKaJ2",
        "outputId": "c5e7b3c6-951c-4ed4-fa52-e0fda7ff1e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "id": "-gSoqaInKaJ2",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-25 20:36:59--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IygVG_j0M87BM4Z0zFsBMA/mlb-teams-2012.csv\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 848 [text/csv]\n",
            "Saving to: ‘mlb-teams-2012.csv’\n",
            "\n",
            "mlb-teams-2012.csv  100%[===================>]     848  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-25 20:37:00 (219 MB/s) - ‘mlb-teams-2012.csv’ saved [848/848]\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CSVLoader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a8ff9d6a1fe5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IygVG_j0M87BM4Z0zFsBMA/mlb-teams-2012.csv'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSVLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mlb-teams-2012.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CSVLoader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "A0upntJvKfy9",
        "outputId": "fd7428dd-d7e2-4368-86d1-8c44494561de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "id": "A0upntJvKfy9",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c5d84736ba45>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you load data from a CSV file, the loader typically creates a separate `Document` object for each row of data in the CSV.\n"
      ],
      "metadata": {
        "id": "-OAReNyrKiHQ"
      },
      "id": "-OAReNyrKiHQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### UnstructuredCSVLoader\n",
        "\n",
        "In contrast to `CSVLoader`, which treats each row as an individual document with headers defining the data, `UnstructuredCSVLoader` considers the entire CSV file as a single unstructured table element. This approach is beneficial when you want to analyze the data as a complete table rather than as separate entries.\n"
      ],
      "metadata": {
        "id": "S6y9cJ_YKkh2"
      },
      "id": "S6y9cJ_YKkh2"
    },
    {
      "cell_type": "code",
      "source": [
        "loader = UnstructuredCSVLoader(\n",
        "    file_path=\"mlb-teams-2012.csv\", mode=\"elements\"\n",
        ")\n",
        "data = loader.load()\n",
        "data[0].page_content"
      ],
      "metadata": {
        "id": "NWDVusmOKgeX",
        "outputId": "85cc1378-8ac1-43e3-f597-016de61f7077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "id": "NWDVusmOKgeX",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'UnstructuredCSVLoader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3d2ae4e53975>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m loader = UnstructuredCSVLoader(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mlb-teams-2012.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"elements\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'UnstructuredCSVLoader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0].metadata[\"text_as_html\"])"
      ],
      "metadata": {
        "id": "-MS699tmKupX",
        "outputId": "210c441b-bfa7-4579-944f-b8f678e0d5c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "id": "-MS699tmKupX",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-acbb30fd3e1d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_as_html\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}