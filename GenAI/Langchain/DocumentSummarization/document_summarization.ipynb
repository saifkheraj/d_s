{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b73ab6",
      "metadata": {
        "id": "34b73ab6",
        "outputId": "42937ae4-b747-4aca-c847-db75640d30c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summarize Private Documents Using RAG, LangChain, and LLMs**\n",
        "\n",
        "\n",
        "\n",
        "---------\n",
        "\n",
        "So, this project steps you through the fascinating world of LLMs and RAG, starting from the basics of what these technologies are, to building a practical application that can read and summarize documents for you. By the end of this tutorial, you have a working tool capable of processing the pile of documents on your desk, allowing you to focus on making meaningful contributions to your projects sooner.\n"
      ],
      "metadata": {
        "id": "wJh45zH5YybJ"
      },
      "id": "wJh45zH5YybJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background\n",
        "\n",
        "### What is RAG?\n",
        "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as retrieval-augmented generation (RAG). RAG is a technique for augmenting LLM knowledge with additional data, which can be your own data.\n",
        "\n",
        "LLMs can reason about wide-ranging topics, but their knowledge is limited to public data up to the specific point in time that they were trained. If you want to build AI applications that can reason about private data or data introduced after a model’s cut-off date, you must augment the knowledge of the model with the specific information that it needs. The process of bringing and inserting the appropriate information into the model prompt is known as RAG.\n",
        "\n",
        "LangChain has several components that are designed to help build Q&A applications and RAG applications, more generally.\n",
        "\n",
        "### RAG architecture\n",
        "A typical RAG application has two main components:\n",
        "\n",
        "* **Indexing**: A pipeline for ingesting and indexing data from a source. This usually happens offline.\n",
        "\n",
        "* **Retrieval and generation**: The actual RAG chain takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
        "\n",
        "The most common full sequence from raw data to answer looks like the following examples.\n"
      ],
      "metadata": {
        "id": "u35_Y6xqZKGe"
      },
      "id": "u35_Y6xqZKGe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Indexing**\n",
        "1. Load: First, you must load your data. This is done with [DocumentLoaders](https://python.langchain.com/docs/how_to/#document-loaders).\n",
        "\n",
        "2. Split: [Text splitters](https://python.langchain.com/docs/how_to/#text-splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and for passing it into a model because large chunks are harder to search and won’t fit in a model’s finite context window.\n",
        "\n",
        "3. Store: You need somewhere to store and index your splits so that they can later be searched. This is often done using a [VectorStore](https://python.langchain.com/docs/how_to/#vector-stores) and [Embeddings](https://python.langchain.com/docs/how_to/embed_text/) model.\n",
        "\n",
        "\n",
        "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WEE3pjeJvSZP0R7UL7CYTA.png\" width=\"50%\" alt=\"indexing\"/> <br>\n",
        "<span style=\"font-size: 10px;\">[source](https://python.langchain.com/docs/tutorials/rag/)</span>\n",
        "\n",
        "\n",
        "- **Retrieval and generation**\n",
        "1. Retrieve: Given a user input, relevant splits are retrieved from storage using a retriever.\n",
        "2. Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data.\n",
        "\n",
        "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/SwPO26VeaC8VTZwtmWh5TQ.png\" width=\"50%\" alt=\"retrieval\"/> <br>\n",
        "<span style=\"font-size: 10px;\">[source](https://python.langchain.com/docs/use_cases/question_answering/)</span>\n"
      ],
      "metadata": {
        "id": "XPRALFJVZRac"
      },
      "id": "XPRALFJVZRac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce0b72d3",
      "metadata": {
        "id": "ce0b72d3"
      },
      "outputs": [],
      "source": [
        "print(\"hello\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}