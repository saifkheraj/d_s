{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp4l-UQPG_WS"
      },
      "source": [
        "## Document Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh2L4oUbG_WW",
        "outputId": "0bda30b9-3961-4799-fd0c-dade3c7b6b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Collecting torch==2.0.1+cpu\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.0.1%2Bcpu-cp310-cp310-linux_x86_64.whl (195.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.2+cpu\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.15.2%2Bcpu-cp310-cp310-linux_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.15.2+cpu\n",
            "  Downloading https://download.pytorch.org/whl/torchtext-0.15.2%2Bcpu-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cpu) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cpu) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cpu) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cpu) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cpu) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cpu) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cpu) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cpu) (11.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2+cpu) (4.67.1)\n",
            "Collecting torchdata==0.6.1 (from torchtext==0.15.2+cpu)\n",
            "  Downloading https://download.pytorch.org/whl/torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext==0.15.2+cpu) (2.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1+cpu) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cpu) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cpu) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cpu) (2024.12.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1+cpu) (1.3.0)\n",
            "Installing collected packages: torch, torchvision, torchdata, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.10.1\n",
            "    Uninstalling torchdata-0.10.1:\n",
            "      Successfully uninstalled torchdata-0.10.1\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.18.0\n",
            "    Uninstalling torchtext-0.18.0:\n",
            "      Successfully uninstalled torchtext-0.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.1+cpu which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.1+cpu torchdata-0.6.1 torchtext-0.15.2+cpu torchvision-0.15.2+cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install -qq torchtext\n",
        "!pip install -qq torchdata\n",
        "!pip install torch==2.0.1+cpu torchvision==0.15.2+cpu torchtext==0.15.2+cpu --index-url https://download.pytorch.org/whl/cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNfy4B8cK87P",
        "outputId": "dfb10a48-e8a6-4841-e7eb-843dc189ee9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import accumulate\n",
        "import matplotlib.pyplot as plt\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from IPython.display import Markdown as md\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.graph_objs as go\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "-aIybIzAHA5b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(COST,ACC):\n",
        "    fig, ax1 = plt.subplots()\n",
        "    color = 'tab:red'\n",
        "    ax1.plot(COST, color=color)\n",
        "    ax1.set_xlabel('epoch', color=color)\n",
        "    ax1.set_ylabel('total loss', color=color)\n",
        "    ax1.tick_params(axis='y', color=color)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:blue'\n",
        "    ax2.set_ylabel('accuracy', color=color)  # you already handled the x-label with ax1\n",
        "    ax2.plot(ACC, color=color)\n",
        "    ax2.tick_params(axis='y', color=color)\n",
        "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wPS5rC5fKpS8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating iterator and checking text, associated labels"
      ],
      "metadata": {
        "id": "GRJk4svJMtRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter= iter(AG_NEWS(split=\"train\"))"
      ],
      "metadata": {
        "id": "j4IOjXZAKtID"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = sum(1 for _ in train_iter)  # Count the number of items\n",
        "print(f\"Size of train_iter: {size}\")"
      ],
      "metadata": {
        "id": "G6MzE5gPrdWM",
        "outputId": "e92d2217-54a5-4701-a687-a43173fc2c6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of train_iter: 120000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter= iter(AG_NEWS(split=\"train\"))\n",
        "y,text= next((train_iter))\n",
        "print(y,text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW0ea2oULBk4",
        "outputId": "738c6aaa-a224-41fb-c66d-3cd2f0cb55c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next((train_iter)) ## we can use next and keep iterating and get label, text"
      ],
      "metadata": {
        "id": "OPlaFPv2tVJg",
        "outputId": "ce5f3325-cd10-4585-8263-85f6842999a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,\n",
              " 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
        "ag_news_label[y]"
      ],
      "metadata": {
        "id": "c0U4_nkpMngH",
        "outputId": "f62ff729-4828-4466-b8a0-392c1e410afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Business'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_class = len(set([label for (label, text) in train_iter ]))\n",
        "num_class"
      ],
      "metadata": {
        "id": "R9DGOi7SM3kw",
        "outputId": "6db9918b-82fa-45fe-b0ab-4cb066af57cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "THfWT9yyy9FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "1iO7i7iY2z9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is an Iterable?\n",
        "\n",
        "Definition: An iterable is any Python object that can be looped over (iterated through). It contains elements that you can access one at a time.\n",
        "Key Property: An iterable implements the __iter__() method, which returns an iterator.\n",
        "\n",
        "Examples: Lists, tuples, dictionaries, strings, and objects that define __iter__() or __getitem__(). How to Identify an Iterable\n",
        "\n",
        "You can pass an iterable to iter() to get an iterator. **AG_NEWS is an iterable object**\n",
        "\n",
        "The AG_NEWS dataset in torchtext does not support direct indexing like a list or tuple. It is not a random access dataset but rather an iterable dataset that needs to be used with an iterator. This approach is more effective for text data.\n",
        "\n"
      ],
      "metadata": {
        "id": "t6-LMaz1y9ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Reinitialize train_iter, loads AG_NEWS dataset which contains labels and text, without iter. AG_NEWS is an iterable object\n",
        "train_iter = AG_NEWS(split=\"train\")\n",
        "\n",
        "\n",
        "# Define tokenizer and yield_tokens\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# The purpose of the generator function yield_tokens is to yield tokenized texts one at a time.\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text.lower())  # Lowercase conversion for consistency"
      ],
      "metadata": {
        "id": "3VMo2O75NCf9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we had initalized AG News with iter and then called yield_tokens then it will give you list of tokens for one sentence at a time and then calling next(yield_tokens(train_iter)) will give next sentence list of tokens.\n",
        "\n",
        "What build_vocab_from_iterator Expects?\n",
        "\n",
        "The function build_vocab_from_iterator works with any iterable that provides tokens one at a time. It does not require an explicit iterator.\n",
        "It will internally convert the iterable into an iterator using iter() if necessary."
      ],
      "metadata": {
        "id": "kJnVvu5tuXW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build vocabulary, unk for unknown words\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Print the vocabulary size and sample tokens\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Sample tokens: {list(vocab.get_stoi().keys())[:10]}\")\n"
      ],
      "metadata": {
        "id": "FmAwGfQXrCG6",
        "outputId": "8a7a1e04-2294-471e-bc17-6d2e8fff2219",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 95811\n",
            "Sample tokens: ['zzz', 'zygmunt', 'zwiki', 'zvidauri', 'zurine', 'zurab', 'zuo', 'zuloaga', 'zovko', 'zotinca']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab([\"age\",\"hello\"]) ## get token indices"
      ],
      "metadata": {
        "id": "AEE44_H7vNHU",
        "outputId": "2d6778ed-32e1-4ba2-ae6f-9c4ce4b13d10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2120, 12544]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Steps to\n",
        "\n",
        "Load the dataset: train_iter and test_iter hold training and test data.\n",
        "Convert to map-style datasets: Make datasets compatible with random access (train_dataset and test_dataset).\n",
        "Split the training dataset:\n",
        "95% for training (split_train_).\n",
        "5% for validation (split_valid_).\n",
        "Prepare for GPU/CPU: Ensures that the training process utilizes GPU if available, otherwise defaults to CPU.\n",
        "\n",
        "\n",
        "**PyTorch supports two types of datasets**:\n",
        "\n",
        "Iterable-style datasets: Provide samples one by one (like train_iter).\n",
        "Map-style datasets: Allow indexing (e.g., train_dataset[0] returns the first sample).\n",
        "\n",
        "to_map_style_dataset converts the iterable-style dataset (train_iter) into a map-style dataset (train_dataset) so it can be indexed and used with functions like random_split."
      ],
      "metadata": {
        "id": "mVZZFGkr37f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing iterators.\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "\n",
        "# Convert the training and testing iterators to map-style datasets.\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "# Determine the number of samples to be used for training and validation (5% for validation).\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "\n",
        "# Randomly split the training dataset into training and validation datasets using `random_split`.\n",
        "# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.\n",
        "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
      ],
      "metadata": {
        "id": "XZJOi80wQUhR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wAjAE95IEUwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "DJag7wjbQ-3H",
        "outputId": "f7fc964c-65c0-43c6-9d23-28e30f168217",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader\n",
        "Prepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to process the raw data strings from the dataset iterators.\n",
        "\n",
        "\n",
        "The function text_pipeline will tokenize the input text, and vocab will then be applied to get the token indices. The label_pipeline will ensure that the labels start at zero."
      ],
      "metadata": {
        "id": "7VDBpaLURPCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_pipeline(x):\n",
        "  ### tokenizer as we have already seen tokenizes the text and vocab converts these tokens into numerical indices as we have seen above.\n",
        "  ## this is the preprocessing step. we first tokenize and create vocab indices exact same way using vocab defined already.\n",
        "  return vocab(tokenizer(x))\n",
        "\n",
        "def label_pipeline(x):\n",
        "   ## Its purpose is to convert raw labels into numerical format\n",
        "   return int(x) - 1"
      ],
      "metadata": {
        "id": "M2j7F6l-RAqk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, the collate_fn function is used in conjunction with data loaders to customize the way batches are created from individual samples. The provided code defines a collate_batch function in PyTorch, which is used with data loaders to customize batch creation from individual samples. It processes a batch of data, including labels and text sequences. It applies the label_pipeline and text_pipeline functions to preprocess the labels and texts, respectively. The processed data is then converted into PyTorch tensors and returned as a tuple containing the label tensor, text tensor, and offsets tensor representing the starting positions of each text sequence in the combined tensor.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PqNMm_8bU8MS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for _label, _text in batch:\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)"
      ],
      "metadata": {
        "id": "mHOUMDEhU-Ru"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_train_"
      ],
      "metadata": {
        "id": "yeeZTyKbVfhf",
        "outputId": "a8196bc7-5a45-4429-f922-16a9cd692a4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataset.Subset at 0x7990cb7849d0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
        ")"
      ],
      "metadata": {
        "id": "RcELJwn_Va7y"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cNO1C9GnUZli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label, text, offsets=next(iter(valid_dataloader ))\n",
        "label, text, offsets"
      ],
      "metadata": {
        "id": "99UOBAw-LFVN",
        "outputId": "b4006f35-9d2e-4b5f-cf26-e5bc137c78ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([3, 1, 2, 3, 3, 2, 0, 0, 3, 1, 2, 3, 3, 1, 1, 2, 2, 2, 3, 3, 3, 0, 0, 1,\n",
              "         1, 3, 1, 3, 2, 0, 0, 2, 1, 2, 3, 0, 1, 1, 2, 3, 2, 3, 0, 2, 0, 0, 1, 1,\n",
              "         3, 0, 0, 3, 0, 2, 2, 2, 3, 1, 2, 1, 0, 3, 2, 3]),\n",
              " tensor([  195,  1840,    12,  ..., 11901, 24528,     1]),\n",
              " tensor([   0,   47,   86,  121,  167,  239,  285,  333,  362,  434,  466,  514,\n",
              "          568,  613,  656,  698,  721,  761,  804,  856,  919,  975, 1004, 1046,\n",
              "         1095, 1132, 1177, 1220, 1243, 1286, 1331, 1367, 1424, 1459, 1511, 1560,\n",
              "         1608, 1648, 1677, 1719, 1748, 1800, 1840, 1887, 1922, 1997, 2051, 2087,\n",
              "         2125, 2179, 2223, 2257, 2314, 2354, 2401, 2463, 2523, 2570, 2610, 2652,\n",
              "         2704, 2751, 2807, 2844]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label, text, offsets=next(iter(valid_dataloader ))\n",
        "label, text, offsets"
      ],
      "metadata": {
        "id": "utZfbCY8UJxf",
        "outputId": "c0b935b2-4c6d-44c3-b613-e888b4d16c7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([2, 2, 2, 1, 2, 2, 3, 3, 3, 3, 1, 0, 0, 3, 0, 3, 2, 3, 0, 2, 0, 0, 0, 2,\n",
              "         2, 3, 3, 3, 2, 0, 3, 0, 0, 2, 0, 3, 2, 2, 3, 1, 2, 1, 0, 0, 3, 2, 0, 3,\n",
              "         3, 1, 3, 3, 2, 0, 2, 1, 3, 3, 3, 1, 2, 0, 2, 2]),\n",
              " tensor([202, 649, 178,  ...,   1, 616,   1]),\n",
              " tensor([   0,   32,   73,  108,  138,  170,  208,  245,  292,  339,  396,  447,\n",
              "          509,  570,  598,  659,  699,  731,  775,  806,  846,  903,  934,  993,\n",
              "         1029, 1060, 1085, 1135, 1177, 1210, 1265, 1297, 1336, 1379, 1416, 1447,\n",
              "         1484, 1534, 1578, 1608, 1650, 1690, 1740, 1793, 1837, 1865, 1898, 1939,\n",
              "         1977, 2061, 2124, 2178, 2208, 2245, 2300, 2348, 2394, 2438, 2473, 2521,\n",
              "         2560, 2592, 2651, 2683]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### As we can see we have variable length data not padded because we will be using embedding bag\n",
        "for label, text, offsets in valid_dataloader:\n",
        "    #print(text)\n",
        "    print(len(text))\n"
      ],
      "metadata": {
        "id": "NA6IG6kFUcyx",
        "outputId": "57bb02a0-3bdf-4593-ede5-33281149f5e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2721\n",
            "2773\n",
            "2788\n",
            "2869\n",
            "2816\n",
            "2852\n",
            "2624\n",
            "2821\n",
            "2838\n",
            "2677\n",
            "2893\n",
            "2765\n",
            "2836\n",
            "2805\n",
            "2819\n",
            "2816\n",
            "2758\n",
            "2800\n",
            "2839\n",
            "2831\n",
            "2876\n",
            "2743\n",
            "2816\n",
            "2841\n",
            "2673\n",
            "2989\n",
            "3024\n",
            "2641\n",
            "2646\n",
            "2681\n",
            "2889\n",
            "2641\n",
            "2700\n",
            "2869\n",
            "2656\n",
            "2744\n",
            "2773\n",
            "2697\n",
            "2770\n",
            "2648\n",
            "2770\n",
            "2804\n",
            "2702\n",
            "2897\n",
            "2875\n",
            "2796\n",
            "2592\n",
            "2871\n",
            "2604\n",
            "2634\n",
            "2529\n",
            "2675\n",
            "2794\n",
            "2791\n",
            "2746\n",
            "2708\n",
            "2818\n",
            "2895\n",
            "2682\n",
            "2496\n",
            "2713\n",
            "2794\n",
            "2745\n",
            "2883\n",
            "2973\n",
            "2937\n",
            "2744\n",
            "2757\n",
            "2752\n",
            "2634\n",
            "2765\n",
            "2592\n",
            "2805\n",
            "2804\n",
            "2928\n",
            "2690\n",
            "2872\n",
            "2852\n",
            "2621\n",
            "2990\n",
            "2752\n",
            "2897\n",
            "2893\n",
            "2725\n",
            "2868\n",
            "2699\n",
            "2622\n",
            "2772\n",
            "2800\n",
            "2936\n",
            "2711\n",
            "2860\n",
            "2621\n",
            "1941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "65iEkdxoV4Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why is Padding Usually Needed?\n",
        "\n",
        "In text processing tasks with models like RNNs or Transformers, padding is required when:\n",
        "\n",
        "Sentences have variable lengths.\n",
        "\n",
        "Since models process inputs in batches, all sentences in a batch must have the same length. Padding ensures this by adding extra tokens (e.g., <pad>) to shorter sentences.\n",
        "\n",
        "\n",
        "2. Why No Padding with nn.EmbeddingBag?\n",
        "\n",
        "nn.EmbeddingBag handles variable-length sequences in a more efficient way, so padding is unnecessary. Instead, it uses:\n",
        "\n",
        "A flattened input tensor (text): This contains all the token indices for the entire batch, concatenated into a single tensor.\n",
        "\n",
        "An offsets tensor (offsets): This marks the starting index of each sentence in the flattened tensor.\n",
        "\n",
        "3. How nn.EmbeddingBag Works Without Padding\n",
        "\n",
        "Consider a batch of 3 sentences:\n",
        "\n",
        "Sentence 1: [1, 2, 3]\n",
        "\n",
        "Sentence 2: [4, 5]\n",
        "\n",
        "Sentence 3: [6, 7, 8, 9]\n",
        "\n",
        "Instead of padding, nn.EmbeddingBag:\n",
        "\n",
        "Flattens the sentences into a single tensor:\n",
        "\n",
        "text = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "Creates an offsets tensor to indicate where each sentence starts:\n",
        "\n",
        "offsets = [0, 3, 5]  # Sentence 1 starts at index 0, Sentence 2 at index 3, Sentence 3 at index 5\n",
        "\n",
        "During the forward pass:\n",
        "\n",
        "nn.EmbeddingBag uses the offsets to determine which tokens belong to each sentence.\n",
        "\n",
        "It directly computes the aggregated embedding for each sentence by summing or averaging the embeddings of the tokens (without needing padding).\n",
        "\n",
        "4. Advantages of Not Using Padding\n",
        "\n",
        "Efficiency:\n",
        "\n",
        "Padding adds unnecessary computations for the padded tokens, especially if sentences have highly variable lengths.\n",
        "nn.EmbeddingBag avoids this by only processing actual tokens, leading to faster and more memory-efficient training.\n",
        "\n",
        "Simplified Preprocessing:\n",
        "\n",
        "You don’t need to manually pad sentences to the same length.\n",
        "The collate_batch function only needs to flatten the token indices and create the offsets tensor, which is simpler than handling padding."
      ],
      "metadata": {
        "id": "bVMaZ6T0V4-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "metadata": {
        "id": "3yNHhp3qMz87"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emsize=64\n",
        "vocab_size=len(vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "kpq2F7wCWZfy",
        "outputId": "08c6aedc-d8b1-4de5-fb26-0d77b37592b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95811"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_class"
      ],
      "metadata": {
        "id": "KFoeSq8sWe9M",
        "outputId": "af1967d7-12b1-48e1-bc95-d261b7d7183a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## creating model\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "7qpr7wTzWhlR",
        "outputId": "0e65ab80-a2ee-4758-d166-de58344fc2ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextClassificationModel(\n",
              "  (embedding): EmbeddingBag(95811, 64, mode='mean')\n",
              "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code line predicted_label=model(text, offsets) is used to obtain predicted labels from a machine learning model for a given input text and its corresponding offsets. The model is the machine learning model being used for text classification or similar tasks."
      ],
      "metadata": {
        "id": "AGF94W9-Wm5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_label=model(text, offsets)"
      ],
      "metadata": {
        "id": "fO1ptvzwWmsw"
      },
      "execution_count": 33,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}