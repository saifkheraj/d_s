{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rp4l-UQPG_WS"
   },
   "source": [
    "## Document Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sh2L4oUbG_WW",
    "outputId": "0bda30b9-3961-4799-fd0c-dade3c7b6b06",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qq torchtext\n",
    "!pip install -qq torchdata\n",
    "!pip install torch==2.0.1+cpu torchvision==0.15.2+cpu torchtext==0.15.2+cpu --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNfy4B8cK87P",
    "outputId": "dfb10a48-e8a6-4841-e7eb-843dc189ee9f"
   },
   "outputs": [],
   "source": [
    "!pip install portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aIybIzAHA5b"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import accumulate\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from IPython.display import Markdown as md\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPS5rC5fKpS8"
   },
   "outputs": [],
   "source": [
    "def plot(COST,ACC):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoch', color=color)\n",
    "    ax1.set_ylabel('total loss', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)  # you already handled the x-label with ax1\n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRJk4svJMtRj"
   },
   "source": [
    "### Creating iterator and checking text, associated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4IOjXZAKtID"
   },
   "outputs": [],
   "source": [
    "train_iter= iter(AG_NEWS(split=\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6MzE5gPrdWM",
    "outputId": "e92d2217-54a5-4701-a687-a43173fc2c6c"
   },
   "outputs": [],
   "source": [
    "size = sum(1 for _ in train_iter)  # Count the number of items\n",
    "print(f\"Size of train_iter: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW0ea2oULBk4",
    "outputId": "738c6aaa-a224-41fb-c66d-3cd2f0cb55c5"
   },
   "outputs": [],
   "source": [
    "train_iter= iter(AG_NEWS(split=\"train\"))\n",
    "y,text= next((train_iter))\n",
    "print(y,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OPlaFPv2tVJg",
    "outputId": "ce5f3325-cd10-4585-8263-85f6842999a9"
   },
   "outputs": [],
   "source": [
    "next((train_iter)) ## we can use next and keep iterating and get label, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "c0U4_nkpMngH",
    "outputId": "f62ff729-4828-4466-b8a0-392c1e410afd"
   },
   "outputs": [],
   "source": [
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "ag_news_label[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9DGOi7SM3kw",
    "outputId": "6db9918b-82fa-45fe-b0ab-4cb066af57cc"
   },
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter ]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THfWT9yyy9FM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iO7i7iY2z9B"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6-LMaz1y9ut"
   },
   "source": [
    "1. What is an Iterable?\n",
    "\n",
    "Definition: An iterable is any Python object that can be looped over (iterated through). It contains elements that you can access one at a time.\n",
    "Key Property: An iterable implements the __iter__() method, which returns an iterator.\n",
    "\n",
    "Examples: Lists, tuples, dictionaries, strings, and objects that define __iter__() or __getitem__(). How to Identify an Iterable\n",
    "\n",
    "You can pass an iterable to iter() to get an iterator. **AG_NEWS is an iterable object**\n",
    "\n",
    "The AG_NEWS dataset in torchtext does not support direct indexing like a list or tuple. It is not a random access dataset but rather an iterable dataset that needs to be used with an iterator. This approach is more effective for text data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VMo2O75NCf9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reinitialize train_iter, loads AG_NEWS dataset which contains labels and text, without iter. AG_NEWS is an iterable object\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "\n",
    "\n",
    "# Define tokenizer and yield_tokens\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# The purpose of the generator function yield_tokens is to yield tokenized texts one at a time.\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text.lower())  # Lowercase conversion for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJnVvu5tuXW_"
   },
   "source": [
    "If we had initalized AG News with iter and then called yield_tokens then it will give you list of tokens for one sentence at a time and then calling next(yield_tokens(train_iter)) will give next sentence list of tokens.\n",
    "\n",
    "What build_vocab_from_iterator Expects?\n",
    "\n",
    "The function build_vocab_from_iterator works with any iterable that provides tokens one at a time. It does not require an explicit iterator.\n",
    "It will internally convert the iterable into an iterator using iter() if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FmAwGfQXrCG6",
    "outputId": "8a7a1e04-2294-471e-bc17-6d2e8fff2219"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build vocabulary, unk for unknown words\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Print the vocabulary size and sample tokens\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Sample tokens: {list(vocab.get_stoi().keys())[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEE44_H7vNHU",
    "outputId": "2d6778ed-32e1-4ba2-ae6f-9c4ce4b13d10"
   },
   "outputs": [],
   "source": [
    "vocab([\"age\",\"hello\"]) ## get token indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVZZFGkr37f6"
   },
   "source": [
    "### Next Steps to\n",
    "\n",
    "Load the dataset: train_iter and test_iter hold training and test data.\n",
    "Convert to map-style datasets: Make datasets compatible with random access (train_dataset and test_dataset).\n",
    "Split the training dataset:\n",
    "95% for training (split_train_).\n",
    "5% for validation (split_valid_).\n",
    "Prepare for GPU/CPU: Ensures that the training process utilizes GPU if available, otherwise defaults to CPU.\n",
    "\n",
    "\n",
    "**PyTorch supports two types of datasets**:\n",
    "\n",
    "Iterable-style datasets: Provide samples one by one (like train_iter).\n",
    "Map-style datasets: Allow indexing (e.g., train_dataset[0] returns the first sample).\n",
    "\n",
    "to_map_style_dataset converts the iterable-style dataset (train_iter) into a map-style dataset (train_dataset) so it can be indexed and used with functions like random_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZJOi80wQUhR"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing iterators.\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "\n",
    "# Convert the training and testing iterators to map-style datasets.\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Determine the number of samples to be used for training and validation (5% for validation).\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "# Randomly split the training dataset into training and validation datasets using `random_split`.\n",
    "# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAjAE95IEUwk"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJag7wjbQ-3H",
    "outputId": "f7fc964c-65c0-43c6-9d23-28e30f168217"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VDBpaLURPCE"
   },
   "source": [
    "## Dataloader\n",
    "Prepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to process the raw data strings from the dataset iterators.\n",
    "\n",
    "\n",
    "The function text_pipeline will tokenize the input text, and vocab will then be applied to get the token indices. The label_pipeline will ensure that the labels start at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2j7F6l-RAqk"
   },
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "  ### tokenizer as we have already seen tokenizes the text and vocab converts these tokens into numerical indices as we have seen above.\n",
    "  ## this is the preprocessing step. we first tokenize and create vocab indices exact same way using vocab defined already.\n",
    "  return vocab(tokenizer(x))\n",
    "\n",
    "def label_pipeline(x):\n",
    "   ## Its purpose is to convert raw labels into numerical format\n",
    "   return int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqNMm_8bU8MS"
   },
   "source": [
    "In PyTorch, the collate_fn function is used in conjunction with data loaders to customize the way batches are created from individual samples. The provided code defines a collate_batch function in PyTorch, which is used with data loaders to customize batch creation from individual samples. It processes a batch of data, including labels and text sequences. It applies the label_pipeline and text_pipeline functions to preprocess the labels and texts, respectively. The processed data is then converted into PyTorch tensors and returned as a tuple containing the label tensor, text tensor, and offsets tensor representing the starting positions of each text sequence in the combined tensor.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHOUMDEhU-Ru"
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeeZTyKbVfhf",
    "outputId": "a8196bc7-5a45-4429-f922-16a9cd692a4a"
   },
   "outputs": [],
   "source": [
    "split_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcELJwn_Va7y"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNO1C9GnUZli"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99UOBAw-LFVN",
    "outputId": "b4006f35-9d2e-4b5f-cf26-e5bc137c78ff"
   },
   "outputs": [],
   "source": [
    "label, text, offsets=next(iter(valid_dataloader ))\n",
    "label, text, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utZfbCY8UJxf",
    "outputId": "c0b935b2-4c6d-44c3-b613-e888b4d16c7f"
   },
   "outputs": [],
   "source": [
    "label, text, offsets=next(iter(valid_dataloader ))\n",
    "label, text, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NA6IG6kFUcyx",
    "outputId": "57bb02a0-3bdf-4593-ede5-33281149f5e4"
   },
   "outputs": [],
   "source": [
    "### As we can see we have variable length data not padded because we will be using embedding bag\n",
    "for label, text, offsets in valid_dataloader:\n",
    "    #print(text)\n",
    "    print(len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65iEkdxoV4Z6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVMaZ6T0V4-D"
   },
   "source": [
    "1. Why is Padding Usually Needed?\n",
    "\n",
    "In text processing tasks with models like RNNs or Transformers, padding is required when:\n",
    "\n",
    "Sentences have variable lengths.\n",
    "\n",
    "Since models process inputs in batches, all sentences in a batch must have the same length. Padding ensures this by adding extra tokens (e.g., <pad>) to shorter sentences.\n",
    "\n",
    "\n",
    "2. Why No Padding with nn.EmbeddingBag?\n",
    "\n",
    "nn.EmbeddingBag handles variable-length sequences in a more efficient way, so padding is unnecessary. Instead, it uses:\n",
    "\n",
    "A flattened input tensor (text): This contains all the token indices for the entire batch, concatenated into a single tensor.\n",
    "\n",
    "An offsets tensor (offsets): This marks the starting index of each sentence in the flattened tensor.\n",
    "\n",
    "3. How nn.EmbeddingBag Works Without Padding\n",
    "\n",
    "Consider a batch of 3 sentences:\n",
    "\n",
    "Sentence 1: [1, 2, 3]\n",
    "\n",
    "Sentence 2: [4, 5]\n",
    "\n",
    "Sentence 3: [6, 7, 8, 9]\n",
    "\n",
    "Instead of padding, nn.EmbeddingBag:\n",
    "\n",
    "Flattens the sentences into a single tensor:\n",
    "\n",
    "text = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "Creates an offsets tensor to indicate where each sentence starts:\n",
    "\n",
    "offsets = [0, 3, 5]  # Sentence 1 starts at index 0, Sentence 2 at index 3, Sentence 3 at index 5\n",
    "\n",
    "During the forward pass:\n",
    "\n",
    "nn.EmbeddingBag uses the offsets to determine which tokens belong to each sentence.\n",
    "\n",
    "It directly computes the aggregated embedding for each sentence by summing or averaging the embeddings of the tokens (without needing padding).\n",
    "\n",
    "4. Advantages of Not Using Padding\n",
    "\n",
    "Efficiency:\n",
    "\n",
    "Padding adds unnecessary computations for the padded tokens, especially if sentences have highly variable lengths.\n",
    "nn.EmbeddingBag avoids this by only processing actual tokens, leading to faster and more memory-efficient training.\n",
    "\n",
    "Simplified Preprocessing:\n",
    "\n",
    "You don’t need to manually pad sentences to the same length.\n",
    "The collate_batch function only needs to flatten the token indices and create the offsets tensor, which is simpler than handling padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yNHhp3qMz87"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpq2F7wCWZfy",
    "outputId": "08c6aedc-d8b1-4de5-fb26-0d77b37592b7"
   },
   "outputs": [],
   "source": [
    "emsize=64\n",
    "vocab_size=len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFoeSq8sWe9M",
    "outputId": "af1967d7-12b1-48e1-bc95-d261b7d7183a"
   },
   "outputs": [],
   "source": [
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qpr7wTzWhlR",
    "outputId": "0e65ab80-a2ee-4758-d166-de58344fc2ca"
   },
   "outputs": [],
   "source": [
    "## creating model\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGF94W9-Wm5X"
   },
   "source": [
    "The code line predicted_label=model(text, offsets) is used to obtain predicted labels from a machine learning model for a given input text and its corresponding offsets. The model is the machine learning model being used for text classification or similar tasks.\n",
    "\n",
    "it triggers the forward() method of the TextClassificationModel class. In PyTorch, calling a model instance like model(text, offsets) implicitly invokes the forward() method defined in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fO1ptvzwWmsw"
   },
   "outputs": [],
   "source": [
    "predicted_label=model(text, offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NE7SgULZYvBk",
    "outputId": "b7d24646-41b8-496c-d886-017b043b7d61"
   },
   "outputs": [],
   "source": [
    "predicted_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_m2BdNorbTXf"
   },
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0])) ### predict and then return the max\n",
    "        return ag_news_label[output.argmax(1).item() + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FItA-TExb1l6",
    "outputId": "3c7665fa-7e85-44dd-a356-00b690146320"
   },
   "outputs": [],
   "source": [
    "predict(\"I like sports\",text_pipeline )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcQI5JPYifgc"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYWvXjybicuo"
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count= 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kY-Y6c4ilBb",
    "outputId": "4ea7a8ad-9d50-4d4c-c857-fad331eb10a2"
   },
   "outputs": [],
   "source": [
    "evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIibm-yUioqH"
   },
   "source": [
    "The model was evaluated, and it was found that its performance is no better than average. This outcome is expected, considering that the model has not undergone any training yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYCjgTSfj-YN"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Set the learning rate (LR) to 0.1, which determines the step size at which the optimizer updates the model's parameters during training. The CrossEntropyLoss criterion is used to calculate the loss between the model's predicted outputs and the ground truth labels. This loss function is commonly employed for multi-class classification tasks.\n",
    "\n",
    "The chosen optimizer is Stochastic Gradient Descent (SGD), which optimizes the model's parameters based on the computed gradients with respect to the loss function. The SGD optimizer uses the specified learning rate to control the size of the weight updates.\n",
    "\n",
    "Additionally, a learning rate scheduler is defined using StepLR. This scheduler adjusts the learning rate during training, reducing it by a factor (gamma) of 0.1 after every epoch (step) to improve convergence and fine-tune the model's performance. These components together form the essential setup for training a neural network using the specified learning rate, loss criterion, optimizer, and learning rate scheduler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQQYlCEgj5--"
   },
   "outputs": [],
   "source": [
    "LR=0.1\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oX1GnDfWkX0e",
    "outputId": "2b6e5ba8-408d-4245-ff96-6413f4df6c23"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "cum_loss_list=[]\n",
    "acc_epoch=[]\n",
    "acc_old=0\n",
    "\n",
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    model.train()\n",
    "    cum_loss=0\n",
    "    for idx, (label, text, offsets) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        cum_loss+=loss.item()\n",
    "\n",
    "    cum_loss_list.append(cum_loss)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    acc_epoch.append(accu_val)\n",
    "\n",
    "    if accu_val > acc_old:\n",
    "      acc_old= accu_val\n",
    "      torch.save(model.state_dict(), 'my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "FvE2bO6_kiHy",
    "outputId": "e28467f3-bb83-4ff5-8eab-25c1ed3befcb"
   },
   "outputs": [],
   "source": [
    "plot(cum_loss_list,acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWljJvolkkd6",
    "outputId": "6ee7faf3-b327-4095-d46c-ce3420a38096"
   },
   "outputs": [],
   "source": [
    "evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqM7Gpsrkr2B"
   },
   "source": [
    "This code snippet provides a summary for generating a 3D t-SNE visualization of embeddings using Plotly. It demonstrates how words that are similar to each other are positioned closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "MYHvG1X-kt6D",
    "outputId": "bb47fad4-ffa5-4dc0-a2da-2b12fbc06aa8"
   },
   "outputs": [],
   "source": [
    "# Get the first batch from the validation data\n",
    "batch = next(iter(valid_dataloader))\n",
    "\n",
    "# Extract the text and offsets from the batch\n",
    "label, text, offsets = batch\n",
    "\n",
    "# Send the data to the device (GPU if available)\n",
    "text = text.to(device)\n",
    "offsets = offsets.to(device)\n",
    "\n",
    "# Get the embeddings bag output for the batch\n",
    "embedded = model.embedding(text, offsets)\n",
    "\n",
    "# Convert the embeddings tensor to a numpy array\n",
    "embeddings_numpy = embedded.detach().cpu().numpy()\n",
    "\n",
    "# Perform t-SNE on the embeddings to reduce their dimensionality to 3D.\n",
    "X_embedded_3d = TSNE(n_components=3).fit_transform(embeddings_numpy)\n",
    "\n",
    "# Create a 3D scatter plot using Plotly\n",
    "trace = go.Scatter3d(\n",
    "    x=X_embedded_3d[:, 0],\n",
    "    y=X_embedded_3d[:, 1],\n",
    "    z=X_embedded_3d[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=label.numpy(),  # Use label information for color\n",
    "        colorscale='Viridis',  # Choose a colorscale\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "layout = go.Layout(title=\"3D t-SNE Visualization of Embeddings\",\n",
    "                   scene=dict(xaxis_title='Dimension 1',\n",
    "                              yaxis_title='Dimension 2',\n",
    "                              zaxis_title='Dimension 3'))\n",
    "\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5OaXA3HlYlw"
   },
   "source": [
    "### Lets make prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkHqyfExlaiz"
   },
   "outputs": [],
   "source": [
    "article=\"\"\"Canada navigated a stiff test against the Republic of Ireland on a rain soaked evening in Perth, coming from behind to claim a vital 2-1 victory at the Women’s World Cup.\n",
    "Katie McCabe opened the scoring with an incredible Olimpico goal – scoring straight from a corner kick – as her corner flew straight over the despairing Canada goalkeeper Kailen Sheridan at Perth Rectangular Stadium in Australia.\n",
    "Just when Ireland thought it had safely navigated itself to half time with a lead, Megan Connolly failed to get a clean connection on a clearance with the resulting contact squirming into her own net to level the score.\n",
    "Minutes into the second half, Adriana Leon completed the turnaround for the Olympic champion, slotting home from the edge of the area to seal the three points.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "hoxw3QmwlgA_",
    "outputId": "2d8251a6-7668-4745-beca-89d3440957d5"
   },
   "outputs": [],
   "source": [
    "result = predict(article, text_pipeline)\n",
    "\n",
    "markdown_content = f'''\n",
    "<div style=\"background-color: lightgray; padding: 10px;\">\n",
    "    <h3>{article}</h3>\n",
    "    <h4>The category of the news article: {result}</h4>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "md(markdown_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9iPiXhxlyLo"
   },
   "outputs": [],
   "source": [
    "\n",
    "###for any new articles, new_artiles is a list of articles with text. Use this to predict\n",
    "for i, article in enumerate(new_articles, start=1):\n",
    "    prediction = predict(article, text_pipeline)\n",
    "    print(f\"Article {i} is classified as: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icKMSWdnj5sJ"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
