# ğŸ“š InstructionÂ Tuning â€” A PracticalÂ README

## Table of Contents

1. [WhatÂ Is InstructionÂ Tuning?](#what-is-instruction-tuning)
2. [Where It Fits in the LLM TrainingÂ Pipeline](#where-it-fits-in-the-llm-training-pipeline)
3. [Dataset AnatomyÂ ğŸ“‘](#dataset-anatomy)
4. [Prompt &Â Specialâ€‘TokenÂ Formats](#prompt--special-token-formats)
5. [InstructionÂ MaskingÂ ğŸ¯](#instruction-masking)
6. [Key DesignÂ Considerations](#key-design-considerations)
7. [Endâ€‘toâ€‘End Example](#end-to-end-example)
8. [TypicalÂ UseÂ Cases](#typical-use-cases)
9. [FurtherÂ Reading &Â Resources](#further-reading--resources)

---

## WhatÂ Is InstructionÂ Tuning?

**Instruction tuning**Â (aka **Supervised Fineâ€‘TuningÂ â€”Â SFT**) means retraining a *preâ€‘trained* language model on a curated set ofÂ ***instructionâ€“response*** pairs written by experts. The fresh supervision teaches the model *how* to follow naturalâ€‘language commands rather than just predicting the next token.

> ğŸ”‘Â **Goal**Â â†’ Improve alignment, taskâ€‘specific accuracy, and safety *before* applying preferenceâ€‘based methods such as RLHF or Direct Preference OptimisationÂ (DPO).

---

## Where It Fits in the LLM TrainingÂ Pipeline

| Stage                            | Objective                            | Data                             | Typical Loss                                        |
| -------------------------------- | ------------------------------------ | -------------------------------- | --------------------------------------------------- |
| 1ï¸âƒ£Â **Preâ€‘training**             | General language knowledge           | 100Â B+ tokens (web, books, code) | Nextâ€‘token, unmasked                                |
| 2ï¸âƒ£Â **InstructionÂ tuningÂ (SFT)** | Teach the model to *follow commands* | â‰¤1Â M highâ€‘quality triples        | Crossâ€‘entropy on answer tokens (masked or unmasked) |
| 3ï¸âƒ£Â **RLHFÂ /Â DPO**               | Align outputs with human preferences | Human comparison data            | Policyâ€‘gradient / Kâ€‘Lâ€‘regularised loss              |

---

## Dataset AnatomyÂ ğŸ“‘

Each record usually contains **three**Â parts:

```yaml
instruction:    "Translate to French."
input:          "Good morning!"     # optional
output:         "BonjourÂ !"
```

* `instruction`Â â†’ Describes the task.
* `input`Â â†’ Context the model must process. May be empty.
* `output`Â â†’ Groundâ€‘truth answer.

Some public corpora (e.g. **Selfâ€‘Instruct**, **Flan**) omit `input` for simpler commands.

---

## Prompt &Â Specialâ€‘TokenÂ Formats

LLMs rely on *special tokens* so the decoder can distinguish instructions from answers.
Popular templates include:

### 1. Tripleâ€‘HashÂ Blocks (Flanâ€‘T5)

````text
### Instruction:
Create a Python function that squares numbers.

### Response:
```python
def f(x):
    return x**2
````

````

### 2. Chatâ€‘Style Roles (LlamaÂ /Â Vicuna)
```text
### Human:
What is the capital of Japan?

### Assistant:
Tokyo.
````

### 3. Explicit `<|prompt|>` Tokens (GPTâ€‘Jâ€‘Tuned)

```text
<|instruction|> Summarise: "The food was awesome!" <|response|> The review is positive.
```

ğŸ’¡Â *Why bother?*Â Using reserved IDs prevents the tokenizer from splitting the tags, ensuring deterministic boundary detection during training and inference.

> **Whitespace matters!**Â Even an invisible `\n` may shift every token position. Always verify with `tokenizer.encode_plus()`.

---

## InstructionÂ MaskingÂ ğŸ¯

During SFT we **donâ€™t** always want to backâ€‘propagate through *every* generated token. *Instruction masking* restricts the loss to the answer span:

```mermaid
sequenceDiagram
    participant T as Tokens
    Note right of T: "### Instruction" â€”Â no loss
    T->>T: Question tokens â€”Â no loss
    T->>T: "### Response" â€”Â no loss
    T->>T: âœ…Â Answer tokens (loss)
    T->>T: âœ…Â <EOS> (loss)
```

### Why Mask?

* Prevents the model from overfitting to the literal wording of instructions.
* Reduces compute â€” fewer tokens in `CrossEntropyLoss`.
* Empirically boosts performance on smaller, highâ€‘quality datasets.

### Implementation Snippet (ğŸ¤—Â Transformers â‰¥Â 4.39)

```python
from transformers import AutoTokenizer, DataCollatorForCompletionOnlyLM

collator = DataCollatorForCompletionOnlyLM(
    tokenizer=tok,
    response_template="### Response:",   # text right before the answer
)
```

*Special tokens (e.g. `<bos>`, `<eos>`) are automatically ignored.*

> âš–ï¸Â *Unmasked vsÂ Masked?*Â Recent papers show that **unmasked** loss can help when your dataset is tiny (<5â€‰k examples) because every gradient counts. Try both.

---

## Key DesignÂ Considerations

1. **Data qualityÂ â‰«Â Data size**: 5â€‰k pristine tasks often beat 500â€‰k noisy ones.
2. **Domain match**: For specialised LLMs (medical, legal, education) craft domainâ€‘specific instructions.
3. **Token budget**: Longer prompts â†’ higher training cost. Use concise yet explicit instructions.
4. **Catastrophic forgetting**: Overâ€‘fineâ€‘tuning can erase skills learnt during preâ€‘training. Use smaller learning rates (1â€¯â€“â€¯2â€¯Ã—â€¯10â»âµ) and early stopping.
5. **Evaluation**: Probe *followâ€‘ability* (e.g.Â HELM, MTâ€‘Bench) rather than perplexity alone.

---

## Endâ€‘toâ€‘End Example

### Task

> **Instruction**: "Answer the question."
>
> **Input**: "Which is the largest ocean?"
>
> **Output**: "The Pacific Ocean."

### Packed Training Sequence

```text
### Instruction:
Answer the question.
### Input:
Which is the largest ocean?
### Response:
The Pacific Ocean.
<|eos|>
```

The model is asked to predict each next token. With masking, gradients are applied **only** on:

```
The â–²Pacific â–²Ocean. â–²<|eos|>
```

(symbol â–² = tokens that contribute to loss)

---

## TypicalÂ UseÂ Cases

| Domain          | Example Instructionâ€‘Tuned Model   | Realâ€‘World Impact                            |
| --------------- | --------------------------------- | -------------------------------------------- |
| Chat Assistants | ChatGPT (GPTâ€‘3.5â€‘Turbo SFT stage) | Safer & more helpful responses               |
| Code Generation | StarCoderBase â†¦ StarChat          | Better instruction compliance for developers |
| Education       | Mathâ€‘Tutor LlamaÂ 7B               | Stepâ€‘byâ€‘step explanations for students       |
| Medicine        | MedAlpaca / Gatortronâ€‘SFT         | Reliable answers to clinical queries         |
| RoboticsÂ &Â IoT  | Inâ€‘house SFT on task specs        | Accurate naturalâ€‘language robot control      |

---

## FurtherÂ Reading &Â Resources

* **Stanford Alpaca**Â paper â€”Â first lowâ€‘cost 7â€‰B instruction tuning.
* **FLANâ€‘T5** â€”Â 62 diverse datasets merged for zeroâ€‘shot gen.
* **"InstructGPT"** â€”Â OpenAIâ€™s original RLHF work (SFT stage details).
* **HuggingFaceâ€¯ğŸ¤— Course â€”Â SFT Chapter**.
* **DataCollatorForCompletionOnlyLM**Â docs.
* **mtâ€‘bench** by LMSys â€”Â instructionâ€‘following benchmark.

---

> ğŸ *Instruction tuning forms the bridge between raw language knowledge and taskâ€‘specific mastery. Master it, and your LLM will listen â€” not just talk.*
