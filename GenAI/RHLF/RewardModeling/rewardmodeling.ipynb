{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ef80e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4a13fba7dd5e47a59d20c1772ecb7829",
      "0d468ec023444fe094efae0dccc5a62c",
      "479aafedabf34da1a6ad5a880c5e37ee",
      "fa1f9f8c95cc479ea2d3ea16d21871b1",
      "d1d19ef7141b4d8aa41d26384fe54f4a",
      "5af13f361b3b47679cda82b615c65d83",
      "c9cbc0d184de4a08b22f95b1aec2c1c7",
      "5d37d3cd772049f9a893d8bc1f41837a",
      "49bea984afec4032827e55c70582d5f6",
      "ec6313f0d64749eaaf3b4807626ff15e",
      "9325e5bef65d46099f21973cae6c7f40"
     ]
    },
    "id": "8e2ef80e",
    "outputId": "610dd2c6-6529-4695-accd-0f9c5a4f96ac"
   },
   "outputs": [],
   "source": [
    "# Reward Modeling with Hugging Face - Complete Google Colab Tutorial\n",
    "# This notebook demonstrates how to train a reward model using Hugging Face\n",
    "\n",
    "# ============================\n",
    "# 1. INSTALLATION AND SETUP\n",
    "# ============================\n",
    "\n",
    "# Install required packages\n",
    "!pip install transformers datasets accelerate peft trl torch matplotlib numpy\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import RewardTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ó Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# ============================\n",
    "# 2. DATASET LOADING AND EXPLORATION\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä LOADING DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load the synthetic instruction dataset from Hugging Face\n",
    "dataset_name = \"Dahoas/synthetic-instruct-gptj-pairwise\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "print(f\"Dataset features: {dataset.features}\")\n",
    "\n",
    "# Display a sample data point to understand the structure\n",
    "print(\"\\nüîç Sample data point:\")\n",
    "sample = dataset[0]\n",
    "print(f\"Prompt: {sample['prompt'][:200]}...\")\n",
    "print(f\"Chosen response: {sample['chosen'][:150]}...\")\n",
    "print(f\"Rejected response: {sample['rejected'][:150]}...\")\n",
    "\n",
    "# ============================\n",
    "# 3. MODEL AND TOKENIZER SETUP\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ü§ñ MODEL AND TOKENIZER SETUP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model_name = \"gpt2\"  # Using GPT-2 as base model for sequence classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model for sequence classification (binary classification for reward modeling)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=1,  # Single output for reward score\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "# Resize token embeddings to account for the new pad token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model_name}\")\n",
    "print(f\"üìù Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"üéØ Model configuration: {model.config.num_labels} label(s)\")\n",
    "\n",
    "# ============================\n",
    "# 4. DATA PREPROCESSING\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîÑ DATA PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Maximum sequence length for tokenization\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "def format_prompt_response(prompt, response):\n",
    "    \"\"\"Format prompt and response with clear delimiters\"\"\"\n",
    "    return f\"Human: {prompt}\\n\\nAssistant: {response}\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset for reward modeling training.\n",
    "    Creates tokenized inputs for both chosen and rejected responses.\n",
    "    \"\"\"\n",
    "    # Format chosen and rejected responses\n",
    "    chosen_texts = [\n",
    "        format_prompt_response(prompt, chosen)\n",
    "        for prompt, chosen in zip(examples[\"prompt\"], examples[\"chosen\"])\n",
    "    ]\n",
    "    rejected_texts = [\n",
    "        format_prompt_response(prompt, rejected)\n",
    "        for prompt, rejected in zip(examples[\"prompt\"], examples[\"rejected\"])\n",
    "    ]\n",
    "\n",
    "    # Tokenize chosen responses\n",
    "    chosen_encodings = tokenizer(\n",
    "        chosen_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Tokenize rejected responses\n",
    "    rejected_encodings = tokenizer(\n",
    "        rejected_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids_chosen\": chosen_encodings[\"input_ids\"],\n",
    "        \"attention_mask_chosen\": chosen_encodings[\"attention_mask\"],\n",
    "        \"input_ids_rejected\": rejected_encodings[\"input_ids\"],\n",
    "        \"attention_mask_rejected\": rejected_encodings[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "print(\"üîÑ Preprocessing dataset...\")\n",
    "\n",
    "# Use a smaller subset for faster training in this demo\n",
    "dataset_subset = dataset.select(range(1000))  # Use first 1000 samples\n",
    "processed_dataset = dataset_subset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_subset.column_names\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Preprocessed {len(processed_dataset)} samples\")\n",
    "print(f\"üìã Processed features: {processed_dataset.features}\")\n",
    "\n",
    "# Split dataset into train and eval\n",
    "train_dataset = processed_dataset.select(range(800))  # 80% for training\n",
    "eval_dataset = processed_dataset.select(range(800, 1000))  # 20% for evaluation\n",
    "\n",
    "print(f\"üéØ Train samples: {len(train_dataset)}\")\n",
    "print(f\"üìä Eval samples: {len(eval_dataset)}\")\n",
    "\n",
    "# ============================\n",
    "# 5. LORA CONFIGURATION\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚öôÔ∏è LORA CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Configure LoRA for parameter-efficient training\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # Rank of adaptation\n",
    "    lora_alpha=32,           # LoRA scaling parameter\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Target modules for GPT-2\n",
    "    lora_dropout=0.1,        # LoRA dropout\n",
    "    bias=\"none\",             # Bias type\n",
    "    task_type=\"SEQ_CLS\",     # Task type: Sequence Classification\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"‚úÖ LoRA configuration applied\")\n",
    "print(f\"üìä Trainable parameters: {model.num_parameters()}\")\n",
    "print(f\"üéØ LoRA parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# ============================\n",
    "# 6. TRAINING CONFIGURATION\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üèãÔ∏è TRAINING CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./reward_model_output\",\n",
    "    per_device_train_batch_size=2,      # Batch size per device\n",
    "    per_device_eval_batch_size=2,       # Evaluation batch size\n",
    "    num_train_epochs=2,                 # Number of training epochs\n",
    "    gradient_accumulation_steps=4,       # Gradient accumulation steps\n",
    "    learning_rate=1.41e-5,              # Learning rate\n",
    "    logging_steps=50,                   # Log every 50 steps\n",
    "    eval_strategy=\"steps\",              # Fixed: was evaluation_strategy\n",
    "    eval_steps=100,                     # Evaluate every 100 steps\n",
    "    save_steps=200,                     # Save checkpoint every 200 steps\n",
    "    warmup_steps=100,                   # Warmup steps\n",
    "    remove_unused_columns=False,        # Keep all columns\n",
    "    dataloader_drop_last=False,\n",
    "    report_to=\"none\",                   # Disable wandb logging for this demo\n",
    ")\n",
    "\n",
    "# Add missing attributes that RewardTrainer expects\n",
    "training_args.disable_dropout = True\n",
    "training_args.max_length = MAX_LENGTH\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "print(f\"üìä Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"üîÑ Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"üìà Learning rate: {training_args.learning_rate}\")\n",
    "\n",
    "# ============================\n",
    "# 7. REWARD TRAINER SETUP AND TRAINING\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ REWARD TRAINER SETUP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Try RewardTrainer with fallback to standard Trainer\n",
    "try:\n",
    "    # Initialize the RewardTrainer\n",
    "    trainer = RewardTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "    print(\"‚úÖ RewardTrainer initialized successfully\")\n",
    "    training_approach = \"reward_trainer\"\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è RewardTrainer error: {e}\")\n",
    "    print(\"üîÑ Using standard Trainer with custom loss function...\")\n",
    "\n",
    "    # Fallback to standard Trainer with custom reward modeling loss\n",
    "    from transformers import Trainer\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    class RewardModelTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            \"\"\"Custom loss function for reward modeling\"\"\"\n",
    "            # Get chosen and rejected inputs\n",
    "            chosen_input_ids = inputs[\"input_ids_chosen\"]\n",
    "            chosen_attention_mask = inputs[\"attention_mask_chosen\"]\n",
    "            rejected_input_ids = inputs[\"input_ids_rejected\"]\n",
    "            rejected_attention_mask = inputs[\"attention_mask_rejected\"]\n",
    "\n",
    "            # Forward pass for chosen responses\n",
    "            chosen_outputs = model(\n",
    "                input_ids=chosen_input_ids,\n",
    "                attention_mask=chosen_attention_mask\n",
    "            )\n",
    "            chosen_rewards = chosen_outputs.logits.squeeze(-1)\n",
    "\n",
    "            # Forward pass for rejected responses\n",
    "            rejected_outputs = model(\n",
    "                input_ids=rejected_input_ids,\n",
    "                attention_mask=rejected_attention_mask\n",
    "            )\n",
    "            rejected_rewards = rejected_outputs.logits.squeeze(-1)\n",
    "\n",
    "            # Reward modeling loss: chosen should have higher reward than rejected\n",
    "            loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()\n",
    "\n",
    "            if return_outputs:\n",
    "                return loss, {\"chosen_rewards\": chosen_rewards, \"rejected_rewards\": rejected_rewards}\n",
    "            return loss\n",
    "\n",
    "    def reward_data_collator(features):\n",
    "        \"\"\"Custom data collator for reward modeling\"\"\"\n",
    "        batch = {}\n",
    "        for key in features[0].keys():\n",
    "            if key.startswith('input_ids') or key.startswith('attention_mask'):\n",
    "                batch[key] = torch.stack([f[key] for f in features])\n",
    "        return batch\n",
    "\n",
    "    trainer = RewardModelTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=reward_data_collator,\n",
    "    )\n",
    "    print(\"‚úÖ Custom RewardModelTrainer initialized successfully\")\n",
    "    training_approach = \"custom_trainer\"\n",
    "\n",
    "# Start training\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "print(\"This may take several minutes depending on your hardware...\")\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "\n",
    "    # Save the final model\n",
    "    trainer.save_model(\"./final_reward_model\")\n",
    "    print(\"üíæ Model saved to ./final_reward_model\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training error: {e}\")\n",
    "    print(\"üí° Try reducing batch size or number of epochs if you encounter memory issues\")\n",
    "\n",
    "# ============================\n",
    "# 8. MODEL EVALUATION\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä MODEL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def get_reward_score(text):\n",
    "    \"\"\"Get reward score for a given text\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        score = outputs.logits.squeeze().item()\n",
    "\n",
    "    return score\n",
    "\n",
    "def evaluate_model_performance(num_samples=50):\n",
    "    \"\"\"Evaluate model performance by comparing chosen vs rejected responses\"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_comparisons = 0\n",
    "\n",
    "    print(f\"üîç Evaluating on {num_samples} samples...\")\n",
    "\n",
    "    for i in range(min(num_samples, len(eval_dataset))):\n",
    "        # Get original data\n",
    "        original_sample = dataset[i + 800]  # Offset for eval set\n",
    "\n",
    "        # Format texts\n",
    "        chosen_text = format_prompt_response(original_sample[\"prompt\"], original_sample[\"chosen\"])\n",
    "        rejected_text = format_prompt_response(original_sample[\"prompt\"], original_sample[\"rejected\"])\n",
    "\n",
    "        # Get scores\n",
    "        chosen_score = get_reward_score(chosen_text)\n",
    "        rejected_score = get_reward_score(rejected_text)\n",
    "\n",
    "        # Check if model prefers chosen over rejected\n",
    "        if chosen_score > rejected_score:\n",
    "            correct_predictions += 1\n",
    "\n",
    "        total_comparisons += 1\n",
    "\n",
    "        # Print some examples\n",
    "        if i < 3:\n",
    "            print(f\"\\nüìù Example {i+1}:\")\n",
    "            print(f\"Prompt: {original_sample['prompt'][:100]}...\")\n",
    "            print(f\"Chosen score: {chosen_score:.4f}\")\n",
    "            print(f\"Rejected score: {rejected_score:.4f}\")\n",
    "            print(f\"Correct prediction: {chosen_score > rejected_score}\")\n",
    "\n",
    "    win_rate = correct_predictions / total_comparisons * 100\n",
    "    return win_rate, correct_predictions, total_comparisons\n",
    "\n",
    "# Run evaluation\n",
    "try:\n",
    "    win_rate, correct, total = evaluate_model_performance(50)\n",
    "\n",
    "    print(f\"\\nüéØ EVALUATION RESULTS:\")\n",
    "    print(f\"Win Rate: {win_rate:.2f}%\")\n",
    "    print(f\"Correct Predictions: {correct}/{total}\")\n",
    "\n",
    "    if win_rate > 60:\n",
    "        print(\"üéâ Great! Your model shows good performance!\")\n",
    "    elif win_rate > 50:\n",
    "        print(\"üëç Decent performance! Consider more training for improvement.\")\n",
    "    else:\n",
    "        print(\"üîÑ Model needs more training. Try increasing epochs or adjusting hyperparameters.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation error: {e}\")\n",
    "\n",
    "# ============================\n",
    "# 9. INTERACTIVE TESTING\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéÆ INTERACTIVE TESTING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def compare_responses(prompt, response1, response2):\n",
    "    \"\"\"Compare two responses to a given prompt\"\"\"\n",
    "    text1 = format_prompt_response(prompt, response1)\n",
    "    text2 = format_prompt_response(prompt, response2)\n",
    "\n",
    "    score1 = get_reward_score(text1)\n",
    "    score2 = get_reward_score(text2)\n",
    "\n",
    "    print(f\"üìù Prompt: {prompt}\")\n",
    "    print(f\"\\nüÖ∞Ô∏è Response 1: {response1}\")\n",
    "    print(f\"Score: {score1:.4f}\")\n",
    "    print(f\"\\nüÖ±Ô∏è Response 2: {response2}\")\n",
    "    print(f\"Score: {score2:.4f}\")\n",
    "\n",
    "    if score1 > score2:\n",
    "        print(f\"\\nüèÜ Winner: Response 1 (higher by {score1 - score2:.4f})\")\n",
    "    elif score2 > score1:\n",
    "        print(f\"\\nüèÜ Winner: Response 2 (higher by {score2 - score1:.4f})\")\n",
    "    else:\n",
    "        print(f\"\\nü§ù Tie! Both responses have similar scores\")\n",
    "\n",
    "# Test with sample responses\n",
    "test_prompt = \"How do I learn programming?\"\n",
    "good_response = \"Start with the basics: choose a beginner-friendly language like Python, practice regularly with small projects, and use online resources like tutorials and coding exercises. Don't be afraid to make mistakes - they're part of the learning process!\"\n",
    "bad_response = \"Just figure it out yourself. Programming is hard and not for everyone.\"\n",
    "\n",
    "print(\"üß™ Testing with sample responses:\")\n",
    "compare_responses(test_prompt, good_response, bad_response)\n",
    "\n",
    "# ============================\n",
    "# 10. VISUALIZATION AND SUMMARY\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìà TRAINING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Plot training loss if available\n",
    "try:\n",
    "    if hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n",
    "        train_losses = [log['train_loss'] for log in trainer.state.log_history if 'train_loss' in log]\n",
    "\n",
    "        if train_losses:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(train_losses, label='Training Loss', color='blue', linewidth=2)\n",
    "            plt.title('üèãÔ∏è Reward Model Training Loss')\n",
    "            plt.xlabel('Training Steps')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"üìâ Final training loss: {train_losses[-1]:.4f}\")\n",
    "        else:\n",
    "            print(\"üìä No training loss data available for plotting\")\n",
    "    else:\n",
    "        print(\"üìä Training history not available\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"üìä Could not plot training loss: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ REWARD MODELING TUTORIAL COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "üìö What we accomplished:\n",
    "‚úÖ Loaded synthetic instruction dataset from Hugging Face\n",
    "‚úÖ Preprocessed data for reward modeling\n",
    "‚úÖ Configured GPT-2 with LoRA for efficient training\n",
    "‚úÖ Trained a reward model using RewardTrainer\n",
    "‚úÖ Evaluated model performance with win rate metrics\n",
    "‚úÖ Created interactive testing capabilities\n",
    "\n",
    "üöÄ Next steps you can try:\n",
    "- Experiment with different base models (GPT-3.5, LLaMA, etc.)\n",
    "- Adjust LoRA configuration parameters\n",
    "- Try different learning rates and batch sizes\n",
    "- Use larger datasets for better performance\n",
    "- Implement more sophisticated evaluation metrics\n",
    "\n",
    "üí° Tips for better results:\n",
    "- Use more training data (we used only 1000 samples for demo)\n",
    "- Train for more epochs with a larger dataset\n",
    "- Fine-tune hyperparameters based on your specific use case\n",
    "- Consider using models pre-trained on instruction-following tasks\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüîó Useful resources:\")\n",
    "print(\"- Hugging Face Transformers: https://huggingface.co/transformers/\")\n",
    "print(\"- TRL (Transformer Reinforcement Learning): https://github.com/lvwerra/trl\")\n",
    "print(\"- PEFT (Parameter-Efficient Fine-Tuning): https://github.com/huggingface/peft\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d468ec023444fe094efae0dccc5a62c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5af13f361b3b47679cda82b615c65d83",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c9cbc0d184de4a08b22f95b1aec2c1c7",
      "value": "Map:‚Äá100%"
     }
    },
    "479aafedabf34da1a6ad5a880c5e37ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d37d3cd772049f9a893d8bc1f41837a",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_49bea984afec4032827e55c70582d5f6",
      "value": 1000
     }
    },
    "49bea984afec4032827e55c70582d5f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4a13fba7dd5e47a59d20c1772ecb7829": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0d468ec023444fe094efae0dccc5a62c",
       "IPY_MODEL_479aafedabf34da1a6ad5a880c5e37ee",
       "IPY_MODEL_fa1f9f8c95cc479ea2d3ea16d21871b1"
      ],
      "layout": "IPY_MODEL_d1d19ef7141b4d8aa41d26384fe54f4a"
     }
    },
    "5af13f361b3b47679cda82b615c65d83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d37d3cd772049f9a893d8bc1f41837a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9325e5bef65d46099f21973cae6c7f40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c9cbc0d184de4a08b22f95b1aec2c1c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1d19ef7141b4d8aa41d26384fe54f4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec6313f0d64749eaaf3b4807626ff15e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa1f9f8c95cc479ea2d3ea16d21871b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec6313f0d64749eaaf3b4807626ff15e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9325e5bef65d46099f21973cae6c7f40",
      "value": "‚Äá1000/1000‚Äá[00:00&lt;00:00,‚Äá1385.38‚Äáexamples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
