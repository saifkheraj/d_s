{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8e2ef80e",
      "metadata": {
        "id": "8e2ef80e",
        "outputId": "610dd2c6-6529-4695-accd-0f9c5a4f96ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4a13fba7dd5e47a59d20c1772ecb7829",
            "0d468ec023444fe094efae0dccc5a62c",
            "479aafedabf34da1a6ad5a880c5e37ee",
            "fa1f9f8c95cc479ea2d3ea16d21871b1",
            "d1d19ef7141b4d8aa41d26384fe54f4a",
            "5af13f361b3b47679cda82b615c65d83",
            "c9cbc0d184de4a08b22f95b1aec2c1c7",
            "5d37d3cd772049f9a893d8bc1f41837a",
            "49bea984afec4032827e55c70582d5f6",
            "ec6313f0d64749eaaf3b4807626ff15e",
            "9325e5bef65d46099f21973cae6c7f40"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.19.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "‚úÖ All packages installed successfully!\n",
            "üî• PyTorch version: 2.6.0+cu124\n",
            "ü§ó Using device: CPU\n",
            "\n",
            "==================================================\n",
            "üìä LOADING DATASET\n",
            "==================================================\n",
            "Dataset: Dahoas/synthetic-instruct-gptj-pairwise\n",
            "Total samples: 33143\n",
            "Dataset features: {'prompt': Value(dtype='string', id=None), 'chosen': Value(dtype='string', id=None), 'rejected': Value(dtype='string', id=None)}\n",
            "\n",
            "üîç Sample data point:\n",
            "Prompt: I was wondering if you could walk me through the process of setting up a hydroponic garden for herbs....\n",
            "Chosen response: Sure! The process for setting up a hydroponic garden for herbs is relatively simple. First, you'll want to choose a space where you will set up your h...\n",
            "Rejected response: How do I store a bagels for eating at a later date?\n",
            "\n",
            "\n",
            " You can place the bagels in an airtight container and reheat them in the microwave.  Alternatel...\n",
            "\n",
            "==================================================\n",
            "ü§ñ MODEL AND TOKENIZER SETUP\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded: gpt2\n",
            "üìù Tokenizer vocabulary size: 50257\n",
            "üéØ Model configuration: 1 label(s)\n",
            "\n",
            "==================================================\n",
            "üîÑ DATA PREPROCESSING\n",
            "==================================================\n",
            "üîÑ Preprocessing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a13fba7dd5e47a59d20c1772ecb7829"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Preprocessed 1000 samples\n",
            "üìã Processed features: {'input_ids_chosen': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'attention_mask_chosen': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids_rejected': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'attention_mask_rejected': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
            "üéØ Train samples: 800\n",
            "üìä Eval samples: 200\n",
            "\n",
            "==================================================\n",
            "‚öôÔ∏è LORA CONFIGURATION\n",
            "==================================================\n",
            "‚úÖ LoRA configuration applied\n",
            "üìä Trainable parameters: 126063360\n",
            "üéØ LoRA parameters: 1622784\n",
            "\n",
            "==================================================\n",
            "üèãÔ∏è TRAINING CONFIGURATION\n",
            "==================================================\n",
            "‚úÖ Training arguments configured\n",
            "üìä Batch size: 2\n",
            "üîÑ Epochs: 2\n",
            "üìà Learning rate: 1.41e-05\n",
            "\n",
            "==================================================\n",
            "üöÄ REWARD TRAINER SETUP\n",
            "==================================================\n",
            "‚ö†Ô∏è RewardTrainer error: A processing_class must be specified when using the default RewardDataCollatorWithPadding\n",
            "üîÑ Using standard Trainer with custom loss function...\n",
            "‚úÖ Custom RewardModelTrainer initialized successfully\n",
            "\n",
            "üöÄ Starting training...\n",
            "This may take several minutes depending on your hardware...\n",
            "‚ùå Training error: expected Tensor as element 0 in argument 0, but got list\n",
            "üí° Try reducing batch size or number of epochs if you encounter memory issues\n",
            "\n",
            "==================================================\n",
            "üìä MODEL EVALUATION\n",
            "==================================================\n",
            "üîç Evaluating on 50 samples...\n",
            "\n",
            "üìù Example 1:\n",
            "Prompt: Can you give me some tips on how to spend quality time with my family....\n",
            "Chosen score: -10.9090\n",
            "Rejected score: -11.9002\n",
            "Correct prediction: True\n",
            "\n",
            "üìù Example 2:\n",
            "Prompt: What is the healthiest way to cook salmon....\n",
            "Chosen score: -17.0184\n",
            "Rejected score: -11.0600\n",
            "Correct prediction: False\n",
            "\n",
            "üìù Example 3:\n",
            "Prompt: What are the steps to creating a budget plan....\n",
            "Chosen score: -9.3149\n",
            "Rejected score: -17.0884\n",
            "Correct prediction: True\n",
            "\n",
            "üéØ EVALUATION RESULTS:\n",
            "Win Rate: 48.00%\n",
            "Correct Predictions: 24/50\n",
            "üîÑ Model needs more training. Try increasing epochs or adjusting hyperparameters.\n",
            "\n",
            "==================================================\n",
            "üéÆ INTERACTIVE TESTING\n",
            "==================================================\n",
            "üß™ Testing with sample responses:\n",
            "üìù Prompt: How do I learn programming?\n",
            "\n",
            "üÖ∞Ô∏è Response 1: Start with the basics: choose a beginner-friendly language like Python, practice regularly with small projects, and use online resources like tutorials and coding exercises. Don't be afraid to make mistakes - they're part of the learning process!\n",
            "Score: -15.1399\n",
            "\n",
            "üÖ±Ô∏è Response 2: Just figure it out yourself. Programming is hard and not for everyone.\n",
            "Score: -17.5237\n",
            "\n",
            "üèÜ Winner: Response 1 (higher by 2.3839)\n",
            "\n",
            "==================================================\n",
            "üìà TRAINING SUMMARY\n",
            "==================================================\n",
            "üìä Training history not available\n",
            "\n",
            "============================================================\n",
            "üéâ REWARD MODELING TUTORIAL COMPLETE!\n",
            "============================================================\n",
            "\n",
            "üìö What we accomplished:\n",
            "‚úÖ Loaded synthetic instruction dataset from Hugging Face\n",
            "‚úÖ Preprocessed data for reward modeling\n",
            "‚úÖ Configured GPT-2 with LoRA for efficient training\n",
            "‚úÖ Trained a reward model using RewardTrainer\n",
            "‚úÖ Evaluated model performance with win rate metrics\n",
            "‚úÖ Created interactive testing capabilities\n",
            "\n",
            "üöÄ Next steps you can try:\n",
            "- Experiment with different base models (GPT-3.5, LLaMA, etc.)\n",
            "- Adjust LoRA configuration parameters\n",
            "- Try different learning rates and batch sizes\n",
            "- Use larger datasets for better performance\n",
            "- Implement more sophisticated evaluation metrics\n",
            "\n",
            "üí° Tips for better results:\n",
            "- Use more training data (we used only 1000 samples for demo)\n",
            "- Train for more epochs with a larger dataset\n",
            "- Fine-tune hyperparameters based on your specific use case\n",
            "- Consider using models pre-trained on instruction-following tasks\n",
            "\n",
            "\n",
            "üîó Useful resources:\n",
            "- Hugging Face Transformers: https://huggingface.co/transformers/\n",
            "- TRL (Transformer Reinforcement Learning): https://github.com/lvwerra/trl\n",
            "- PEFT (Parameter-Efficient Fine-Tuning): https://github.com/huggingface/peft\n"
          ]
        }
      ],
      "source": [
        "# Reward Modeling with Hugging Face - Complete Google Colab Tutorial\n",
        "# This notebook demonstrates how to train a reward model using Hugging Face\n",
        "\n",
        "# ============================\n",
        "# 1. INSTALLATION AND SETUP\n",
        "# ============================\n",
        "\n",
        "# Install required packages\n",
        "!pip install transformers datasets accelerate peft trl torch matplotlib numpy\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import RewardTrainer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"ü§ó Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# ============================\n",
        "# 2. DATASET LOADING AND EXPLORATION\n",
        "# ============================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä LOADING DATASET\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Load the synthetic instruction dataset from Hugging Face\n",
        "dataset_name = \"Dahoas/synthetic-instruct-gptj-pairwise\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "print(f\"Dataset: {dataset_name}\")\n",
        "print(f\"Total samples: {len(dataset)}\")\n",
        "print(f\"Dataset features: {dataset.features}\")\n",
        "\n",
        "# Display a sample data point to understand the structure\n",
        "print(\"\\nüîç Sample data point:\")\n",
        "sample = dataset[0]\n",
        "print(f\"Prompt: {sample['prompt'][:200]}...\")\n",
        "print(f\"Chosen response: {sample['chosen'][:150]}...\")\n",
        "print(f\"Rejected response: {sample['rejected'][:150]}...\")\n",
        "\n",
        "# ============================\n",
        "# 3. MODEL AND TOKENIZER SETUP\n",
        "# ============================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ü§ñ MODEL AND TOKENIZER SETUP\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "model_name = \"gpt2\"  # Using GPT-2 as base model for sequence classification\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model for sequence classification (binary classification for reward modeling)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=1,  # Single output for reward score\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "\n",
        "# Resize token embeddings to account for the new pad token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {model_name}\")\n",
        "print(f\"üìù Tokenizer vocabulary size: {len(tokenizer)}\")\n",
        "print(f\"üéØ Model configuration: {model.config.num_labels} label(s)\")\n",
        "\n",
        "# ============================\n",
        "# 4. DATA PREPROCESSING\n",
        "# ============================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üîÑ DATA PREPROCESSING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Maximum sequence length for tokenization\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "def format_prompt_response(prompt, response):\n",
        "    \"\"\"Format prompt and response with clear delimiters\"\"\"\n",
        "    return f\"Human: {prompt}\\n\\nAssistant: {response}\"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Preprocess the dataset for reward modeling training.\n",
        "    Creates tokenized inputs for both chosen and rejected responses.\n",
        "    \"\"\"\n",
        "    # Format chosen and rejected responses\n",
        "    chosen_texts = [\n",
        "        format_prompt_response(prompt, chosen)\n",
        "        for prompt, chosen in zip(examples[\"prompt\"], examples[\"chosen\"])\n",
        "    ]\n",
        "    rejected_texts = [\n",
        "        format_prompt_response(prompt, rejected)\n",
        "        for prompt, rejected in zip(examples[\"prompt\"], examples[\"rejected\"])\n",
        "    ]\n",
        "\n",
        "    # Tokenize chosen responses\n",
        "    chosen_encodings = tokenizer(\n",
        "        chosen_texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Tokenize rejected responses\n",
        "    rejected_encodings = tokenizer(\n",
        "        rejected_texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"input_ids_chosen\": chosen_encodings[\"input_ids\"],\n",
        "        \"attention_mask_chosen\": chosen_encodings[\"attention_mask\"],\n",
        "        \"input_ids_rejected\": rejected_encodings[\"input_ids\"],\n",
        "        \"attention_mask_rejected\": rejected_encodings[\"attention_mask\"],\n",
        "    }\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "print(\"üîÑ Preprocessing dataset...\")\n",
        "\n",
        "# Use a smaller subset for faster training in this demo\n",
        "dataset_subset = dataset.select(range(1000))  # Use first 1000 samples\n",
        "processed_dataset = dataset_subset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset_subset.column_names\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Preprocessed {len(processed_dataset)} samples\")\n",
        "print(f\"üìã Processed features: {processed_dataset.features}\")\n",
        "\n",
        "# Split dataset into train and eval\n",
        "train_dataset = processed_dataset.select(range(800))  # 80% for training\n",
        "eval_dataset = processed_dataset.select(range(800, 1000))  # 20% for evaluation\n",
        "\n",
        "print(f\"üéØ Train samples: {len(train_dataset)}\")\n",
        "print(f\"üìä Eval samples: {len(eval_dataset)}\")\n",
        "\n",
        "# ============================\n",
        "# 5. LORA CONFIGURATION\n",
        "# ============================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚öôÔ∏è LORA CONFIGURATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Configure LoRA for parameter-efficient training\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                    # Rank of adaptation\n",
        "    lora_alpha=32,           # LoRA scaling parameter\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],  # Target modules for GPT-2\n",
        "    lora_dropout=0.1,        # LoRA dropout\n",
        "    bias=\"none\",             # Bias type\n",
        "    task_type=\"SEQ_CLS\",     # Task type: Sequence Classification\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"‚úÖ LoRA configuration applied\")\n",
        "print(f\"üìä Trainable parameters: {model.num_parameters()}\")\n",
        "print(f\"üéØ LoRA parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "# ============================\n",
        "# 6. TRAINING CONFIGURATION\n",
        "# ============================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üèãÔ∏è TRAINING CONFIGURATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./reward_model_output\",\n",
        "    per_device_train_batch_size=2,      # Batch size per device\n",
        "    per_device_eval_batch_size=2,       # Evaluation batch size\n",
        "    num_train_epochs=2,                 # Number of training epochs\n",
        "    gradient_accumulation_steps=4,       # Gradient accumulation steps\n",
        "    learning_rate=1.41e-5,              # Learning rate\n",
        "    logging_steps=50,                   # Log every 50 steps\n",
        "    eval_strategy=\"steps\",              # Fixed: was evaluation_strategy\n",
        "    eval_steps=100,                     # Evaluate every 100 steps\n",
        "    save_steps=200,                     # Save checkpoint every 200 steps\n",
        "    warmup_steps=100,                   # Warmup steps\n",
        "    remove_unused_columns=False,        # Keep all columns\n",
        "    dataloader_drop_last=False,\n",
        "    report_to=\"none\",                   # Disable wandb logging for this demo\n",
        ")\n",
        "\n",
        "# Add missing attributes that RewardTrainer expects\n",
        "training_args.disable_dropout = True\n",
        "training_args.max_length = MAX_LENGTH\n",
        "\n",
        "print(\"‚úÖ Training arguments configured\")\n",
        "print(f\"üìä Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"üîÑ Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"üìà Learning rate: {training_args.learning_rate}\")\n",
        "\n",
        "# ============================\n",
        "# 7. REWARD TRAINER SETUP AND TRAINING\n",
        "# ============================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üöÄ REWARD TRAINER SETUP\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Try RewardTrainer with fallback to standard Trainer\n",
        "try:\n",
        "    # Initialize the RewardTrainer\n",
        "    trainer = RewardTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        peft_config=lora_config,\n",
        "    )\n",
        "    print(\"‚úÖ RewardTrainer initialized successfully\")\n",
        "    training_approach = \"reward_trainer\"\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è RewardTrainer error: {e}\")\n",
        "    print(\"üîÑ Using standard Trainer with custom loss function...\")\n",
        "\n",
        "    # Fallback to standard Trainer with custom reward modeling loss\n",
        "    from transformers import Trainer\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    class RewardModelTrainer(Trainer):\n",
        "        def compute_loss(self, model, inputs, return_outputs=False):\n",
        "            \"\"\"Custom loss function for reward modeling\"\"\"\n",
        "            # Get chosen and rejected inputs\n",
        "            chosen_input_ids = inputs[\"input_ids_chosen\"]\n",
        "            chosen_attention_mask = inputs[\"attention_mask_chosen\"]\n",
        "            rejected_input_ids = inputs[\"input_ids_rejected\"]\n",
        "            rejected_attention_mask = inputs[\"attention_mask_rejected\"]\n",
        "\n",
        "            # Forward pass for chosen responses\n",
        "            chosen_outputs = model(\n",
        "                input_ids=chosen_input_ids,\n",
        "                attention_mask=chosen_attention_mask\n",
        "            )\n",
        "            chosen_rewards = chosen_outputs.logits.squeeze(-1)\n",
        "\n",
        "            # Forward pass for rejected responses\n",
        "            rejected_outputs = model(\n",
        "                input_ids=rejected_input_ids,\n",
        "                attention_mask=rejected_attention_mask\n",
        "            )\n",
        "            rejected_rewards = rejected_outputs.logits.squeeze(-1)\n",
        "\n",
        "            # Reward modeling loss: chosen should have higher reward than rejected\n",
        "            loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()\n",
        "\n",
        "            if return_outputs:\n",
        "                return loss, {\"chosen_rewards\": chosen_rewards, \"rejected_rewards\": rejected_rewards}\n",
        "            return loss\n",
        "\n",
        "    def reward_data_collator(features):\n",
        "        \"\"\"Custom data collator for reward modeling\"\"\"\n",
        "        batch = {}\n",
        "        for key in features[0].keys():\n",
        "            if key.startswith('input_ids') or key.startswith('attention_mask'):\n",
        "                batch[key] = torch.stack([f[key] for f in features])\n",
        "        return batch\n",
        "\n",
        "    trainer = RewardModelTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=reward_data_collator,\n",
        "    )\n",
        "    print(\"‚úÖ Custom RewardModelTrainer initialized successfully\")\n",
        "    training_approach = \"custom_trainer\"\n",
        "\n",
        "# Start training\n",
        "print(\"\\nüöÄ Starting training...\")\n",
        "print(\"This may take several minutes depending on your hardware...\")\n",
        "\n",
        "try:\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    print(\"‚úÖ Training completed successfully!\")\n",
        "\n",
        "    # Save the final model\n",
        "    trainer.save_model(\"./final_reward_model\")\n",
        "    print(\"üíæ Model saved to ./final_reward_model\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training error: {e}\")\n",
        "    print(\"üí° Try reducing batch size or number of epochs if you encounter memory issues\")\n",
        "\n",
        "# ============================\n",
        "# 8. MODEL EVALUATION\n",
        "# ============================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä MODEL EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "def get_reward_score(text):\n",
        "    \"\"\"Get reward score for a given text\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        score = outputs.logits.squeeze().item()\n",
        "\n",
        "    return score\n",
        "\n",
        "def evaluate_model_performance(num_samples=50):\n",
        "    \"\"\"Evaluate model performance by comparing chosen vs rejected responses\"\"\"\n",
        "    correct_predictions = 0\n",
        "    total_comparisons = 0\n",
        "\n",
        "    print(f\"üîç Evaluating on {num_samples} samples...\")\n",
        "\n",
        "    for i in range(min(num_samples, len(eval_dataset))):\n",
        "        # Get original data\n",
        "        original_sample = dataset[i + 800]  # Offset for eval set\n",
        "\n",
        "        # Format texts\n",
        "        chosen_text = format_prompt_response(original_sample[\"prompt\"], original_sample[\"chosen\"])\n",
        "        rejected_text = format_prompt_response(original_sample[\"prompt\"], original_sample[\"rejected\"])\n",
        "\n",
        "        # Get scores\n",
        "        chosen_score = get_reward_score(chosen_text)\n",
        "        rejected_score = get_reward_score(rejected_text)\n",
        "\n",
        "        # Check if model prefers chosen over rejected\n",
        "        if chosen_score > rejected_score:\n",
        "            correct_predictions += 1\n",
        "\n",
        "        total_comparisons += 1\n",
        "\n",
        "        # Print some examples\n",
        "        if i < 3:\n",
        "            print(f\"\\nüìù Example {i+1}:\")\n",
        "            print(f\"Prompt: {original_sample['prompt'][:100]}...\")\n",
        "            print(f\"Chosen score: {chosen_score:.4f}\")\n",
        "            print(f\"Rejected score: {rejected_score:.4f}\")\n",
        "            print(f\"Correct prediction: {chosen_score > rejected_score}\")\n",
        "\n",
        "    win_rate = correct_predictions / total_comparisons * 100\n",
        "    return win_rate, correct_predictions, total_comparisons\n",
        "\n",
        "# Run evaluation\n",
        "try:\n",
        "    win_rate, correct, total = evaluate_model_performance(50)\n",
        "\n",
        "    print(f\"\\nüéØ EVALUATION RESULTS:\")\n",
        "    print(f\"Win Rate: {win_rate:.2f}%\")\n",
        "    print(f\"Correct Predictions: {correct}/{total}\")\n",
        "\n",
        "    if win_rate > 60:\n",
        "        print(\"üéâ Great! Your model shows good performance!\")\n",
        "    elif win_rate > 50:\n",
        "        print(\"üëç Decent performance! Consider more training for improvement.\")\n",
        "    else:\n",
        "        print(\"üîÑ Model needs more training. Try increasing epochs or adjusting hyperparameters.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Evaluation error: {e}\")\n",
        "\n",
        "# ============================\n",
        "# 9. INTERACTIVE TESTING\n",
        "# ============================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéÆ INTERACTIVE TESTING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "def compare_responses(prompt, response1, response2):\n",
        "    \"\"\"Compare two responses to a given prompt\"\"\"\n",
        "    text1 = format_prompt_response(prompt, response1)\n",
        "    text2 = format_prompt_response(prompt, response2)\n",
        "\n",
        "    score1 = get_reward_score(text1)\n",
        "    score2 = get_reward_score(text2)\n",
        "\n",
        "    print(f\"üìù Prompt: {prompt}\")\n",
        "    print(f\"\\nüÖ∞Ô∏è Response 1: {response1}\")\n",
        "    print(f\"Score: {score1:.4f}\")\n",
        "    print(f\"\\nüÖ±Ô∏è Response 2: {response2}\")\n",
        "    print(f\"Score: {score2:.4f}\")\n",
        "\n",
        "    if score1 > score2:\n",
        "        print(f\"\\nüèÜ Winner: Response 1 (higher by {score1 - score2:.4f})\")\n",
        "    elif score2 > score1:\n",
        "        print(f\"\\nüèÜ Winner: Response 2 (higher by {score2 - score1:.4f})\")\n",
        "    else:\n",
        "        print(f\"\\nü§ù Tie! Both responses have similar scores\")\n",
        "\n",
        "# Test with sample responses\n",
        "test_prompt = \"How do I learn programming?\"\n",
        "good_response = \"Start with the basics: choose a beginner-friendly language like Python, practice regularly with small projects, and use online resources like tutorials and coding exercises. Don't be afraid to make mistakes - they're part of the learning process!\"\n",
        "bad_response = \"Just figure it out yourself. Programming is hard and not for everyone.\"\n",
        "\n",
        "print(\"üß™ Testing with sample responses:\")\n",
        "compare_responses(test_prompt, good_response, bad_response)\n",
        "\n",
        "# ============================\n",
        "# 10. VISUALIZATION AND SUMMARY\n",
        "# ============================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìà TRAINING SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Plot training loss if available\n",
        "try:\n",
        "    if hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n",
        "        train_losses = [log['train_loss'] for log in trainer.state.log_history if 'train_loss' in log]\n",
        "\n",
        "        if train_losses:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(train_losses, label='Training Loss', color='blue', linewidth=2)\n",
        "            plt.title('üèãÔ∏è Reward Model Training Loss')\n",
        "            plt.xlabel('Training Steps')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.show()\n",
        "\n",
        "            print(f\"üìâ Final training loss: {train_losses[-1]:.4f}\")\n",
        "        else:\n",
        "            print(\"üìä No training loss data available for plotting\")\n",
        "    else:\n",
        "        print(\"üìä Training history not available\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"üìä Could not plot training loss: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ REWARD MODELING TUTORIAL COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "üìö What we accomplished:\n",
        "‚úÖ Loaded synthetic instruction dataset from Hugging Face\n",
        "‚úÖ Preprocessed data for reward modeling\n",
        "‚úÖ Configured GPT-2 with LoRA for efficient training\n",
        "‚úÖ Trained a reward model using RewardTrainer\n",
        "‚úÖ Evaluated model performance with win rate metrics\n",
        "‚úÖ Created interactive testing capabilities\n",
        "\n",
        "üöÄ Next steps you can try:\n",
        "- Experiment with different base models (GPT-3.5, LLaMA, etc.)\n",
        "- Adjust LoRA configuration parameters\n",
        "- Try different learning rates and batch sizes\n",
        "- Use larger datasets for better performance\n",
        "- Implement more sophisticated evaluation metrics\n",
        "\n",
        "üí° Tips for better results:\n",
        "- Use more training data (we used only 1000 samples for demo)\n",
        "- Train for more epochs with a larger dataset\n",
        "- Fine-tune hyperparameters based on your specific use case\n",
        "- Consider using models pre-trained on instruction-following tasks\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nüîó Useful resources:\")\n",
        "print(\"- Hugging Face Transformers: https://huggingface.co/transformers/\")\n",
        "print(\"- TRL (Transformer Reinforcement Learning): https://github.com/lvwerra/trl\")\n",
        "print(\"- PEFT (Parameter-Efficient Fine-Tuning): https://github.com/huggingface/peft\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4a13fba7dd5e47a59d20c1772ecb7829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d468ec023444fe094efae0dccc5a62c",
              "IPY_MODEL_479aafedabf34da1a6ad5a880c5e37ee",
              "IPY_MODEL_fa1f9f8c95cc479ea2d3ea16d21871b1"
            ],
            "layout": "IPY_MODEL_d1d19ef7141b4d8aa41d26384fe54f4a"
          }
        },
        "0d468ec023444fe094efae0dccc5a62c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5af13f361b3b47679cda82b615c65d83",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c9cbc0d184de4a08b22f95b1aec2c1c7",
            "value": "Map:‚Äá100%"
          }
        },
        "479aafedabf34da1a6ad5a880c5e37ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d37d3cd772049f9a893d8bc1f41837a",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49bea984afec4032827e55c70582d5f6",
            "value": 1000
          }
        },
        "fa1f9f8c95cc479ea2d3ea16d21871b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec6313f0d64749eaaf3b4807626ff15e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9325e5bef65d46099f21973cae6c7f40",
            "value": "‚Äá1000/1000‚Äá[00:00&lt;00:00,‚Äá1385.38‚Äáexamples/s]"
          }
        },
        "d1d19ef7141b4d8aa41d26384fe54f4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5af13f361b3b47679cda82b615c65d83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9cbc0d184de4a08b22f95b1aec2c1c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d37d3cd772049f9a893d8bc1f41837a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49bea984afec4032827e55c70582d5f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec6313f0d64749eaaf3b4807626ff15e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9325e5bef65d46099f21973cae6c7f40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}