{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_cell"
      },
      "source": [
        "# ğŸš€ Instruction Fine-Tuning Tutorial - Google Colab Edition\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
        "\n",
        "## ğŸ¯ **Quick Start Guide for Google Colab**\n",
        "\n",
        "### **Step 1**: Enable GPU\n",
        "1. Go to `Runtime` â†’ `Change runtime type`\n",
        "2. Select `T4 GPU` (free tier)\n",
        "3. Click `Save`\n",
        "\n",
        "### **Step 2**: Run all cells\n",
        "- Use `Runtime` â†’ `Run all` or\n",
        "- Run cells one by one with `Shift + Enter`\n",
        "\n",
        "### **âš ï¸ Important Notes:**\n",
        "- â±ï¸ **Runtime Limit**: Colab free tier has ~12 hours max\n",
        "- ğŸ’¾ **Memory**: ~15GB RAM, manage your batch sizes\n",
        "- ğŸ”„ **Auto-disconnect**: Save your work periodically\n",
        "- ğŸ“± **Mobile-friendly**: Works on tablets/phones too!\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š What You'll Learn:\n",
        "âœ… Transform a base model into an instruction-following assistant  \n",
        "âœ… Use LoRA for efficient fine-tuning  \n",
        "âœ… Evaluate model performance with BLEU scores  \n",
        "âœ… Practice with real code generation tasks  \n",
        "âœ… Compare before/after model performance  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_cell"
      },
      "source": [
        "## ğŸ”§ **Colab Setup & Environment Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available and get system info\n",
        "import torch\n",
        "import psutil\n",
        "import GPUtil\n",
        "\n",
        "print(\"ğŸ–¥ï¸  **System Information**\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# GPU Check\n",
        "if torch.cuda.is_available():\n",
        "    gpu = GPUtil.getGPUs()[0]\n",
        "    print(f\"âœ… GPU Available: {gpu.name}\")\n",
        "    print(f\"ğŸ“Š GPU Memory: {gpu.memoryTotal}MB\")\n",
        "    print(f\"ğŸ”¥ CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"âŒ No GPU available. Go to Runtime â†’ Change runtime type â†’ Select GPU\")\n",
        "\n",
        "# RAM Check\n",
        "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
        "print(f\"ğŸ’¾ RAM Available: {ram_gb:.1f} GB\")\n",
        "\n",
        "# Python Version\n",
        "import sys\n",
        "print(f\"ğŸ Python Version: {sys.version.split()[0]}\")\n",
        "\n",
        "print(\"\\nğŸš€ **Ready to start fine-tuning!**\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_libraries"
      },
      "outputs": [],
      "source": [
        "# Install required libraries (optimized for Colab)\n",
        "print(\"ğŸ“¦ Installing libraries (this takes ~2-3 minutes)...\")\n",
        "\n",
        "!pip install -q transformers==4.42.3\n",
        "!pip install -q datasets==2.20.0\n",
        "!pip install -q peft==0.11.1\n",
        "!pip install -q trl==0.9.6\n",
        "!pip install -q evaluate==0.4.2\n",
        "!pip install -q sacrebleu==2.4.2\n",
        "!pip install -q accelerate\n",
        "\n",
        "print(\"âœ… All libraries installed!\")\n",
        "\n",
        "# Import libraries with error handling\n",
        "try:\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "    \n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "    from datasets import load_dataset\n",
        "    from torch.utils.data import Dataset\n",
        "    from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "    from peft import get_peft_model, LoraConfig, TaskType\n",
        "    import evaluate\n",
        "    import matplotlib.pyplot as plt\n",
        "    from tqdm import tqdm\n",
        "    import json\n",
        "    \n",
        "    print(\"âœ… Libraries imported successfully!\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Import error: {e}\")\n",
        "    print(\"ğŸ’¡ Try restarting runtime: Runtime â†’ Restart runtime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "theory_cell"
      },
      "source": [
        "## ğŸ§  **Quick Theory: What is Instruction Tuning?**\n",
        "\n",
        "```\n",
        "ğŸ”„ Base Model â†’ ğŸ¯ Instruction Tuned â†’ ğŸš€ Helpful Assistant\n",
        "   (Generic)      (Task-Specific)      (Production-Ready)\n",
        "```\n",
        "\n",
        "### **The Magic Formula:**\n",
        "```\n",
        "### Instruction:\n",
        "Write a Python function to calculate factorial\n",
        "\n",
        "### Response:\n",
        "def factorial(n):\n",
        "    if n <= 1:\n",
        "        return 1\n",
        "    return n * factorial(n-1)\n",
        "```\n",
        "\n",
        "**Key Benefits:**\n",
        "- ğŸ¯ **Better instruction following**\n",
        "- ğŸ“ **Appropriate response length**\n",
        "- ğŸ¨ **Higher quality outputs**\n",
        "- âš¡ **LoRA makes it efficient**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_cell"
      },
      "source": [
        "## ğŸ“Š **Step 1: Load Dataset (CodeAlpaca-20k)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_dataset"
      },
      "outputs": [],
      "source": [
        "# Download and load the CodeAlpaca dataset\n",
        "print(\"ğŸ“¥ Downloading CodeAlpaca dataset...\")\n",
        "\n",
        "!wget -q https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WzOT_CwDALWedTtXjwH7bA/CodeAlpaca-20k.json\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"CodeAlpaca-20k.json\", split=\"train\")\n",
        "print(f\"âœ… Dataset loaded: {len(dataset):,} examples\")\n",
        "\n",
        "# Show a sample\n",
        "sample = dataset[1000]\n",
        "print(\"\\nğŸ” **Sample Entry:**\")\n",
        "print(f\"ğŸ“ Instruction: {sample['instruction'][:100]}...\")\n",
        "print(f\"ğŸ’» Output: {sample['output'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_dataset"
      },
      "outputs": [],
      "source": [
        "# Prepare dataset for Colab (memory-efficient)\n",
        "print(\"ğŸ”§ Preparing dataset for Colab...\")\n",
        "\n",
        "# Filter examples without input (simpler cases)\n",
        "dataset = dataset.filter(lambda x: x[\"input\"] == '')\n",
        "print(f\"ğŸ” Filtered dataset: {len(dataset):,} examples\")\n",
        "\n",
        "# Shuffle and create small datasets for Colab\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Use smaller sizes for Colab efficiency\n",
        "train_size = min(1000, len(dataset_split['train']))  # Max 1000 for training\n",
        "test_size = min(100, len(dataset_split['test']))     # Max 100 for testing\n",
        "\n",
        "train_dataset = dataset_split['train'].select(range(train_size))\n",
        "test_dataset = dataset_split['test'].select(range(test_size))\n",
        "\n",
        "print(f\"ğŸ“š Training examples: {len(train_dataset):,}\")\n",
        "print(f\"ğŸ§ª Test examples: {len(test_dataset):,}\")\n",
        "print(\"âœ… Dataset ready for Colab!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_cell"
      },
      "source": [
        "## ğŸ¤– **Step 2: Load Model (OPT-350M - Perfect for Colab!)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer (Colab-optimized)\n",
        "print(\"ğŸ”½ Loading OPT-350M model...\")\n",
        "\n",
        "model_name = \"facebook/opt-350m\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load with memory optimization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"âœ… Model loaded on {device}\")\n",
        "\n",
        "# Check model size\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"ğŸ“Š Model parameters: {total_params:,}\")\n",
        "print(f\"ğŸ’¾ Estimated size: ~{total_params * 2 / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "format_cell"
      },
      "source": [
        "## ğŸ“ **Step 3: Format Data for Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "format_functions"
      },
      "outputs": [],
      "source": [
        "# Create formatting functions\n",
        "def formatting_prompts_func(dataset):\n",
        "    \"\"\"Format dataset for instruction tuning\"\"\"\n",
        "    output_texts = []\n",
        "    for i in range(len(dataset['instruction'])):\n",
        "        text = (\n",
        "            f\"### Instruction:\\n{dataset['instruction'][i]}\"\n",
        "            f\"\\n\\n### Response:\\n{dataset['output'][i]}</s>\"\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "def formatting_prompts_func_no_response(dataset):\n",
        "    \"\"\"Format dataset for inference (no response)\"\"\"\n",
        "    output_texts = []\n",
        "    for i in range(len(dataset['instruction'])):\n",
        "        text = (\n",
        "            f\"### Instruction:\\n{dataset['instruction'][i]}\"\n",
        "            f\"\\n\\n### Response:\\n\"\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "# Test formatting\n",
        "sample_formatted = formatting_prompts_func(test_dataset.select(range(1)))\n",
        "print(\"ğŸ“ **Example Formatted Data:**\")\n",
        "print(\"=\" * 60)\n",
        "print(sample_formatted[0])\n",
        "print(\"=\" * 60)\n",
        "print(\"âœ… Formatting functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_base_cell"
      },
      "source": [
        "## ğŸ§ª **Step 4: Test Base Model (Before Fine-tuning)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_base_model"
      },
      "outputs": [],
      "source": [
        "# Test base model performance\n",
        "print(\"ğŸ¤– Testing base model (before fine-tuning)...\")\n",
        "\n",
        "# Create generation pipeline\n",
        "gen_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    max_length=150,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# Test on 3 examples\n",
        "test_instructions = formatting_prompts_func_no_response(test_dataset.select(range(3)))\n",
        "base_outputs = []\n",
        "\n",
        "for instruction in test_instructions:\n",
        "    output = gen_pipeline(\n",
        "        instruction,\n",
        "        max_length=150,\n",
        "        num_beams=3,\n",
        "        early_stopping=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    base_outputs.append(output[0]['generated_text'])\n",
        "\n",
        "print(\"âœ… Base model testing complete!\")\n",
        "\n",
        "# Show results\n",
        "for i in range(3):\n",
        "    print(f\"\\nğŸ“‹ **Example {i+1}:**\")\n",
        "    print(f\"ğŸ¯ Task: {test_dataset[i]['instruction'][:80]}...\")\n",
        "    print(f\"âœ… Expected: {test_dataset[i]['output'][:80]}...\")\n",
        "    print(f\"ğŸ¤– Base Model: {base_outputs[i][:80]}...\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lora_cell"
      },
      "source": [
        "## âš¡ **Step 5: Setup LoRA (Memory-Efficient Fine-tuning)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_lora"
      },
      "outputs": [],
      "source": [
        "# Configure LoRA for efficient training\n",
        "print(\"âš¡ Setting up LoRA (Low-Rank Adaptation)...\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                                    # Rank: higher = more parameters but better performance\n",
        "    lora_alpha=32,                          # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],    # Which layers to adapt\n",
        "    lora_dropout=0.1,                       # Dropout for regularization\n",
        "    task_type=TaskType.CAUSAL_LM           # Task type\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Check trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "percentage = 100 * trainable_params / total_params\n",
        "\n",
        "print(f\"ğŸ“Š **LoRA Statistics:**\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Percentage trainable: {percentage:.2f}%\")\n",
        "print(f\"   Memory savings: ~{100-percentage:.1f}%\")\n",
        "print(\"âœ… LoRA setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_cell"
      },
      "source": [
        "## ğŸ‹ï¸ **Step 6: Fine-tune the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_training"
      },
      "outputs": [],
      "source": [
        "# Setup training configuration (Colab-optimized)\n",
        "print(\"âš™ï¸ Configuring training for Colab...\")\n",
        "\n",
        "# Data collator for instruction masking\n",
        "response_template = \"### Response:\\n\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "\n",
        "# Training arguments optimized for Colab\n",
        "training_args = SFTConfig(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=2,                    # Fewer epochs for Colab\n",
        "    per_device_train_batch_size=2,        # Small batch size\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,         # Simulate larger batch\n",
        "    learning_rate=5e-5,\n",
        "    max_seq_length=512,                    # Reasonable length\n",
        "    warmup_steps=50,\n",
        "    logging_steps=25,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=torch.cuda.is_available(),        # Mixed precision if GPU\n",
        "    dataloader_num_workers=0,              # Avoid multiprocessing issues\n",
        "    remove_unused_columns=False,\n",
        "    report_to=None,                        # Disable wandb/tensorboard\n",
        ")\n",
        "\n",
        "print(\"âœ… Training configuration ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_trainer"
      },
      "outputs": [],
      "source": [
        "# Create trainer\n",
        "print(\"ğŸ‹ï¸ Creating SFT Trainer...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset.select(range(10)),  # Small eval set\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    args=training_args,\n",
        "    packing=False,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer ready!\")\n",
        "print(f\"ğŸ“š Training on {len(train_dataset)} examples\")\n",
        "print(f\"ğŸ§ª Evaluating on {len(test_dataset.select(range(10)))} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_training"
      },
      "outputs": [],
      "source": [
        "# Start training!\n",
        "print(\"ğŸš€ Starting instruction fine-tuning...\")\n",
        "print(\"â±ï¸ Expected time: 10-15 minutes on T4 GPU\")\n",
        "print(\"â˜• Grab some coffee while the model trains!\")\n",
        "\n",
        "# Clear cache before training\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Train the model\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    print(\"ğŸ‰ Training completed successfully!\")\n",
        "    print(f\"ğŸ“‰ Final training loss: {train_result.training_loss:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Training error: {e}\")\n",
        "    print(\"ğŸ’¡ Try reducing batch_size or max_seq_length if you get OOM errors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation_cell"
      },
      "source": [
        "## ğŸ¯ **Step 7: Test Fine-tuned Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_finetuned"
      },
      "outputs": [],
      "source": [
        "# Test the fine-tuned model\n",
        "print(\"ğŸ§ª Testing fine-tuned model...\")\n",
        "\n",
        "# Create new pipeline for fine-tuned model\n",
        "finetuned_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    max_length=150,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# Generate responses\n",
        "finetuned_outputs = []\n",
        "for instruction in test_instructions:\n",
        "    output = finetuned_pipeline(\n",
        "        instruction,\n",
        "        max_length=150,\n",
        "        num_beams=3,\n",
        "        early_stopping=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    finetuned_outputs.append(output[0]['generated_text'])\n",
        "\n",
        "print(\"âœ… Fine-tuned model testing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "compare_results"
      },
      "outputs": [],
      "source": [
        "# Compare results side by side\n",
        "print(\"ğŸ¯ **BEFORE vs AFTER COMPARISON**\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nğŸ“‹ **Example {i+1}:**\")\n",
        "    print(f\"ğŸ¯ **Task:** {test_dataset[i]['instruction']}\")\n",
        "    print(f\"\\nâœ… **Expected Response:**\")\n",
        "    print(f\"   {test_dataset[i]['output']}\")\n",
        "    print(f\"\\nğŸ¤– **Base Model (Before):**\")\n",
        "    print(f\"   {base_outputs[i]}\")\n",
        "    print(f\"\\nğŸš€ **Fine-tuned Model (After):**\")\n",
        "    print(f\"   {finetuned_outputs[i]}\")\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "\n",
        "print(\"\\nğŸ’¡ **Key Improvements to Notice:**\")\n",
        "print(\"   âœ… More relevant and focused responses\")\n",
        "print(\"   âœ… Better instruction following\")\n",
        "print(\"   âœ… Appropriate response length\")\n",
        "print(\"   âœ… Higher quality code generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "score_cell"
      },
      "source": [
        "## ğŸ“Š **Step 8: Calculate BLEU Scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calculate_bleu"
      },
      "outputs": [],
      "source": [
        "# Calculate BLEU scores for comparison\n",
        "print(\"ğŸ“Š Calculating BLEU scores...\")\n",
        "\n",
        "# Prepare expected outputs\n",
        "expected_outputs = [test_dataset[i]['output'] for i in range(3)]\n",
        "\n",
        "# Load BLEU metric\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# Calculate scores\n",
        "base_bleu = sacrebleu.compute(predictions=base_outputs, references=expected_outputs)\n",
        "finetuned_bleu = sacrebleu.compute(predictions=finetuned_outputs, references=expected_outputs)\n",
        "\n",
        "print(f\"\\nğŸ“ˆ **BLEU Score Results:**\")\n",
        "print(f\"   Base Model: {base_bleu['score']:.2f}\")\n",
        "print(f\"   Fine-tuned Model: {finetuned_bleu['score']:.2f}\")\n",
        "\n",
        "improvement = finetuned_bleu['score'] - base_bleu['score']\n",
        "print(f\"   Improvement: +{improvement:.2f} points\")\n",
        "\n",
        "if improvement > 0:\n",
        "    print(\"\\nğŸ‰ **Success!** Fine-tuning improved the model!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ **Note:** Small sample size may not show full improvement.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_results"
      },
      "outputs": [],
      "source": [
        "# Visualize the improvement\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "models = ['Base Model\\n(Before)', 'Fine-tuned Model\\n(After)']\n",
        "scores = [base_bleu['score'], finetuned_bleu['score']]\n",
        "colors = ['#ff7f7f', '#7fbf7f']\n",
        "\n",
        "bars = plt.bar(models, scores, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "\n",
        "# Add value labels\n",
        "for bar, score in zip(bars, scores):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
        "             f'{score:.1f}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.ylabel('BLEU Score', fontsize=12)\n",
        "plt.title('ğŸš€ Model Performance: Before vs After Fine-tuning', fontsize=14, fontweight='bold')\n",
        "plt.ylim(0, max(scores) * 1.3)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add improvement arrow\n",
        "if improvement > 0:\n",
        "    plt.annotate(f'+{improvement:.1f}', \n",
        "                xy=(1, finetuned_bleu['score']), xytext=(1.2, finetuned_bleu['score'] + 2),\n",
        "                arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
        "                fontsize=12, fontweight='bold', color='green')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ“ˆ Chart shows the quantitative improvement!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercises_cell"
      },
      "source": [
        "## ğŸ“ **Hands-On Exercises (Try These!)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_1"
      },
      "source": [
        "### ğŸ”¥ **Exercise 1: Try Different Templates**\n",
        "\n",
        "Experiment with different formatting templates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise_1_code"
      },
      "outputs": [],
      "source": [
        "# Try a Q&A template instead\n",
        "def formatting_qa_template(dataset):\n",
        "    \"\"\"Format using Question-Answer template\"\"\"\n",
        "    output_texts = []\n",
        "    for i in range(len(dataset['instruction'])):\n",
        "        text = (\n",
        "            f\"### Question: {dataset['instruction'][i]}\\n\"\n",
        "            f\"### Answer: {dataset['output'][i]}</s>\"\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "# Test it\n",
        "qa_sample = formatting_qa_template(test_dataset.select(range(1)))\n",
        "print(\"ğŸ“ Q&A Template Example:\")\n",
        "print(qa_sample[0])\n",
        "\n",
        "# YOUR TURN: Try creating a \"Code Task\" template or \"Recipe\" template!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_2"
      },
      "source": [
        "### ğŸ”¥ **Exercise 2: Try Different Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise_2_code"
      },
      "outputs": [],
      "source": [
        "# Try a different model (GPT-Neo-125M)\n",
        "# Uncomment to try:\n",
        "\n",
        "# print(\"ğŸ”„ Loading GPT-Neo-125M...\")\n",
        "# model_neo = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
        "# tokenizer_neo = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
        "# \n",
        "# # Apply LoRA\n",
        "# lora_config_small = LoraConfig(\n",
        "#     r=8,  # Smaller rank for smaller model\n",
        "#     lora_alpha=16,\n",
        "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
        "#     lora_dropout=0.1,\n",
        "#     task_type=TaskType.CAUSAL_LM\n",
        "# )\n",
        "# model_neo = get_peft_model(model_neo, lora_config_small)\n",
        "# print(\"âœ… GPT-Neo ready for fine-tuning!\")\n",
        "\n",
        "print(\"ğŸ’¡ Uncomment the code above to try GPT-Neo-125M!\")\n",
        "print(\"ğŸ” Compare how different model sizes affect performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_3"
      },
      "source": [
        "### ğŸ”¥ **Exercise 3: Experiment with Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise_3_code"
      },
      "outputs": [],
      "source": [
        "# Try different LoRA configurations\n",
        "print(\"ğŸ”§ Experiment with these LoRA settings:\")\n",
        "print(\"\\n1. **Conservative Setup** (faster, less memory):\")\n",
        "print(\"   r=8, lora_alpha=16\")\n",
        "print(\"\\n2. **Balanced Setup** (current):\")\n",
        "print(\"   r=16, lora_alpha=32\")\n",
        "print(\"\\n3. **Aggressive Setup** (better performance, more memory):\")\n",
        "print(\"   r=32, lora_alpha=64\")\n",
        "\n",
        "print(\"\\nğŸ¯ Try different learning rates:\")\n",
        "print(\"   - 1e-5: Conservative (safer)\")\n",
        "print(\"   - 5e-5: Balanced (current)\")\n",
        "print(\"   - 1e-4: Aggressive (faster learning)\")\n",
        "\n",
        "print(\"\\nğŸ’¡ Pro Tip: Higher rank = more trainable parameters = better performance but slower training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_cell"
      },
      "source": [
        "## ğŸ‰ **Congratulations! You've Mastered Instruction Fine-tuning!**\n",
        "\n",
        "### ğŸ† **What You've Accomplished:**\n",
        "âœ… **Transformed** a base model into an instruction-following assistant  \n",
        "âœ… **Applied** LoRA for memory-efficient training  \n",
        "âœ… **Measured** performance improvements with BLEU scores  \n",
        "âœ… **Optimized** for Google Colab constraints  \n",
        "âœ… **Experimented** with different templates and configurations  \n",
        "\n",
        "### ğŸš€ **Next Steps:**\n",
        "1. **Scale Up**: Try with larger datasets (10k+ examples)\n",
        "2. **Advanced Techniques**: Explore RLHF and DPO\n",
        "3. **Domain-Specific**: Fine-tune for specific domains (medical, legal, etc.)\n",
        "4. **Production**: Deploy your models with FastAPI or Gradio\n",
        "5. **Multimodal**: Try vision-language instruction tuning\n",
        "\n",
        "### ğŸ’¡ **Key Takeaways:**\n",
        "- **LoRA** makes fine-tuning accessible and efficient\n",
        "- **Template design** is crucial for good results\n",
        "- **Small improvements** in metrics = big improvements in usability\n",
        "- **Colab** is perfect for learning and experimentation\n",
        "\n",
        "### ğŸ“š **Continue Learning:**\n",
        "- [Hugging Face Course](https://huggingface.co/course)\n",
        "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
        "- [TRL Library](https://huggingface.co/docs/trl)\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ’¾ **Save Your Work:**\n",
        "```python\n",
        "# Save your fine-tuned model\n",
        "model.save_pretrained(\"./my_instruction_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./my_instruction_tuned_model\")\n",
        "\n",
        "# Download to your computer\n",
        "from google.colab import files\n",
        "!tar -czf my_model.tar.gz ./my_instruction_tuned_model\n",
        "files.download('my_model.tar.gz')\n",
        "```\n",
        "\n",
        "**ğŸ¯ Happy Fine-tuning! ğŸš€âœ¨**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
