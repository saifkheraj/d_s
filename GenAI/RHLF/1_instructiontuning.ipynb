{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_cell"
      },
      "source": [
        "# ğŸš€ Instruction Fine-Tuning Tutorial - Google Colab Edition\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
        "\n",
        "## ğŸ¯ **Quick Start Guide for Google Colab**\n",
        "\n",
        "### **Step 1**: Enable GPU\n",
        "1. Go to `Runtime` â†’ `Change runtime type`\n",
        "2. Select `T4 GPU` (free tier)\n",
        "3. Click `Save`\n",
        "\n",
        "### **Step 2**: Run all cells\n",
        "- Use `Runtime` â†’ `Run all` or\n",
        "- Run cells one by one with `Shift + Enter`\n",
        "\n",
        "### **âš ï¸ Important Notes:**\n",
        "- â±ï¸ **Runtime Limit**: Colab free tier has ~12 hours max\n",
        "- ğŸ’¾ **Memory**: ~15GB RAM, manage your batch sizes\n",
        "- ğŸ”„ **Auto-disconnect**: Save your work periodically\n",
        "- ğŸ“± **Mobile-friendly**: Works on tablets/phones too!\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š What You'll Learn:\n",
        "âœ… Transform a base model into an instruction-following assistant  \n",
        "âœ… Use LoRA for efficient fine-tuning  \n",
        "âœ… Evaluate model performance with BLEU scores  \n",
        "âœ… Practice with real code generation tasks  \n",
        "âœ… Compare before/after model performance  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_cell"
      },
      "source": [
        "## ğŸ”§ **Colab Setup & Environment Check**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q trl==0.9.6"
      ],
      "metadata": {
        "id": "ljF5e2YYudkc",
        "outputId": "e466c6eb-a9f2-4e36-b937-7d0d3a0fb3da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.3/128.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q evaluate==0.4.2"
      ],
      "metadata": {
        "id": "aOQkRh6Fv_Ay"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "check_gpu",
        "outputId": "19673d33-d773-41f4-e99e-57617d1ad79d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'GPUtil'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-606129075.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mGPUtil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ–¥ï¸  **System Information**\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'GPUtil'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Check if GPU is available and get system info\n",
        "import torch\n",
        "import psutil\n",
        "import GPUtil\n",
        "\n",
        "print(\"ğŸ–¥ï¸  **System Information**\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# GPU Check\n",
        "if torch.cuda.is_available():\n",
        "    gpu = GPUtil.getGPUs()[0]\n",
        "    print(f\"âœ… GPU Available: {gpu.name}\")\n",
        "    print(f\"ğŸ“Š GPU Memory: {gpu.memoryTotal}MB\")\n",
        "    print(f\"ğŸ”¥ CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"âŒ No GPU available. Go to Runtime â†’ Change runtime type â†’ Select GPU\")\n",
        "\n",
        "# RAM Check\n",
        "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
        "print(f\"ğŸ’¾ RAM Available: {ram_gb:.1f} GB\")\n",
        "\n",
        "# Python Version\n",
        "import sys\n",
        "print(f\"ğŸ Python Version: {sys.version.split()[0]}\")\n",
        "\n",
        "print(\"\\nğŸš€ **Ready to start fine-tuning!**\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_libraries",
        "outputId": "5c56d8d8-1b4c-4b40-95d5-9bd676b0f0a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Installing libraries (this takes ~2-3 minutes)...\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries (optimized for Colab)\n",
        "print(\"ğŸ“¦ Installing libraries (this takes ~2-3 minutes)...\")\n",
        "\n",
        "!pip install -q transformers==4.42.3\n",
        "!pip install -q datasets==2.20.0\n",
        "!pip install -q peft==0.11.1\n",
        "!pip install -q trl==0.9.6\n",
        "!pip install -q evaluate==0.4.2\n",
        "!pip install -q sacrebleu==2.4.2\n",
        "!pip install -q accelerate\n",
        "\n",
        "print(\"âœ… All libraries installed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries with error handling\n",
        "try:\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "    from datasets import load_dataset\n",
        "    from torch.utils.data import Dataset\n",
        "    from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "    from peft import get_peft_model, LoraConfig, TaskType\n",
        "    import evaluate\n",
        "    import matplotlib.pyplot as plt\n",
        "    from tqdm import tqdm\n",
        "    import json\n",
        "\n",
        "    print(\"âœ… Libraries imported successfully!\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Import error: {e}\")\n",
        "    print(\"ğŸ’¡ Try restarting runtime: Runtime â†’ Restart runtime\")"
      ],
      "metadata": {
        "id": "4kN7bkcxtZOA",
        "outputId": "3abd5986-78c4-4f45-8289-61b3eed390fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "theory_cell"
      },
      "source": [
        "## ğŸ§  **Quick Theory: What is Instruction Tuning?**\n",
        "\n",
        "```\n",
        "ğŸ”„ Base Model â†’ ğŸ¯ Instruction Tuned â†’ ğŸš€ Helpful Assistant\n",
        "   (Generic)      (Task-Specific)      (Production-Ready)\n",
        "```\n",
        "\n",
        "### **The Magic Formula:**\n",
        "```\n",
        "### Instruction:\n",
        "Write a Python function to calculate factorial\n",
        "\n",
        "### Response:\n",
        "def factorial(n):\n",
        "    if n <= 1:\n",
        "        return 1\n",
        "    return n * factorial(n-1)\n",
        "```\n",
        "\n",
        "**Key Benefits:**\n",
        "- ğŸ¯ **Better instruction following**\n",
        "- ğŸ“ **Appropriate response length**\n",
        "- ğŸ¨ **Higher quality outputs**\n",
        "- âš¡ **LoRA makes it efficient**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_cell"
      },
      "source": [
        "## ğŸ“Š **Step 1: Load Dataset (CodeAlpaca-20k)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "load_dataset",
        "outputId": "33d68b07-dc5c-488a-fb33-fcc51e30c591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ Downloading CodeAlpaca dataset...\n",
            "âœ… Downloaded with wget\n",
            "âš ï¸ wget failed: Extra data: line 2 column 1 (char 141)\n",
            "ğŸŒ Trying direct download with requests...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Extra data: line 2 column 1 (char 141)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-385743365.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CodeAlpaca-20k.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 141)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, allow_nan, **kw)\u001b[0m\n\u001b[1;32m    513\u001b[0m             and not use_decimal and not allow_nan and not kw):\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 - line 20023 column 1 (char 141 - 6957007)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-385743365.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WzOT_CwDALWedTtXjwH7bA/CodeAlpaca-20k.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ… Downloaded with requests\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;31m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;31m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 141)"
          ]
        }
      ],
      "source": [
        "# âœ… WORKING SOLUTION FOR COLAB - Replace your dataset cell with this:\n",
        "\n",
        "import json\n",
        "import requests\n",
        "from datasets import Dataset\n",
        "\n",
        "print(\"ğŸ“¥ Downloading CodeAlpaca dataset...\")\n",
        "\n",
        "# Method 1: Try wget first\n",
        "try:\n",
        "    !wget -q https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WzOT_CwDALWedTtXjwH7bA/CodeAlpaca-20k.json\n",
        "    print(\"âœ… Downloaded with wget\")\n",
        "\n",
        "    # Load the JSON file directly\n",
        "    with open(\"CodeAlpaca-20k.json\", \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ wget failed: {e}\")\n",
        "    print(\"ğŸŒ Trying direct download with requests...\")\n",
        "\n",
        "    # Method 2: Direct download with requests\n",
        "    url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WzOT_CwDALWedTtXjwH7bA/CodeAlpaca-20k.json\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    print(\"âœ… Downloaded with requests\")\n",
        "\n",
        "# Convert to HuggingFace Dataset (this always works)\n",
        "dataset = Dataset.from_list(data)\n",
        "\n",
        "print(f\"âœ… Dataset loaded successfully: {len(dataset):,} examples\")\n",
        "print(f\"ğŸ“‹ Dataset features: {list(dataset.features.keys())}\")\n",
        "\n",
        "# Test the dataset\n",
        "sample = dataset[1000]\n",
        "print(f\"\\nğŸ” Sample Entry:\")\n",
        "print(f\"ğŸ“ Instruction: {sample['instruction']}\")\n",
        "print(f\"ğŸ“¥ Input: '{sample['input']}'\")\n",
        "print(f\"ğŸ“¤ Output: {sample['output'][:100]}...\")\n",
        "\n",
        "print(\"\\nğŸ‰ Dataset ready! Continue with the next cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_dataset"
      },
      "outputs": [],
      "source": [
        "# Prepare dataset for Colab (memory-efficient)\n",
        "print(\"ğŸ”§ Preparing dataset for Colab...\")\n",
        "\n",
        "# Filter examples without input (simpler cases)\n",
        "dataset = dataset.filter(lambda x: x[\"input\"] == '')\n",
        "print(f\"ğŸ” Filtered dataset: {len(dataset):,} examples\")\n",
        "\n",
        "# Shuffle and create small datasets for Colab\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Use smaller sizes for Colab efficiency\n",
        "train_size = min(1000, len(dataset_split['train']))  # Max 1000 for training\n",
        "test_size = min(100, len(dataset_split['test']))     # Max 100 for testing\n",
        "\n",
        "train_dataset = dataset_split['train'].select(range(train_size))\n",
        "test_dataset = dataset_split['test'].select(range(test_size))\n",
        "\n",
        "print(f\"ğŸ“š Training examples: {len(train_dataset):,}\")\n",
        "print(f\"ğŸ§ª Test examples: {len(test_dataset):,}\")\n",
        "print(\"âœ… Dataset ready for Colab!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_cell"
      },
      "source": [
        "## ğŸ¤– **Step 2: Load Model (OPT-350M - Perfect for Colab!)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer (Colab-optimized)\n",
        "print(\"ğŸ”½ Loading OPT-350M model...\")\n",
        "\n",
        "model_name = \"facebook/opt-350m\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load with memory optimization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"âœ… Model loaded on {device}\")\n",
        "\n",
        "# Check model size\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"ğŸ“Š Model parameters: {total_params:,}\")\n",
        "print(f\"ğŸ’¾ Estimated size: ~{total_params * 2 / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "format_cell"
      },
      "source": [
        "## ğŸ“ **Step 3: Format Data for Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "format_functions"
      },
      "outputs": [],
      "source": [
        "# Create formatting functions\n",
        "def formatting_prompts_func(dataset):\n",
        "    \"\"\"Format dataset for instruction tuning\"\"\"\n",
        "    output_texts = []\n",
        "    for i in range(len(dataset['instruction'])):\n",
        "        text = (\n",
        "            f\"### Instruction:\\n{dataset['instruction'][i]}\"\n",
        "            f\"\\n\\n### Response:\\n{dataset['output'][i]}</s>\"\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "def formatting_prompts_func_no_response(dataset):\n",
        "    \"\"\"Format dataset for inference (no response)\"\"\"\n",
        "    output_texts = []\n",
        "    for i in range(len(dataset['instruction'])):\n",
        "        text = (\n",
        "            f\"### Instruction:\\n{dataset['instruction'][i]}\"\n",
        "            f\"\\n\\n### Response:\\n\"\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "# Test formatting\n",
        "sample_formatted = formatting_prompts_func(test_dataset.select(range(1)))\n",
        "print(\"ğŸ“ **Example Formatted Data:**\")\n",
        "print(\"=\" * 60)\n",
        "print(sample_formatted[0])\n",
        "print(\"=\" * 60)\n",
        "print(\"âœ… Formatting functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_base_cell"
      },
      "source": [
        "## ğŸ§ª **Step 4: Test Base Model (Before Fine-tuning)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_base_model"
      },
      "outputs": [],
      "source": [
        "# Test base model performance\n",
        "print(\"ğŸ¤– Testing base model (before fine-tuning)...\")\n",
        "\n",
        "# Create generation pipeline\n",
        "gen_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    max_length=150,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# Test on 3 examples\n",
        "test_instructions = formatting_prompts_func_no_response(test_dataset.select(range(3)))\n",
        "base_outputs = []\n",
        "\n",
        "for instruction in test_instructions:\n",
        "    output = gen_pipeline(\n",
        "        instruction,\n",
        "        max_length=150,\n",
        "        num_beams=3,\n",
        "        early_stopping=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    base_outputs.append(output[0]['generated_text'])\n",
        "\n",
        "print(\"âœ… Base model testing complete!\")\n",
        "\n",
        "# Show results\n",
        "for i in range(3):\n",
        "    print(f\"\\nğŸ“‹ **Example {i+1}:**\")\n",
        "    print(f\"ğŸ¯ Task: {test_dataset[i]['instruction'][:80]}...\")\n",
        "    print(f\"âœ… Expected: {test_dataset[i]['output'][:80]}...\")\n",
        "    print(f\"ğŸ¤– Base Model: {base_outputs[i][:80]}...\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lora_cell"
      },
      "source": [
        "## âš¡ **Step 5: Setup LoRA (Memory-Efficient Fine-tuning)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_lora"
      },
      "outputs": [],
      "source": [
        "# Configure LoRA for efficient training\n",
        "print(\"âš¡ Setting up LoRA (Low-Rank Adaptation)...\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                                    # Rank: higher = more parameters but better performance\n",
        "    lora_alpha=32,                          # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],    # Which layers to adapt\n",
        "    lora_dropout=0.1,                       # Dropout for regularization\n",
        "    task_type=TaskType.CAUSAL_LM           # Task type\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Check trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "percentage = 100 * trainable_params / total_params\n",
        "\n",
        "print(f\"ğŸ“Š **LoRA Statistics:**\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Percentage trainable: {percentage:.2f}%\")\n",
        "print(f\"   Memory savings: ~{100-percentage:.1f}%\")\n",
        "print(\"âœ… LoRA setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_cell"
      },
      "source": [
        "## ğŸ‹ï¸ **Step 6: Fine-tune the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_training"
      },
      "outputs": [],
      "source": [
        "# Setup training configuration (Colab-optimized)\n",
        "print(\"âš™ï¸ Configuring training for Colab...\")\n",
        "\n",
        "# Data collator for instruction masking\n",
        "response_template = \"### Response:\\n\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "\n",
        "# Training arguments optimized for Colab\n",
        "training_args = SFTConfig(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=2,                    # Fewer epochs for Colab\n",
        "    per_device_train_batch_size=2,        # Small batch size\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,         # Simulate larger batch\n",
        "    learning_rate=5e-5,\n",
        "    max_seq_length=512,                    # Reasonable length\n",
        "    warmup_steps=50,\n",
        "    logging_steps=25,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=torch.cuda.is_available(),        # Mixed precision if GPU\n",
        "    dataloader_num_workers=0,              # Avoid multiprocessing issues\n",
        "    remove_unused_columns=False,\n",
        "    report_to=None,                        # Disable wandb/tensorboard\n",
        ")\n",
        "\n",
        "print(\"âœ… Training configuration ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_trainer"
      },
      "outputs": [],
      "source": [
        "# Create trainer\n",
        "print(\"ğŸ‹ï¸ Creating SFT Trainer...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset.select(range(10)),  # Small eval set\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    args=training_args,\n",
        "    packing=False,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer ready!\")\n",
        "print(f\"ğŸ“š Training on {len(train_dataset)} examples\")\n",
        "print(f\"ğŸ§ª Evaluating on {len(test_dataset.select(range(10)))} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_training"
      },
      "outputs": [],
      "source": [
        "# Start training!\n",
        "print(\"ğŸš€ Starting instruction fine-tuning...\")\n",
        "print(\"â±ï¸ Expected time: 10-15 minutes on T4 GPU\")\n",
        "print(\"â˜• Grab some coffee while the model trains!\")\n",
        "\n",
        "# Clear cache before training\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Train the model\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    print(\"ğŸ‰ Training completed successfully!\")\n",
        "    print(f\"ğŸ“‰ Final training loss: {train_result.training_loss:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Training error: {e}\")\n",
        "    print(\"ğŸ’¡ Try reducing batch_size or max_seq_length if you get OOM errors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation_cell"
      },
      "source": [
        "## ğŸ¯ **Step 7: Test Fine-tuned Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_finetuned"
      },
      "outputs": [],
      "source": [
        "# Test the fine-tuned model\n",
        "print(\"ğŸ§ª Testing fine-tuned model...\")\n",
        "\n",
        "# Create new pipeline for fine-tuned model\n",
        "finetuned_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    max_length=150,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# Generate responses\n",
        "finetuned_outputs = []\n",
        "for instruction in test_instructions:\n",
        "    output = finetuned_pipeline(\n",
        "        instruction,\n",
        "        max_length=150,\n",
        "        num_beams=3,\n",
        "        early_stopping=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    finetuned_outputs.append(output[0]['generated_text'])\n",
        "\n",
        "print(\"âœ… Fine-tuned model testing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "compare_results"
      },
      "outputs": [],
      "source": [
        "# Compare results side by side\n",
        "print(\"ğŸ¯ **BEFORE vs AFTER COMPARISON**\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nğŸ“‹ **Example {i+1}:**\")\n",
        "    print(f\"ğŸ¯ **Task:** {test_dataset[i]['instruction']}\")\n",
        "    print(f\"\\nâœ… **Expected Response:**\")\n",
        "    print(f\"   {test_dataset[i]['output']}\")\n",
        "    print(f\"\\nğŸ¤– **Base Model (Before):**\")\n",
        "    print(f\"   {base_outputs[i]}\")\n",
        "    print(f\"\\nğŸš€ **Fine-tuned Model (After):**\")\n",
        "    print(f\"   {finetuned_outputs[i]}\")\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "\n",
        "print(\"\\nğŸ’¡ **Key Improvements to Notice:**\")\n",
        "print(\"   âœ… More relevant and focused responses\")\n",
        "print(\"   âœ… Better instruction following\")\n",
        "print(\"   âœ… Appropriate response length\")\n",
        "print(\"   âœ… Higher quality code generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "score_cell"
      },
      "source": [
        "## ğŸ“Š **Step 8: Calculate BLEU Scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calculate_bleu"
      },
      "outputs": [],
      "source": [
        "# Calculate BLEU scores for comparison\n",
        "print(\"ğŸ“Š Calculating BLEU scores...\")\n",
        "\n",
        "# Prepare expected outputs\n",
        "expected_outputs = [test_dataset[i]['output'] for i in range(3)]\n",
        "\n",
        "# Load BLEU metric\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# Calculate scores\n",
        "base_bleu = sacrebleu.compute(predictions=base_outputs, references=expected_outputs)\n",
        "finetuned_bleu = sacrebleu.compute(predictions=finetuned_outputs, references=expected_outputs)\n",
        "\n",
        "print(f\"\\nğŸ“ˆ **BLEU Score Results:**\")\n",
        "print(f\"   Base Model: {base_bleu['score']:.2f}\")\n",
        "print(f\"   Fine-tuned Model: {finetuned_bleu['score']:.2f}\")\n",
        "\n",
        "improvement = finetuned_bleu['score'] - base_bleu['score']\n",
        "print(f\"   Improvement: +{improvement:.2f} points\")\n",
        "\n",
        "if improvement > 0:\n",
        "    print(\"\\nğŸ‰ **Success!** Fine-tuning improved the model!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ **Note:** Small sample size may not show full improvement.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_results"
      },
      "outputs": [],
      "source": [
        "# Visualize the improvement\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "models = ['Base Model\\n(Before)', 'Fine-tuned Model\\n(After)']\n",
        "scores = [base_bleu['score'], finetuned_bleu['score']]\n",
        "colors = ['#ff7f7f', '#7fbf7f']\n",
        "\n",
        "bars = plt.bar(models, scores, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "\n",
        "# Add value labels\n",
        "for bar, score in zip(bars, scores):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "             f'{score:.1f}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.ylabel('BLEU Score', fontsize=12)\n",
        "plt.title('ğŸš€ Model Performance: Before vs After Fine-tuning', fontsize=14, fontweight='bold')\n",
        "plt.ylim(0, max(scores) * 1.3)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add improvement arrow\n",
        "if improvement > 0:\n",
        "    plt.annotate(f'+{improvement:.1f}',\n",
        "                xy=(1, finetuned_bleu['score']), xytext=(1.2, finetuned_bleu['score'] + 2),\n",
        "                arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
        "                fontsize=12, fontweight='bold', color='green')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ“ˆ Chart shows the quantitative improvement!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercises_cell"
      },
      "source": [
        "## ğŸ“ **Hands-On Exercises (Try These!)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_1"
      },
      "source": [
        "### ğŸ”¥ **Exercise 1: Try Different Templates**\n",
        "\n",
        "Experiment with different formatting templates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise_1_code"
      },
      "outputs": [],
      "source": [
        "# Try a Q&A template instead\n",
        "def formatting_qa_template(dataset):\n",
        "    \"\"\"Format using Question-Answer template\"\"\"\n",
        "    output_texts = []\n",
        "    for i in range(len(dataset['instruction'])):\n",
        "        text = (\n",
        "            f\"### Question: {dataset['instruction'][i]}\\n\"\n",
        "            f\"### Answer: {dataset['output'][i]}</s>\"\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "# Test it\n",
        "qa_sample = formatting_qa_template(test_dataset.select(range(1)))\n",
        "print(\"ğŸ“ Q&A Template Example:\")\n",
        "print(qa_sample[0])\n",
        "\n",
        "# YOUR TURN: Try creating a \"Code Task\" template or \"Recipe\" template!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_2"
      },
      "source": [
        "### ğŸ”¥ **Exercise 2: Try Different Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise_2_code"
      },
      "outputs": [],
      "source": [
        "# Try a different model (GPT-Neo-125M)\n",
        "# Uncomment to try:\n",
        "\n",
        "# print(\"ğŸ”„ Loading GPT-Neo-125M...\")\n",
        "# model_neo = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
        "# tokenizer_neo = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
        "#\n",
        "# # Apply LoRA\n",
        "# lora_config_small = LoraConfig(\n",
        "#     r=8,  # Smaller rank for smaller model\n",
        "#     lora_alpha=16,\n",
        "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
        "#     lora_dropout=0.1,\n",
        "#     task_type=TaskType.CAUSAL_LM\n",
        "# )\n",
        "# model_neo = get_peft_model(model_neo, lora_config_small)\n",
        "# print(\"âœ… GPT-Neo ready for fine-tuning!\")\n",
        "\n",
        "print(\"ğŸ’¡ Uncomment the code above to try GPT-Neo-125M!\")\n",
        "print(\"ğŸ” Compare how different model sizes affect performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_3"
      },
      "source": [
        "### ğŸ”¥ **Exercise 3: Experiment with Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise_3_code"
      },
      "outputs": [],
      "source": [
        "# Try different LoRA configurations\n",
        "print(\"ğŸ”§ Experiment with these LoRA settings:\")\n",
        "print(\"\\n1. **Conservative Setup** (faster, less memory):\")\n",
        "print(\"   r=8, lora_alpha=16\")\n",
        "print(\"\\n2. **Balanced Setup** (current):\")\n",
        "print(\"   r=16, lora_alpha=32\")\n",
        "print(\"\\n3. **Aggressive Setup** (better performance, more memory):\")\n",
        "print(\"   r=32, lora_alpha=64\")\n",
        "\n",
        "print(\"\\nğŸ¯ Try different learning rates:\")\n",
        "print(\"   - 1e-5: Conservative (safer)\")\n",
        "print(\"   - 5e-5: Balanced (current)\")\n",
        "print(\"   - 1e-4: Aggressive (faster learning)\")\n",
        "\n",
        "print(\"\\nğŸ’¡ Pro Tip: Higher rank = more trainable parameters = better performance but slower training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_cell"
      },
      "source": [
        "## ğŸ‰ **Congratulations! You've Mastered Instruction Fine-tuning!**\n",
        "\n",
        "### ğŸ† **What You've Accomplished:**\n",
        "âœ… **Transformed** a base model into an instruction-following assistant  \n",
        "âœ… **Applied** LoRA for memory-efficient training  \n",
        "âœ… **Measured** performance improvements with BLEU scores  \n",
        "âœ… **Optimized** for Google Colab constraints  \n",
        "âœ… **Experimented** with different templates and configurations  \n",
        "\n",
        "### ğŸš€ **Next Steps:**\n",
        "1. **Scale Up**: Try with larger datasets (10k+ examples)\n",
        "2. **Advanced Techniques**: Explore RLHF and DPO\n",
        "3. **Domain-Specific**: Fine-tune for specific domains (medical, legal, etc.)\n",
        "4. **Production**: Deploy your models with FastAPI or Gradio\n",
        "5. **Multimodal**: Try vision-language instruction tuning\n",
        "\n",
        "### ğŸ’¡ **Key Takeaways:**\n",
        "- **LoRA** makes fine-tuning accessible and efficient\n",
        "- **Template design** is crucial for good results\n",
        "- **Small improvements** in metrics = big improvements in usability\n",
        "- **Colab** is perfect for learning and experimentation\n",
        "\n",
        "### ğŸ“š **Continue Learning:**\n",
        "- [Hugging Face Course](https://huggingface.co/course)\n",
        "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
        "- [TRL Library](https://huggingface.co/docs/trl)\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ’¾ **Save Your Work:**\n",
        "```python\n",
        "# Save your fine-tuned model\n",
        "model.save_pretrained(\"./my_instruction_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./my_instruction_tuned_model\")\n",
        "\n",
        "# Download to your computer\n",
        "from google.colab import files\n",
        "!tar -czf my_model.tar.gz ./my_instruction_tuned_model\n",
        "files.download('my_model.tar.gz')\n",
        "```\n",
        "\n",
        "**ğŸ¯ Happy Fine-tuning! ğŸš€âœ¨**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}